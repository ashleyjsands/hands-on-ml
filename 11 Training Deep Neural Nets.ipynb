{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.  28.]\n",
      " [ 49.  64.]]\n"
     ]
    }
   ],
   "source": [
    "#  Create a test graph\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first train the model to identify digits 0 to 4 and then do transfer training to identify images 5 to 9.\n",
    "\n",
    "def divide_dataset_into_two_sets(images, labels):\n",
    "    assert len(images) == len(labels)\n",
    "    zero_to_four_images = []\n",
    "    zero_to_four_labels = []\n",
    "    five_to_nine_images = []\n",
    "    five_to_nine_labels = []\n",
    "    four_label = 4\n",
    "    for i in range(len(images)):\n",
    "        if labels[i] <= four_label:\n",
    "            zero_to_four_images.append(images[i])\n",
    "            zero_to_four_labels.append(labels[i])\n",
    "        else:\n",
    "            five_to_nine_images.append(images[i])\n",
    "            five_to_nine_labels.append(labels[i])\n",
    "    return np.array(zero_to_four_images), np.array(zero_to_four_labels), np.array(five_to_nine_images), np.array(five_to_nine_labels)\n",
    "\n",
    "train_zero_to_four_images, train_zero_to_four_labels, train_five_to_nine_images, train_five_to_nine_labels = divide_dataset_into_two_sets(mnist.train.images, mnist.train.labels)\n",
    "validation_zero_to_four_images, validation_zero_to_four_labels, validation_five_to_nine_images, validation_five_to_nine_labels = divide_dataset_into_two_sets(mnist.validation.images, mnist.validation.labels)\n",
    "test_zero_to_four_images, test_zero_to_four_labels, test_five_to_nine_images, test_five_to_nine_labels = divide_dataset_into_two_sets(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5139, 784)\n",
      "(5139,)\n",
      "(4861, 784)\n",
      "(4861,)\n"
     ]
    }
   ],
   "source": [
    "print(test_zero_to_four_images.shape)\n",
    "print(test_zero_to_four_labels.shape)\n",
    "print(test_five_to_nine_images.shape)\n",
    "print(test_five_to_nine_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import batch_norm\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "try:\n",
    "    # Access a global and if it exists, reset the graph.\n",
    "    number_of_inputs\n",
    "    reset_graph()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "number_of_inputs = 28 * 28\n",
    "n_hidden_per_layer = [90] * 4\n",
    "n_output = 5\n",
    "\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name=\"is_training\")\n",
    "bn_params = {\n",
    "    \"training\": is_training,\n",
    "    \"momentum\": 0.99\n",
    "}\n",
    "\n",
    "def he_normal_initialisation(n_inputs, n_outputs):\n",
    "    stddev = np.power(2 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.truncated_normal((n_inputs, n_outputs), stddev=stddev)\n",
    "\n",
    "def he_uniform_initialisation(n_inputs, n_outputs):\n",
    "    r = np.power(6 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.random_uniform((n_inputs, n_outputs), -r, r)\n",
    "\n",
    "def neuron_layer(X, n_neurons, name):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        W = tf.Variable(he_normal_initialisation(n_inputs, n_neurons), name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        bn = tf.layers.batch_normalization(z, **bn_params)\n",
    "        return tf.nn.elu(bn)\n",
    "\n",
    "# batch normalisation doesn't work on my GPU for some reason.\n",
    "#with tf.device(\"/gpu:0\"):\n",
    "with tf.device(\"/cpu:0\"):    \n",
    "    x = tf.placeholder(tf.float32, shape=(None, number_of_inputs), name=\"input\")\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        input_tensor = x\n",
    "        for i in range(len(n_hidden_per_layer)):\n",
    "            input_tensor = neuron_layer(input_tensor, n_hidden_per_layer[i], \"hidden\" + str(i + 1))\n",
    "        logits = neuron_layer(input_tensor, n_output, \"output\")\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "\n",
    "    with tf.name_scope(\"training\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.005)\n",
    "        # Make the training op depend upon the update ops from the batch normalisation layer.\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    k = 1\n",
    "    correctness = tf.nn.in_top_k(logits, y, k)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctness, tf.float32)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "interim_checkpoint_path = \"./checkpoints/0-4_mnist_model.ckpt\"\n",
    "early_stopping_checkpoint_path = \"./checkpoints/0-4_mnist_model_early_stopping.ckpt\"\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "train_accuracy_summary = tf.summary.scalar(\"train_accuracy\", accuracy)\n",
    "summary_op = tf.summary.merge([loss_summary, train_accuracy_summary])\n",
    "file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "        \n",
    "def create_next_batch_fn(images, labels, batch_size):\n",
    "    assert len(images) == len(labels)\n",
    "    current_batch = 0\n",
    "    def next_batch():\n",
    "        nonlocal current_batch\n",
    "        i = current_batch\n",
    "        #print(current_batch)\n",
    "        current_batch = (current_batch + batch_size) % len(images)\n",
    "        return images[i:i+batch_size], labels[i:i+batch_size]\n",
    "    return next_batch\n",
    "\n",
    "next_test_batch = create_next_batch_fn(test_zero_to_four_images, test_zero_to_four_labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "loss: 1.77468 training accuracy: 18.5\n",
      "validation accuracy 39.6794\n",
      "Saving best model\n",
      "loss: 0.489671 training accuracy: 91.5\n",
      "loss: 0.405178 training accuracy: 95.0\n",
      "validation accuracy 93.8624\n",
      "Saving best model\n",
      "loss: 0.366415 training accuracy: 98.0\n",
      "loss: 0.30016 training accuracy: 98.0\n",
      "validation accuracy 90.8913\n",
      "loss: 0.309318 training accuracy: 97.0\n",
      "loss: 0.249728 training accuracy: 97.0\n",
      "validation accuracy 94.6833\n",
      "Saving best model\n",
      "loss: 0.207218 training accuracy: 99.5\n",
      "loss: 0.203392 training accuracy: 99.5\n",
      "validation accuracy 96.7553\n",
      "Saving best model\n",
      "loss: 0.179441 training accuracy: 99.0\n",
      "loss: 0.162836 training accuracy: 99.5\n",
      "validation accuracy 96.4816\n",
      "loss: 0.181951 training accuracy: 98.5\n",
      "loss: 0.140327 training accuracy: 99.5\n",
      "validation accuracy 96.9507\n",
      "Saving best model\n",
      "loss: 0.132947 training accuracy: 99.5\n",
      "loss: 0.14999 training accuracy: 99.0\n",
      "validation accuracy 97.0289\n",
      "Saving best model\n",
      "loss: 0.100827 training accuracy: 100.0\n",
      "loss: 0.105153 training accuracy: 99.5\n",
      "validation accuracy 96.5207\n",
      "loss: 0.0931711 training accuracy: 100.0\n",
      "loss: 0.108405 training accuracy: 99.0\n",
      "validation accuracy 97.2244\n",
      "Saving best model\n",
      "loss: 0.104995 training accuracy: 99.0\n",
      "loss: 0.0957524 training accuracy: 99.0\n",
      "validation accuracy 97.4589\n",
      "Saving best model\n",
      "loss: 0.0764432 training accuracy: 100.0\n",
      "loss: 0.0690723 training accuracy: 100.0\n",
      "validation accuracy 97.3417\n",
      "loss: 0.067616 training accuracy: 100.0\n",
      "loss: 0.0658722 training accuracy: 100.0\n",
      "validation accuracy 97.0289\n",
      "loss: 0.0536748 training accuracy: 100.0\n",
      "loss: 0.0602934 training accuracy: 100.0\n",
      "validation accuracy 97.0289\n",
      "loss: 0.056395 training accuracy: 100.0\n",
      "epoch 1\n",
      "loss: 0.0468485 training accuracy: 100.0\n",
      "validation accuracy 97.3808\n",
      "loss: 0.0513084 training accuracy: 100.0\n",
      "loss: 0.0498344 training accuracy: 100.0\n",
      "validation accuracy 96.8335\n",
      "loss: 0.0381954 training accuracy: 100.0\n",
      "loss: 0.0442312 training accuracy: 100.0\n",
      "validation accuracy 97.3026\n",
      "loss: 0.038168 training accuracy: 100.0\n",
      "loss: 0.0547621 training accuracy: 99.5\n",
      "validation accuracy 96.4034\n",
      "loss: 0.0514539 training accuracy: 100.0\n",
      "loss: 0.0436377 training accuracy: 100.0\n",
      "validation accuracy 97.1071\n",
      "loss: 0.0381773 training accuracy: 100.0\n",
      "loss: 0.0372872 training accuracy: 100.0\n",
      "validation accuracy 97.3026\n",
      "loss: 0.040594 training accuracy: 100.0\n",
      "loss: 0.0370869 training accuracy: 100.0\n",
      "validation accuracy 97.3808\n",
      "loss: 0.0406021 training accuracy: 100.0\n",
      "loss: 0.0310732 training accuracy: 100.0\n",
      "validation accuracy 96.5989\n",
      "loss: 0.0281323 training accuracy: 100.0\n",
      "loss: 0.0334176 training accuracy: 99.5\n",
      "validation accuracy 97.2244\n",
      "loss: 0.0273753 training accuracy: 100.0\n",
      "loss: 0.0254748 training accuracy: 100.0\n",
      "validation accuracy 97.2244\n",
      "loss: 0.0247623 training accuracy: 100.0\n",
      "loss: 0.0296352 training accuracy: 100.0\n",
      "validation accuracy 97.498\n",
      "Saving best model\n",
      "loss: 0.0196557 training accuracy: 100.0\n",
      "loss: 0.0234523 training accuracy: 100.0\n",
      "validation accuracy 97.3808\n",
      "loss: 0.0206741 training accuracy: 100.0\n",
      "loss: 0.0187071 training accuracy: 100.0\n",
      "validation accuracy 97.5762\n",
      "Saving best model\n",
      "loss: 0.019182 training accuracy: 100.0\n",
      "loss: 0.0169842 training accuracy: 100.0\n",
      "validation accuracy 97.6544\n",
      "Saving best model\n",
      "loss: 0.0168107 training accuracy: 100.0\n",
      "epoch 2\n",
      "loss: 0.0193197 training accuracy: 100.0\n",
      "validation accuracy 97.6544\n",
      "loss: 0.0157179 training accuracy: 100.0\n",
      "loss: 0.0166309 training accuracy: 100.0\n",
      "validation accuracy 97.498\n",
      "loss: 0.0153612 training accuracy: 100.0\n",
      "loss: 0.0148951 training accuracy: 100.0\n",
      "validation accuracy 97.6935\n",
      "Saving best model\n",
      "loss: 0.0145771 training accuracy: 100.0\n",
      "loss: 0.0138626 training accuracy: 100.0\n",
      "validation accuracy 97.7717\n",
      "Saving best model\n",
      "loss: 0.0152923 training accuracy: 100.0\n",
      "loss: 0.0135114 training accuracy: 100.0\n",
      "validation accuracy 97.6153\n",
      "loss: 0.0130215 training accuracy: 100.0\n",
      "loss: 0.0126197 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.0122075 training accuracy: 100.0\n",
      "loss: 0.0147196 training accuracy: 100.0\n",
      "validation accuracy 97.6544\n",
      "loss: 0.0126728 training accuracy: 100.0\n",
      "loss: 0.0111661 training accuracy: 100.0\n",
      "validation accuracy 97.7717\n",
      "loss: 0.0126257 training accuracy: 100.0\n",
      "loss: 0.0109709 training accuracy: 100.0\n",
      "validation accuracy 97.6935\n",
      "loss: 0.011536 training accuracy: 100.0\n",
      "loss: 0.0122094 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.0103793 training accuracy: 100.0\n",
      "loss: 0.0106413 training accuracy: 100.0\n",
      "validation accuracy 97.7717\n",
      "loss: 0.010076 training accuracy: 100.0\n",
      "loss: 0.00972798 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.0102573 training accuracy: 100.0\n",
      "loss: 0.00977038 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.0100038 training accuracy: 100.0\n",
      "loss: 0.00907463 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.00928937 training accuracy: 100.0\n",
      "epoch 3\n",
      "loss: 0.0097317 training accuracy: 100.0\n",
      "validation accuracy 97.8108\n",
      "Saving best model\n",
      "loss: 0.00904748 training accuracy: 100.0\n",
      "loss: 0.00843321 training accuracy: 100.0\n",
      "validation accuracy 97.6935\n",
      "loss: 0.00936987 training accuracy: 100.0\n",
      "loss: 0.00824175 training accuracy: 100.0\n",
      "validation accuracy 97.7717\n",
      "loss: 0.00847493 training accuracy: 100.0\n",
      "loss: 0.00830206 training accuracy: 100.0\n",
      "validation accuracy 97.6153\n",
      "loss: 0.00767498 training accuracy: 100.0\n",
      "loss: 0.00798741 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.00757492 training accuracy: 100.0\n",
      "loss: 0.107477 training accuracy: 100.0\n",
      "validation accuracy 76.9351\n",
      "loss: 0.141093 training accuracy: 94.5\n",
      "loss: 0.129145 training accuracy: 95.5\n",
      "validation accuracy 92.5332\n",
      "loss: 0.0654672 training accuracy: 98.5\n",
      "loss: 0.0487412 training accuracy: 99.0\n",
      "validation accuracy 96.6771\n",
      "loss: 0.04844 training accuracy: 99.0\n",
      "loss: 0.0302773 training accuracy: 100.0\n",
      "validation accuracy 96.6771\n",
      "loss: 0.0185872 training accuracy: 100.0\n",
      "loss: 0.0231891 training accuracy: 100.0\n",
      "validation accuracy 97.068\n",
      "loss: 0.0186849 training accuracy: 100.0\n",
      "loss: 0.0109459 training accuracy: 100.0\n",
      "validation accuracy 97.3417\n",
      "loss: 0.018306 training accuracy: 100.0\n",
      "loss: 0.0112425 training accuracy: 100.0\n",
      "validation accuracy 96.9507\n",
      "loss: 0.0103555 training accuracy: 100.0\n",
      "loss: 0.0134654 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.00759477 training accuracy: 100.0\n",
      "loss: 0.00934426 training accuracy: 100.0\n",
      "validation accuracy 97.8499\n",
      "Saving best model\n",
      "loss: 0.00746031 training accuracy: 100.0\n",
      "epoch 4\n",
      "loss: 0.00858339 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.0095106 training accuracy: 100.0\n",
      "loss: 0.00714071 training accuracy: 100.0\n",
      "validation accuracy 97.889\n",
      "Saving best model\n",
      "loss: 0.00745354 training accuracy: 100.0\n",
      "loss: 0.0080125 training accuracy: 100.0\n",
      "validation accuracy 97.8499\n",
      "loss: 0.00835065 training accuracy: 100.0\n",
      "loss: 0.00778477 training accuracy: 100.0\n",
      "validation accuracy 97.8108\n",
      "loss: 0.0074241 training accuracy: 100.0\n",
      "loss: 0.00629357 training accuracy: 100.0\n",
      "validation accuracy 97.8108\n",
      "loss: 0.00674369 training accuracy: 100.0\n",
      "loss: 0.00584256 training accuracy: 100.0\n",
      "validation accuracy 97.9281\n",
      "Saving best model\n",
      "loss: 0.00616526 training accuracy: 100.0\n",
      "loss: 0.00674412 training accuracy: 100.0\n",
      "validation accuracy 97.889\n",
      "loss: 0.00470815 training accuracy: 100.0\n",
      "loss: 0.00716601 training accuracy: 100.0\n",
      "validation accuracy 97.889\n",
      "loss: 0.00593395 training accuracy: 100.0\n",
      "loss: 0.00602951 training accuracy: 100.0\n",
      "validation accuracy 97.8499\n",
      "loss: 0.00606536 training accuracy: 100.0\n",
      "loss: 0.00537958 training accuracy: 100.0\n",
      "validation accuracy 97.9672\n",
      "Saving best model\n",
      "loss: 0.0053426 training accuracy: 100.0\n",
      "loss: 0.00444021 training accuracy: 100.0\n",
      "validation accuracy 97.9281\n",
      "loss: 0.0049289 training accuracy: 100.0\n",
      "loss: 0.00553991 training accuracy: 100.0\n",
      "validation accuracy 97.8499\n",
      "loss: 0.00481939 training accuracy: 100.0\n",
      "loss: 0.005687 training accuracy: 100.0\n",
      "validation accuracy 97.9281\n",
      "loss: 0.00456736 training accuracy: 100.0\n",
      "loss: 0.004842 training accuracy: 100.0\n",
      "validation accuracy 97.8108\n",
      "loss: 0.00462155 training accuracy: 100.0\n",
      "epoch 5\n",
      "loss: 0.00444915 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.00421567 training accuracy: 100.0\n",
      "loss: 0.00449796 training accuracy: 100.0\n",
      "validation accuracy 97.8499\n",
      "loss: 0.00399983 training accuracy: 100.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.00510313 training accuracy: 100.0\n",
      "validation accuracy 97.8108\n",
      "loss: 0.00476644 training accuracy: 100.0\n",
      "loss: 0.00360579 training accuracy: 100.0\n",
      "validation accuracy 97.6544\n",
      "loss: 0.00396431 training accuracy: 100.0\n",
      "loss: 0.00438855 training accuracy: 100.0\n",
      "validation accuracy 97.7717\n",
      "loss: 0.00468595 training accuracy: 100.0\n",
      "loss: 0.00489555 training accuracy: 100.0\n",
      "validation accuracy 97.7717\n",
      "loss: 0.00364366 training accuracy: 100.0\n",
      "loss: 0.00401819 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.00368568 training accuracy: 100.0\n",
      "loss: 0.0037658 training accuracy: 100.0\n",
      "validation accuracy 97.6544\n",
      "loss: 0.00378363 training accuracy: 100.0\n",
      "loss: 0.00354041 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.00396494 training accuracy: 100.0\n",
      "loss: 0.00349103 training accuracy: 100.0\n",
      "validation accuracy 97.6544\n",
      "loss: 0.00374231 training accuracy: 100.0\n",
      "loss: 0.00334692 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.00327348 training accuracy: 100.0\n",
      "loss: 0.00449158 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.00371403 training accuracy: 100.0\n",
      "loss: 0.00282779 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.00397007 training accuracy: 100.0\n",
      "loss: 0.00313289 training accuracy: 100.0\n",
      "validation accuracy 97.6935\n",
      "loss: 0.00343148 training accuracy: 100.0\n",
      "epoch 6\n",
      "loss: 0.0033675 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.00287054 training accuracy: 100.0\n",
      "loss: 0.00352102 training accuracy: 100.0\n",
      "validation accuracy 97.7326\n",
      "loss: 0.00458881 training accuracy: 100.0\n",
      "loss: 0.00330462 training accuracy: 100.0\n",
      "validation accuracy 97.6935\n",
      "Stopping early during epoch 6\n",
      "Best validation performance 97.9672\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/0-4_mnist_model_early_stopping.ckpt\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "n_batches = int(np.ceil(mnist.train.num_examples // batch_size))\n",
    "\n",
    "early_stopping_check_frequency = batch_size // 10\n",
    "early_stopping_check_limit = batch_size * 2\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init)\n",
    "    #saver.restore(sess, interim_checkpoint_path)\n",
    "    \n",
    "    best_validation_acc = 0.0\n",
    "    best_validation_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch\", epoch)\n",
    "        for batch_index in range(n_batches):\n",
    "            step = epoch * n_batches + batch_index\n",
    "            X_batch, y_batch = next_test_batch()\n",
    "            if batch_index % 10 == 0:\n",
    "                # Output summaries\n",
    "                summary_str = summary_op.eval(feed_dict={x: X_batch, y: y_batch, is_training: False})\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            t, l, a, _ = sess.run([training_op, loss, accuracy, extra_update_ops], feed_dict={x: X_batch, y: y_batch, is_training: True})\n",
    "            if batch_index % 10 == 0: print(\"loss:\", l, \"training accuracy:\", a)\n",
    "            # Early stopping check\n",
    "            if batch_index % early_stopping_check_frequency == 0:\n",
    "                validation_acc = accuracy.eval(feed_dict={x: validation_zero_to_four_images, y: validation_zero_to_four_labels, is_training: False})\n",
    "                print(\"validation accuracy\", validation_acc)\n",
    "                if validation_acc > best_validation_acc:\n",
    "                    print(\"Saving best model\")\n",
    "                    saver.save(sess, early_stopping_checkpoint_path)\n",
    "                    best_validation_acc = validation_acc\n",
    "                    best_validation_step = step\n",
    "                elif step >= (best_validation_step + early_stopping_check_limit):\n",
    "                    print(\"Stopping early during epoch\", epoch)\n",
    "                    print(\"Best validation performance\", best_validation_acc)\n",
    "                    break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "        save_path = saver.save(sess, interim_checkpoint_path)\n",
    "    saver.restore(sess, early_stopping_checkpoint_path)\n",
    "    #test_acc = accuracy.eval(feed_dict={x: test_zero_to_four_images, y: test_zero_to_four_labels, is_training: false})\n",
    "    #print(\">>>>>>>>>> test dataset accuracy:\", test_acc)\n",
    "\n",
    "    save_path = saver.save(sess, \"./checkpoints/0-4_mnist_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalisation does produce a better model (when you explicitly run the extra updates op). It does however, greatly increase the model's learning speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(mnist))\n",
    "print(mnist.train.num_examples)\n",
    "print(mnist.validation.num_examples)\n",
    "print(mnist.test.num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 9\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs = 28 * 28\n",
    "n_hidden_per_layer = [100, 100, 100, 100, 100]\n",
    "n_output = 10\n",
    "\n",
    "def he_normal_initialisation(n_inputs, n_outputs):\n",
    "    stddev = np.power(2 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.truncated_normal((n_inputs, n_outputs), stddev=stddev)\n",
    "\n",
    "def he_uniform_initialisation(n_inputs, n_outputs):\n",
    "    r = np.power(6 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.random_uniform((n_inputs, n_outputs), -r, r)\n",
    "\n",
    "def neuron_layer(X, n_neurons, name):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        W = tf.Variable(he_normal_initialisation(n_inputs, n_neurons), name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        return tf.nn.elu(z)\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    x = tf.placeholder(tf.float32, shape=(None, number_of_inputs), name=\"input\")\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        input_tensor = x\n",
    "        for i in range(len(n_hidden_per_layer)):\n",
    "            input_tensor = neuron_layer(input_tensor, n_hidden_per_layer[i], \"hidden\" + str(i + 1))\n",
    "        logits = neuron_layer(input_tensor, n_output, \"output\")\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "\n",
    "    with tf.name_scope(\"training\"):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    k = 1\n",
    "    correctness = tf.nn.in_top_k(logits, y, k)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctness, tf.float32)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "interim_checkpoint_path = \"./checkpoints/mnist_model.ckpt\"\n",
    "early_stopping_checkpoint_path = \"./checkpoints/mnist_model_early_stopping.ckpt\"\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 200\n",
    "n_batches = int(np.ceil(mnist.train.num_examples // batch_size))\n",
    "\n",
    "early_stopping_check_frequency = batch_size // 4\n",
    "early_stopping_check_limit = batch_size * 2\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init)\n",
    "    #saver.restore(sess, interim_checkpoint_path)\n",
    "    \n",
    "    best_validation_acc = 0.0\n",
    "    best_validation_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch\", epoch)\n",
    "        for batch_index in range(n_batches):\n",
    "            step = epoch * n_batches + batch_index\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = summary_op.eval(feed_dict={x: X_batch, y: y_batch})\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            t, l, a = sess.run([training_op, loss, accuracy], feed_dict={x: X_batch, y: y_batch})\n",
    "            if batch_index % 10 == 0: print(\"loss:\", l, \"test accuracy:\", a)\n",
    "            # Early stopping check\n",
    "            if batch_index % early_stopping_check_frequency == 0:\n",
    "                validation_acc = accuracy.eval(feed_dict={x: mnist.validation.images, y: mnist.validation.labels})\n",
    "                print(\"validation accuracy\", validation_acc)\n",
    "                if validation_acc > best_validation_acc:\n",
    "                    saver.save(sess, early_stopping_checkpoint_path)\n",
    "                    best_validation_acc = validation_acc\n",
    "                    best_validation_step = step\n",
    "                elif step >= (best_validation_step + early_stopping_check_limit):\n",
    "                    print(\"Stopping early during epoch\", epoch)\n",
    "                    break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    save_path = saver.save(sess, interim_checkpoint_path)\n",
    "    test_acc = accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "    print(\">>>>>>>>>> test dataset accuracy:\", test_acc)\n",
    "\n",
    "    save_path = saver.save(sess, \"./checkpoints/mnist_model_final.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
