{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.  28.]\n",
      " [ 49.  64.]]\n"
     ]
    }
   ],
   "source": [
    "#  Create a test graph\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_pl\n",
    "                                        acement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))30째21째\tSun\n",
    "29째22째\tMon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first train the model to identify digits 0 to 4 and then do transfer training to identify images 5 to 9.\n",
    "\n",
    "def divide_dataset_into_two_sets(images, labels):\n",
    "    assert len(images) == len(labels)\n",
    "    zero_to_four_images = []\n",
    "    zero_to_four_labels = []\n",
    "    five_to_nine_images = []\n",
    "    five_to_nine_labels = []\n",
    "    four_label = 4\n",
    "    for i in range(len(images)):\n",
    "        if labels[i] <= four_label:\n",
    "            zero_to_four_images.append(images[i])\n",
    "            zero_to_four_labels.append(labels[i])\n",
    "        else:\n",
    "            five_to_nine_images.append(images[i])\n",
    "            five_to_nine_labels.append(labels[i])\n",
    "    return np.array(zero_to_four_images), np.array(zero_to_four_labels), np.array(five_to_nine_images), np.array(five_to_nine_labels)\n",
    "\n",
    "train_zero_to_four_images, train_zero_to_four_labels, train_five_to_nine_images, train_five_to_nine_labels = divide_dataset_into_two_sets(mnist.train.images, mnist.train.labels)\n",
    "validation_zero_to_four_images, validation_zero_to_four_labels, validation_five_to_nine_images, validation_five_to_nine_labels = divide_dataset_into_two_sets(mnist.validation.images, mnist.validation.labels)\n",
    "test_zero_to_four_images, test_zero_to_four_labels, test_five_to_nine_images, test_five_to_nine_labels = divide_dataset_into_two_sets(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5139, 784)\n",
      "(5139,)\n",
      "(4861, 784)\n",
      "(4861,)\n"
     ]
    }
   ],
   "source": [
    "print(test_zero_to_four_images.shape)\n",
    "print(test_zero_to_four_labels.shape)\n",
    "print(test_five_to_nine_images.shape)\n",
    "print(test_five_to_nine_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs = 28 * 28\n",
    "n_hidden_per_layer = [100, 100, 100, 100, 100]\n",
    "n_output = 5\n",
    "\n",
    "def he_normal_initialisation(n_inputs, n_outputs):\n",
    "    stddev = np.power(2 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.truncated_normal((n_inputs, n_outputs), stddev=stddev)\n",
    "\n",
    "def he_uniform_initialisation(n_inputs, n_outputs):\n",
    "    r = np.power(6 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.random_uniform((n_inputs, n_outputs), -r, r)\n",
    "\n",
    "def neuron_layer(X, n_neurons, name):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        W = tf.Variable(he_normal_initialisation(n_inputs, n_neurons), name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        return tf.nn.elu(z)\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    x = tf.placeholder(tf.float32, shape=(None, number_of_inputs), name=\"input\")\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        input_tensor = x\n",
    "        for i in range(len(n_hidden_per_layer)):\n",
    "            input_tensor = neuron_layer(input_tensor, n_hidden_per_layer[i], \"hidden\" + str(i + 1))\n",
    "        logits = neuron_layer(input_tensor, n_output, \"output\")\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "\n",
    "    with tf.name_scope(\"training\"):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    k = 1\n",
    "    correctness = tf.nn.in_top_k(logits, y, k)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctness, tf.float32)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "interim_checkpoint_path = \"./checkpoints/mnist_model.ckpt\"\n",
    "early_stopping_checkpoint_path = \"./checkpoints/mnist_model_early_stopping.ckpt\"\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "        \n",
    "def create_next_batch_fn(images, labels, batch_size):\n",
    "    assert len(images) == len(labels)\n",
    "    current_batch = 0\n",
    "    def next_batch():\n",
    "        nonlocal current_batch\n",
    "        i = current_batch\n",
    "        #print(current_batch)\n",
    "        current_batch = (current_batch + batch_size) % len(images)\n",
    "        return images[i:i+batch_size], labels[i:i+batch_size]\n",
    "    return next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_test_batch = create_next_batch_fn(test_zero_to_four_images, test_zero_to_four_labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 4 1 4 0 0 1 3 4 4 0 4 0 1 3 1 3 4 2 1 2 1 1 4 2 3 1 2 4 4 3 0 4 1 3\n",
      " 4 4 3 0 0 2 1 3 2 2 4 3 1 3 3 1 4 1 0 4 2 1 4 3 4 4 4 2 4 0 1 0 1 4 3 1 1\n",
      " 2 0 2 1 0 3 4 4 4 4 1 4 4 2 3 2 1 1 1 0 2 0 1 1 1 0 0 3 1 4 2 3 1 1 1 3 2\n",
      " 4 3 0 3 2 2 1 2 4 1 3 3 2 2 4 1 2 3 0 4 4 2 4 1 2 2 1 1 0 3 0 1 4 1 2 1 2\n",
      " 2 4 1 2 2 0 4 0 0 2 4 1 2 4 0 2 4 3 3 0 0 3 1 2 2 3 0 4 2 0 1 1 2 1 3 3 1\n",
      " 3 1 0 1 3 1 1 1 4 2 2 0 3 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(next_test_batch()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "loss: 1.60959 training accuracy: 21.5\n",
      "validation accuracy 39.6403\n",
      "loss: 1.37695 training accuracy: 64.5\n",
      "loss: 0.787918 training accuracy: 71.5\n",
      "loss: 0.532191 training accuracy: 82.0\n",
      "loss: 0.173437 training accuracy: 95.0\n",
      "loss: 0.109017 training accuracy: 96.5\n",
      "validation accuracy 93.6278\n",
      "loss: 0.112182 training accuracy: 96.5\n",
      "loss: 0.110039 training accuracy: 96.5\n",
      "loss: 0.0441707 training accuracy: 98.5\n",
      "loss: 0.1036 training accuracy: 97.5\n",
      "loss: 0.0552331 training accuracy: 98.5\n",
      "validation accuracy 95.5825\n",
      "loss: 0.0486561 training accuracy: 99.0\n",
      "loss: 0.0987659 training accuracy: 97.5\n",
      "loss: 0.0111191 training accuracy: 99.5\n",
      "loss: 0.0429868 training accuracy: 97.5\n",
      "loss: 0.0456751 training accuracy: 96.5\n",
      "validation accuracy 95.6998\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "n_batches = int(np.ceil(mnist.train.num_examples // batch_size))\n",
    "\n",
    "early_stopping_check_frequency = batch_size // 4\n",
    "early_stopping_check_limit = batch_size * 2\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init)\n",
    "    #saver.restore(sess, interim_checkpoint_path)\n",
    "    \n",
    "    best_validation_acc = 0.0\n",
    "    best_validation_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch\", epoch)\n",
    "        for batch_index in range(n_batches):\n",
    "            step = epoch * n_batches + batch_index\n",
    "            X_batch, y_batch = next_test_batch()\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = summary_op.eval(feed_dict={x: X_batch, y: y_batch})\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            t, l, a = sess.run([training_op, loss, accuracy], feed_dict={x: X_batch, y: y_batch})\n",
    "            if batch_index % 10 == 0: print(\"loss:\", l, \"training accuracy:\", a)\n",
    "            # Early stopping check\n",
    "            if batch_index % early_stopping_check_frequency == 0:\n",
    "                validation_acc = accuracy.eval(feed_dict={x: validation_zero_to_four_images, y: validation_zero_to_four_labels})\n",
    "                print(\"validation accuracy\", validation_acc)\n",
    "                if validation_acc > best_validation_acc:\n",
    "                    saver.save(sess, early_stopping_checkpoint_path)\n",
    "                    best_validation_acc = validation_acc\n",
    "                    best_validation_step = step\n",
    "                elif step >= (best_validation_step + early_stopping_check_limit):\n",
    "                    print(\"Stopping early during epoch\", epoch)\n",
    "                    break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    save_path = saver.save(sess, interim_checkpoint_path)\n",
    "    test_acc = accuracy.eval(feed_dict={x: test_zero_to_four_images, y: test_zero_to_four_labels})\n",
    "    print(\">>>>>>>>>> test dataset accuracy:\", test_acc)\n",
    "\n",
    "    save_path = saver.save(sess, \"./checkpoints/mnist_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '_asdict', '_fields', '_make', '_replace', '_source', 'count', 'index', 'test', 'train', 'validation']\n",
      "55000\n",
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(dir(mnist))\n",
    "print(mnist.train.num_examples)\n",
    "print(mnist.validation.num_examples)\n",
    "print(mnist.test.num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 9\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs = 28 * 28\n",
    "n_hidden_per_layer = [100, 100, 100, 100, 100]\n",
    "n_output = 10\n",
    "\n",
    "def he_normal_initialisation(n_inputs, n_outputs):\n",
    "    stddev = np.power(2 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.truncated_normal((n_inputs, n_outputs), stddev=stddev)\n",
    "\n",
    "def he_uniform_initialisation(n_inputs, n_outputs):\n",
    "    r = np.power(6 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.random_uniform((n_inputs, n_outputs), -r, r)\n",
    "\n",
    "def neuron_layer(X, n_neurons, name):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        W = tf.Variable(he_normal_initialisation(n_inputs, n_neurons), name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        return tf.nn.elu(z)\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    x = tf.placeholder(tf.float32, shape=(None, number_of_inputs), name=\"input\")\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        input_tensor = x\n",
    "        for i in range(len(n_hidden_per_layer)):\n",
    "            input_tensor = neuron_layer(input_tensor, n_hidden_per_layer[i], \"hidden\" + str(i + 1))\n",
    "        logits = neuron_layer(input_tensor, n_output, \"output\")\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "\n",
    "    with tf.name_scope(\"training\"):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    k = 1\n",
    "    correctness = tf.nn.in_top_k(logits, y, k)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctness, tf.float32)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "interim_checkpoint_path = \"./checkpoints/mnist_model.ckpt\"\n",
    "early_stopping_checkpoint_path = \"./checkpoints/mnist_model_early_stopping.ckpt\"\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 200\n",
    "n_batches = int(np.ceil(mnist.train.num_examples // batch_size))\n",
    "\n",
    "early_stopping_check_frequency = batch_size // 4\n",
    "early_stopping_check_limit = batch_size * 2\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init)\n",
    "    #saver.restore(sess, interim_checkpoint_path)\n",
    "    \n",
    "    best_validation_acc = 0.0\n",
    "    best_validation_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch\", epoch)\n",
    "        for batch_index in range(n_batches):\n",
    "            step = epoch * n_batches + batch_index\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = summary_op.eval(feed_dict={x: X_batch, y: y_batch})\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            t, l, a = sess.run([training_op, loss, accuracy], feed_dict={x: X_batch, y: y_batch})\n",
    "            if batch_index % 10 == 0: print(\"loss:\", l, \"test accuracy:\", a)\n",
    "            # Early stopping check\n",
    "            if batch_index % early_stopping_check_frequency == 0:\n",
    "                validation_acc = accuracy.eval(feed_dict={x: mnist.validation.images, y: mnist.validation.labels})\n",
    "                print(\"validation accuracy\", validation_acc)\n",
    "                if validation_acc > best_validation_acc:\n",
    "                    saver.save(sess, early_stopping_checkpoint_path)\n",
    "                    best_validation_acc = validation_acc\n",
    "                    best_validation_step = step\n",
    "                elif step >= (best_validation_step + early_stopping_check_limit):\n",
    "                    print(\"Stopping early during epoch\", epoch)\n",
    "                    break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    save_path = saver.save(sess, interim_checkpoint_path)\n",
    "    test_acc = accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "    print(\">>>>>>>>>> test dataset accuracy:\", test_acc)\n",
    "\n",
    "    save_path = saver.save(sess, \"./checkpoints/mnist_model_final.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
