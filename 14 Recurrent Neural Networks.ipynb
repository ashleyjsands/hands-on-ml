{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 7\n",
    "===\n",
    "Create a RNN that can learn a Reber grammer (http://www.willamette.edu/~gorr/classes/cs449/reber.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded reber btbptttttvpxtttttvpsete\n",
      "is valid embedded reber valid? True\n",
      "is invalid embedded reber valid? False\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "\n",
    "start_symbol = \"b\"\n",
    "end_symbol = \"e\"\n",
    "\n",
    "symbols = [start_symbol, \"t\", \"p\", \"s\", \"x\", \"v\", end_symbol]\n",
    "\n",
    "def get_next_symbols(previous_symbol, current_symbol):\n",
    "    if current_symbol == None and previous_symbol == None:\n",
    "        return [start_symbol]\n",
    "    elif current_symbol == start_symbol:\n",
    "        return [\"t\", \"p\"]\n",
    "    elif current_symbol == \"t\":\n",
    "        if previous_symbol == start_symbol:\n",
    "            return [\"s\", \"x\"]\n",
    "        elif previous_symbol in [\"p\", \"x\", \"t\"]:\n",
    "            return [\"t\", \"v\"]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == \"p\":\n",
    "        if previous_symbol == start_symbol:\n",
    "            return [\"t\", \"v\"]\n",
    "        elif previous_symbol == \"v\":\n",
    "            return [\"x\", \"s\"]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == \"s\":\n",
    "        if previous_symbol in [\"t\", \"s\"]:\n",
    "            return [\"x\", \"s\"]\n",
    "        elif previous_symbol in [\"x\", \"p\"]:\n",
    "            return [end_symbol]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == \"x\":\n",
    "        if previous_symbol in [\"t\", \"s\"]:\n",
    "            return [\"x\", \"s\"]\n",
    "        elif previous_symbol in [\"x\", \"p\"]:\n",
    "            return [\"t\", \"v\"]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == \"v\":\n",
    "        if previous_symbol in [\"t\", \"x\", \"p\"]:\n",
    "            return [\"p\", \"v\"]\n",
    "        elif previous_symbol == \"v\":\n",
    "            return [ end_symbol ]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == end_symbol:\n",
    "        return []\n",
    "    else:\n",
    "        raise Exception(\"Invalid symbols: %s and %s.\" % (previous_symbol, current_symbol))\n",
    "\n",
    "def get_next_symbols_for_string(reber_str):\n",
    "    previous_symbol = reber_str[-2] if len(reber_str) >= 2 else None\n",
    "    current_symbol = reber_str[-1] if len(reber_str) >= 1 else None\n",
    "    return get_next_symbols(previous_symbol, current_symbol)\n",
    "\n",
    "def create_reber_string():\n",
    "    reber_str = \"\"\n",
    "    while not reber_str.endswith(end_symbol):\n",
    "        reber_str += random.choice(get_next_symbols_for_string(reber_str))\n",
    "    return reber_str\n",
    "\n",
    "def is_valid_reber_string(value):\n",
    "    if value == \"\" or value == None:\n",
    "        return False\n",
    "    index = 0\n",
    "    while index < len(value):\n",
    "        current = value[index]\n",
    "        next_symbols = get_next_symbols_for_string(value[:index] if index != 0 else \"\")\n",
    "        if current not in next_symbols:\n",
    "            return False\n",
    "        index += 1\n",
    "    return value[len(value) - 1] == end_symbol\n",
    "\n",
    "valid_embedded_reber_second_symbols = [\"t\", \"p\"]\n",
    "\n",
    "def create_embedded_reber_string():\n",
    "    second_symbol = random.choice(valid_embedded_reber_second_symbols)\n",
    "    return start_symbol + second_symbol + create_reber_string() + second_symbol + end_symbol\n",
    "\n",
    "def is_valid_embedded_reber_string(value):\n",
    "    second_symbol_index = 1\n",
    "    second_symbol = value[second_symbol_index]\n",
    "    second_last_symbol_index = -2\n",
    "    second_last_symbol = value[second_last_symbol_index]\n",
    "    valid_second_symbols = second_symbol in valid_embedded_reber_second_symbols and second_last_symbol in valid_embedded_reber_second_symbols \\\n",
    "        and second_symbol == second_last_symbol\n",
    "    embedded_reber_suffix_size = 2\n",
    "    reber_string = value[second_symbol_index + 1:len(value) - embedded_reber_suffix_size]\n",
    "    return value[0] == start_symbol and valid_second_symbols and is_valid_reber_string(reber_string) and value[-1] == end_symbol\n",
    "\n",
    "print(\"embedded reber\", create_embedded_reber_string())\n",
    "print(\"is valid embedded reber valid?\", is_valid_embedded_reber_string(create_embedded_reber_string()))\n",
    "print(\"is invalid embedded reber valid?\", is_valid_embedded_reber_string(\"btbptttttvpxtttttvpsepe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 10000 validation size 2000 test size 2000\n",
      "all_strings.shape (14000, 51, 7)\n",
      "Example sequence: [[1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]] True\n",
      "The max sequence length is 51\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "def create_reber_strings(size):\n",
    "    strings = []\n",
    "    while len(strings) < size:\n",
    "        new_string = create_embedded_reber_string()\n",
    "        if new_string not in strings:\n",
    "            strings.append(new_string)\n",
    "    return strings\n",
    "\n",
    "def invalidate_reber_string(value):\n",
    "    while is_valid_embedded_reber_string(value):\n",
    "        char_list = list(value)\n",
    "        char_list[random.randint(0, len(value) - 1)] = random.choice(symbols)\n",
    "        value = \"\".join(char_list)\n",
    "    return value\n",
    "\n",
    "def invalidate_reber_strings(strings):\n",
    "    return [invalidate_reber_string(x) for x in strings]\n",
    "\n",
    "def symbol_to_one_hot_encoding(symbol):\n",
    "    one_hot_encoding = np.zeros((len(symbols)), dtype=np.int8)\n",
    "    if symbol != '':\n",
    "        one_hot_encoding[symbols.index(symbol)] = 1\n",
    "    return one_hot_encoding\n",
    "\n",
    "def prep_strings_for_model(strings, sequence_length):\n",
    "    padded_strings = np.array([np.pad(list(x), (0, (sequence_length - len(x)) % sequence_length), 'constant') for x in strings])\n",
    "    sequences = np.array([np.array(list(map(symbol_to_one_hot_encoding, x))) for x in padded_strings])\n",
    "    return sequences\n",
    "\n",
    "def generate_dataset(size, error_ratio = 0.5):\n",
    "    if size % 2 != 0:\n",
    "        raise Exception(\"size must be a multiple of 2.\")\n",
    "    correct_strings = create_reber_strings(int(size * error_ratio))\n",
    "    incorrect_strings = invalidate_reber_strings(correct_strings)\n",
    "    correct_val = True\n",
    "    incorrect_val = False\n",
    "    targets = np.array(([ correct_val ] * len(correct_strings)) + ([ incorrect_val ] * len(incorrect_strings)))\n",
    "    indices = np.random.permutation(size)\n",
    "    strings = correct_strings + incorrect_strings\n",
    "    sequence_lengths = np.array([len(x) for x in strings])\n",
    "    max_sequence_length = max(sequence_lengths)\n",
    "    strings = prep_strings_for_model(strings, max_sequence_length)\n",
    "    #print(strings[0])\n",
    "    return strings[indices], sequence_lengths[indices], targets[indices]\n",
    "\n",
    "train_size = 10000\n",
    "validation_size = int(train_size * 0.2)\n",
    "test_size = int(train_size * 0.2)\n",
    "\n",
    "all_strings, sequence_lengths, all_targets = generate_dataset(train_size + validation_size + test_size)\n",
    "train_X = all_strings[:train_size]\n",
    "train_seq_lengths = sequence_lengths[:train_size]\n",
    "train_y = all_targets[:train_size]\n",
    "validation_X = all_strings[train_size:train_size+validation_size]\n",
    "validation_seq_lengths = sequence_lengths[train_size:train_size+validation_size]\n",
    "validation_y = all_targets[train_size:train_size+validation_size]\n",
    "test_X = all_strings[train_size+validation_size:train_size+validation_size+test_size]\n",
    "test_seq_lengths = sequence_lengths[train_size+validation_size:train_size+validation_size+test_size]\n",
    "test_y = all_targets[train_size+validation_size:train_size+validation_size+test_size]\n",
    "\n",
    "#print(list(map(symbol_to_one_hot_encoding, create_reber_string())))\n",
    "print(\"train size\", len(train_X), \"validation size\", len(validation_X), \"test size\", len(test_X))\n",
    "print(\"all_strings.shape\", all_strings.shape)\n",
    "#print(\"string shapes\", list(map(lambda a: a.shape, all_strings)))\n",
    "print(\"Example sequence:\", all_strings[1], all_targets[1])\n",
    "#print(\"sequence_lengths\", sequence_lengths)\n",
    "\n",
    "max_sequence_length = train_X[0].shape[0]\n",
    "print(\"The max sequence length is\", max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "def he_normal_initialisation(n_inputs, n_outputs):\n",
    "    stddev = np.power(2 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.truncated_normal((n_inputs, n_outputs), stddev=stddev)\n",
    "\n",
    "def he_uniform_initialisation(n_inputs, n_outputs):\n",
    "    r = np.power(6 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.random_uniform((n_inputs, n_outputs), -r, r)\n",
    "\n",
    "def create_next_batch_fn(data, sequence_lengths, targets, batch_size):\n",
    "    assert len(data) == len(sequence_lengths) and len(data) == len(targets)\n",
    "    current_batch = 0\n",
    "    def next_batch():\n",
    "        nonlocal current_batch\n",
    "        i = current_batch\n",
    "        #print(current_batch)\n",
    "        current_batch = (current_batch + batch_size) % len(data)\n",
    "        return data[i:i+batch_size], sequence_lengths[i:i+batch_size], targets[i:i+batch_size]\n",
    "    return next_batch\n",
    "\n",
    "class RnnClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_steps, learning_rate=0.001, n_neurons=2):\n",
    "        self.n_steps = n_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_neurons = n_neurons\n",
    "        self._build_graph()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        n_inputs = 1\n",
    "        self.n_output = 1\n",
    "        self.batch_size = 60\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, shape=(None, self.n_steps, len(symbols)), name=\"input\")\n",
    "        self.sequence_length = tf.placeholder(tf.int32, shape=(None), name=\"sequence_length\")\n",
    "        self.y = tf.placeholder(tf.bool, shape=(None), name=\"y\")\n",
    "\n",
    "        with tf.name_scope(\"rnn\"):\n",
    "            #cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "            #    tf.contrib.rnn.BasicRNNCell(num_units=self.n_neurons, activation=tf.nn.relu),\n",
    "            #    output_size=self.n_output)\n",
    "            self.rnn_activation_midpoint = 0.0\n",
    "            cell = tf.contrib.rnn.GRUCell(num_units=self.n_neurons)\n",
    "            outputs, last_outputs = tf.nn.dynamic_rnn(cell, self.x, dtype=tf.float32, sequence_length=self.sequence_length)\n",
    "            #print(\"last_outputs.shape\", last_outputs.shape)\n",
    "            with tf.name_scope(\"fc\"):\n",
    "                W = tf.Variable(tf.truncated_normal((self.n_neurons, self.n_output), stddev=1.1), name=\"weights\")\n",
    "                b = tf.Variable(tf.zeros([self.n_output]), name=\"biases\")\n",
    "                self.logits = tf.matmul(last_outputs, W) + b\n",
    "                self.y_proba = tf.nn.sigmoid(self.logits)\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            y_float = tf.reshape(tf.cast(self.y, tf.float32), (-1, 1))\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_float, logits=self.logits))\n",
    "\n",
    "        with tf.name_scope(\"training\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.training_op = optimizer.minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            self.y_pred = self.logits > self.rnn_activation_midpoint\n",
    "            correctness = tf.equal(self.y_pred, self.y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correctness, tf.float32)) * 100.0\n",
    "            \n",
    "        self.init = tf.global_variables_initializer()\n",
    "\n",
    "    def fit(self, X, sequence_lengths, y, valid_X, valid_sequence_length, valid_y, epochs = 50):\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        interim_checkpoint_path = \"./checkpoints/reber_rnn_model.ckpt\"\n",
    "        early_stopping_checkpoint_path = \"./checkpoints/reber_rnn_model_early_stopping.ckpt\"\n",
    "\n",
    "        from datetime import datetime\n",
    "\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        root_logdir = \"tf_logs\"\n",
    "        log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "        loss_summary = tf.summary.scalar('loss', self.loss)\n",
    "        accuracy_summary = tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "        summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "        file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "        \n",
    "        n_batches = int(np.ceil(len(X) // self.batch_size))\n",
    "        next_batch = create_next_batch_fn(X, sequence_lengths, y, self.batch_size)\n",
    "            \n",
    "        early_stopping_check_frequency = n_batches // 16\n",
    "        early_stopping_check_limit = n_batches * 2\n",
    "\n",
    "        sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        self.session = sess\n",
    "        sess.run(self.init)\n",
    "        #saver.restore(sess, interim_checkpoint_path)\n",
    "\n",
    "        best_validation_acc = 0.0\n",
    "        best_validation_step = 0\n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch\", epoch)\n",
    "            for batch_index in range(n_batches):\n",
    "                step = epoch * n_batches + batch_index\n",
    "                X_batch, seq_length_batch, y_batch = next_batch()\n",
    "                #print(\"training batch\", X_batch.shape, seq_length_batch.shape, y_batch.shape)\n",
    "                #print(seq_length_batch)\n",
    "                if batch_index % 10 == 0:\n",
    "                    summary_str = summary_op.eval(session=sess, feed_dict={self.x: X_batch, self.sequence_length: seq_length_batch, self.y: y_batch})\n",
    "                    file_writer.add_summary(summary_str, step)\n",
    "                t, l, a = sess.run([self.training_op, self.loss, self.accuracy], feed_dict={self.x: X_batch, self.sequence_length: seq_length_batch, self.y: y_batch})\n",
    "                #a = self.prediction_accuracy(X_batch, seq_length_batch, y_batch)\n",
    "                #print(\"y_proba\", proba)\n",
    "                if batch_index % 10 == 0: print(\"loss:\", l, \"train accuracy:\", a)\n",
    "                # Early stopping check\n",
    "                if batch_index % early_stopping_check_frequency == 0:\n",
    "                    validation_acc = self.prediction_accuracy(valid_X, valid_sequence_length, valid_y)\n",
    "                    print(\"validation accuracy\", validation_acc)\n",
    "                    if validation_acc > best_validation_acc:\n",
    "                        saver.save(sess, early_stopping_checkpoint_path)\n",
    "                        best_validation_acc = validation_acc\n",
    "                        best_validation_step = step\n",
    "                    elif step >= (best_validation_step + early_stopping_check_limit):\n",
    "                        print(\"Stopping early during epoch\", epoch, \"with best validation accuracy\", best_validation_acc)\n",
    "                        break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            save_path = saver.save(sess, interim_checkpoint_path)\n",
    "        saver.restore(sess, early_stopping_checkpoint_path)\n",
    "        save_path = saver.save(sess, \"./checkpoints/reber_rnn_model_final.ckpt\")\n",
    "            \n",
    "    def predict_proba(self, X, sequence_lengths):\n",
    "        dataset_size = X.shape[0]\n",
    "        #print \"dataset_size: \", dataset_size, \" batch_size: \", batch_size\n",
    "        predictions = np.ndarray(shape=(dataset_size, self.n_output), dtype=np.float32)\n",
    "        steps = int(math.ceil(dataset_size / self.batch_size))\n",
    "        #print \"steps: \", steps\n",
    "        for step in range(steps):\n",
    "            offset = (step * self.batch_size)\n",
    "            #print \"offset \", offset\n",
    "            data_end_index = min(offset + self.batch_size, dataset_size)\n",
    "            batch_data = X[offset:data_end_index, :]\n",
    "            feed_dict = {\n",
    "                self.x: batch_data,\n",
    "                self.sequence_length: sequence_lengths[offset:data_end_index]\n",
    "            }\n",
    "            predictions[offset:data_end_index, :] = self.y_proba.eval(session=self.session, feed_dict=feed_dict)\n",
    "        #print(\"predict_proba\", predictions)\n",
    "        return predictions\n",
    "\n",
    "    def predict(self, X, sequence_lengths):\n",
    "        return np.argmax(self.predict_proba(X, sequence_lengths), axis=1)\n",
    "    \n",
    "    def _prediction_accuracy(self, predictions, y):\n",
    "        probability_midpoint = 0.5\n",
    "        return (np.sum(((predictions.reshape((-1)) > probability_midpoint) == y).astype(float))\n",
    "              / predictions.shape[0]) * 100\n",
    "    \n",
    "    def prediction_accuracy(self, X, sequence_lengths, y):\n",
    "        predictions = self.predict_proba(X, sequence_lengths)\n",
    "        #print(\"prediction_accuracy predictions\", predictions)\n",
    "        return self._prediction_accuracy(predictions, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "loss: 0.697612 train accuracy: 50.3889\n",
      "validation accuracy 50.9\n",
      "loss: 0.759287 train accuracy: 33.5\n",
      "validation accuracy 51.75\n",
      "loss: 0.672938 train accuracy: 64.0\n",
      "validation accuracy 50.5\n",
      "loss: 0.686846 train accuracy: 50.4444\n",
      "validation accuracy 55.8\n",
      "loss: 0.651556 train accuracy: 63.5\n",
      "validation accuracy 52.5\n",
      "loss: 0.689273 train accuracy: 50.6667\n",
      "validation accuracy 53.25\n",
      "loss: 0.661199 train accuracy: 50.0\n",
      "validation accuracy 53.1\n",
      "loss: 0.692908 train accuracy: 48.3333\n",
      "validation accuracy 51.65\n",
      "loss: 0.682001 train accuracy: 50.0\n",
      "validation accuracy 52.5\n",
      "loss: 0.664027 train accuracy: 57.2222\n",
      "validation accuracy 53.65\n",
      "loss: 0.659903 train accuracy: 50.2222\n",
      "validation accuracy 56.5\n",
      "loss: 0.680538 train accuracy: 42.5\n",
      "validation accuracy 53.0\n",
      "loss: 0.666979 train accuracy: 50.0\n",
      "validation accuracy 55.75\n",
      "loss: 0.689129 train accuracy: 48.6111\n",
      "validation accuracy 54.7\n",
      "loss: 0.63148 train accuracy: 52.5\n",
      "validation accuracy 55.75\n",
      "loss: 0.676705 train accuracy: 49.5556\n",
      "validation accuracy 58.95\n",
      "loss: 0.688532 train accuracy: 47.1111\n",
      "validation accuracy 60.4\n",
      "epoch 1\n",
      "loss: 0.631746 train accuracy: 48.0\n",
      "validation accuracy 54.0\n",
      "loss: 0.698082 train accuracy: 47.7778\n",
      "validation accuracy 58.05\n",
      "loss: 0.648695 train accuracy: 48.6667\n",
      "validation accuracy 56.65\n",
      "loss: 0.679657 train accuracy: 42.3333\n",
      "validation accuracy 61.15\n",
      "loss: 0.624449 train accuracy: 53.5\n",
      "validation accuracy 61.25\n",
      "loss: 0.716971 train accuracy: 45.8333\n",
      "validation accuracy 57.35\n",
      "loss: 0.587014 train accuracy: 54.6667\n",
      "validation accuracy 59.15\n",
      "loss: 0.617702 train accuracy: 57.1111\n",
      "validation accuracy 61.6\n",
      "loss: 0.648643 train accuracy: 47.4444\n",
      "validation accuracy 62.45\n",
      "loss: 0.584021 train accuracy: 52.2222\n",
      "validation accuracy 65.7\n",
      "loss: 0.546295 train accuracy: 47.1111\n",
      "validation accuracy 73.6\n",
      "loss: 0.516443 train accuracy: 55.2778\n",
      "validation accuracy 65.3\n",
      "loss: 0.622369 train accuracy: 49.7778\n",
      "validation accuracy 66.65\n",
      "loss: 0.571686 train accuracy: 49.4444\n",
      "validation accuracy 58.05\n",
      "loss: 0.578117 train accuracy: 48.2222\n",
      "validation accuracy 70.05\n",
      "loss: 0.523006 train accuracy: 50.0\n",
      "validation accuracy 75.05\n",
      "loss: 0.695381 train accuracy: 51.1667\n",
      "validation accuracy 71.85\n",
      "epoch 2\n",
      "loss: 0.463214 train accuracy: 49.1111\n",
      "validation accuracy 79.8\n",
      "loss: 1.23167 train accuracy: 49.1667\n",
      "validation accuracy 74.75\n",
      "loss: 0.547837 train accuracy: 51.6667\n",
      "validation accuracy 70.15\n",
      "loss: 0.594707 train accuracy: 51.4444\n",
      "validation accuracy 75.0\n",
      "loss: 0.515748 train accuracy: 50.8333\n",
      "validation accuracy 79.55\n",
      "loss: 0.580914 train accuracy: 46.3333\n",
      "validation accuracy 79.2\n",
      "loss: 0.38674 train accuracy: 51.2222\n",
      "validation accuracy 80.9\n",
      "loss: 0.437946 train accuracy: 49.5\n",
      "validation accuracy 84.75\n",
      "loss: 0.355411 train accuracy: 50.0\n",
      "validation accuracy 87.35\n",
      "loss: 0.302649 train accuracy: 49.7222\n",
      "validation accuracy 77.55\n",
      "loss: 0.425121 train accuracy: 51.3333\n",
      "validation accuracy 76.0\n",
      "loss: 0.50499 train accuracy: 52.4444\n",
      "validation accuracy 79.75\n",
      "loss: 0.504536 train accuracy: 47.6667\n",
      "validation accuracy 80.3\n",
      "loss: 0.369932 train accuracy: 50.5556\n",
      "validation accuracy 86.9\n",
      "loss: 0.29748 train accuracy: 50.3333\n",
      "validation accuracy 90.05\n",
      "loss: 0.294587 train accuracy: 50.0\n",
      "validation accuracy 91.9\n",
      "loss: 0.221458 train accuracy: 53.0\n",
      "validation accuracy 92.6\n",
      "epoch 3\n",
      "loss: 0.327331 train accuracy: 49.9444\n",
      "validation accuracy 93.1\n",
      "loss: 0.2065 train accuracy: 50.0\n",
      "validation accuracy 94.2\n",
      "loss: 0.160763 train accuracy: 50.5556\n",
      "validation accuracy 93.5\n",
      "loss: 0.160121 train accuracy: 50.0\n",
      "validation accuracy 94.15\n",
      "loss: 0.179125 train accuracy: 50.7778\n",
      "validation accuracy 95.4\n",
      "loss: 0.143064 train accuracy: 50.8333\n",
      "validation accuracy 96.55\n",
      "loss: 0.0709303 train accuracy: 50.3333\n",
      "validation accuracy 96.3\n",
      "loss: 0.0804503 train accuracy: 50.4444\n",
      "validation accuracy 96.95\n",
      "loss: 0.0512869 train accuracy: 49.9444\n",
      "validation accuracy 98.1\n",
      "loss: 0.0984609 train accuracy: 50.1111\n",
      "validation accuracy 98.4\n",
      "loss: 0.0253939 train accuracy: 50.0\n",
      "validation accuracy 97.65\n",
      "loss: 0.013731 train accuracy: 50.0556\n",
      "validation accuracy 97.7\n",
      "loss: 0.0792355 train accuracy: 50.6667\n",
      "validation accuracy 98.1\n",
      "loss: 0.097167 train accuracy: 50.0\n",
      "validation accuracy 98.2\n",
      "loss: 0.114388 train accuracy: 50.6667\n",
      "validation accuracy 96.35\n",
      "loss: 0.229399 train accuracy: 50.5556\n",
      "validation accuracy 94.55\n",
      "loss: 0.0827889 train accuracy: 50.6667\n",
      "validation accuracy 98.05\n",
      "epoch 4\n",
      "loss: 0.0756934 train accuracy: 50.0\n",
      "validation accuracy 98.25\n",
      "loss: 0.0783956 train accuracy: 50.6667\n",
      "validation accuracy 98.0\n",
      "loss: 0.133265 train accuracy: 49.9444\n",
      "validation accuracy 98.45\n",
      "loss: 0.0916615 train accuracy: 51.1111\n",
      "validation accuracy 97.35\n",
      "loss: 0.0689818 train accuracy: 50.0\n",
      "validation accuracy 97.45\n",
      "loss: 0.0390788 train accuracy: 55.5556\n",
      "validation accuracy 98.3\n",
      "loss: 0.0650229 train accuracy: 50.1111\n",
      "validation accuracy 98.15\n",
      "loss: 0.16545 train accuracy: 50.4444\n",
      "validation accuracy 98.95\n",
      "loss: 0.0241929 train accuracy: 50.0556\n",
      "validation accuracy 98.65\n",
      "loss: 0.0667104 train accuracy: 50.0\n",
      "validation accuracy 98.75\n",
      "loss: 0.139938 train accuracy: 50.0\n",
      "validation accuracy 98.8\n",
      "loss: 0.00906959 train accuracy: 51.3889\n",
      "validation accuracy 99.1\n",
      "loss: 0.121948 train accuracy: 51.3333\n",
      "validation accuracy 99.3\n",
      "loss: 0.164936 train accuracy: 50.1667\n",
      "validation accuracy 98.35\n",
      "loss: 0.0179192 train accuracy: 50.2222\n",
      "validation accuracy 99.2\n",
      "loss: 0.0713931 train accuracy: 50.1111\n",
      "validation accuracy 99.35\n",
      "loss: 0.0947057 train accuracy: 52.3333\n",
      "validation accuracy 99.3\n",
      "epoch 5\n",
      "loss: 0.0089912 train accuracy: 50.8889\n",
      "validation accuracy 99.25\n",
      "loss: 0.0176968 train accuracy: 50.5\n",
      "validation accuracy 99.15\n",
      "loss: 0.0176539 train accuracy: 50.0556\n",
      "validation accuracy 99.35\n",
      "loss: 0.00876068 train accuracy: 50.0556\n",
      "validation accuracy 99.35\n",
      "loss: 0.00799125 train accuracy: 52.0\n",
      "validation accuracy 99.35\n",
      "loss: 0.0124787 train accuracy: 51.3889\n",
      "validation accuracy 99.2\n",
      "loss: 0.0138084 train accuracy: 50.2222\n",
      "validation accuracy 99.2\n",
      "loss: 0.00796169 train accuracy: 50.8889\n",
      "validation accuracy 99.35\n",
      "loss: 0.00852449 train accuracy: 50.0556\n",
      "validation accuracy 99.35\n",
      "loss: 0.00888643 train accuracy: 50.0556\n",
      "validation accuracy 99.35\n",
      "loss: 0.0827052 train accuracy: 50.0\n",
      "validation accuracy 99.35\n",
      "loss: 0.0087881 train accuracy: 50.5\n",
      "validation accuracy 99.35\n",
      "loss: 0.00390342 train accuracy: 50.0556\n",
      "validation accuracy 99.35\n",
      "loss: 0.0680305 train accuracy: 50.3333\n",
      "validation accuracy 99.35\n",
      "loss: 0.00720122 train accuracy: 50.2222\n",
      "validation accuracy 99.35\n",
      "loss: 0.0597459 train accuracy: 50.3333\n",
      "validation accuracy 99.35\n",
      "loss: 0.00416154 train accuracy: 50.8889\n",
      "validation accuracy 99.35\n",
      "epoch 6\n",
      "loss: 0.0893532 train accuracy: 50.0\n",
      "validation accuracy 99.35\n",
      "loss: 0.00890468 train accuracy: 50.2222\n",
      "validation accuracy 99.35\n",
      "loss: 0.0185868 train accuracy: 50.5\n",
      "validation accuracy 99.35\n",
      "loss: 0.00506763 train accuracy: 51.3889\n",
      "validation accuracy 99.35\n",
      "loss: 0.00193759 train accuracy: 50.0556\n",
      "validation accuracy 99.35\n",
      "loss: 0.027226 train accuracy: 50.6667\n",
      "validation accuracy 99.4\n",
      "loss: 0.0102469 train accuracy: 56.7222\n",
      "validation accuracy 99.35\n",
      "loss: 0.00888929 train accuracy: 50.2222\n",
      "validation accuracy 99.35\n",
      "loss: 0.00531808 train accuracy: 50.5\n",
      "validation accuracy 99.35\n",
      "loss: 0.014312 train accuracy: 52.7222\n",
      "validation accuracy 99.35\n",
      "loss: 0.0951988 train accuracy: 50.3333\n",
      "validation accuracy 99.35\n",
      "loss: 0.00373371 train accuracy: 50.5\n",
      "validation accuracy 99.35\n",
      "loss: 0.0335907 train accuracy: 50.0\n",
      "validation accuracy 99.35\n",
      "loss: 0.00378661 train accuracy: 53.5556\n",
      "validation accuracy 99.35\n",
      "loss: 0.00551854 train accuracy: 50.0556\n",
      "validation accuracy 99.35\n",
      "loss: 0.0400664 train accuracy: 50.0\n",
      "validation accuracy 99.45\n",
      "loss: 0.00333023 train accuracy: 50.2222\n",
      "validation accuracy 99.45\n",
      "epoch 7\n",
      "loss: 0.00283498 train accuracy: 50.0556\n",
      "validation accuracy 99.65\n",
      "loss: 0.023275 train accuracy: 50.1111\n",
      "validation accuracy 99.65\n",
      "loss: 0.019632 train accuracy: 52.3333\n",
      "validation accuracy 99.85\n",
      "loss: 0.00460501 train accuracy: 50.0556\n",
      "validation accuracy 99.85\n",
      "loss: 0.00259669 train accuracy: 52.7222\n",
      "validation accuracy 99.8\n",
      "loss: 0.00570769 train accuracy: 51.3889\n",
      "validation accuracy 99.85\n",
      "loss: 0.00821627 train accuracy: 50.0556\n",
      "validation accuracy 100.0\n",
      "loss: 0.00253469 train accuracy: 50.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy 99.95\n",
      "loss: 0.000894741 train accuracy: 50.2222\n",
      "validation accuracy 100.0\n",
      "loss: 0.00256913 train accuracy: 50.2222\n",
      "validation accuracy 100.0\n",
      "loss: 0.00106016 train accuracy: 50.0556\n",
      "validation accuracy 100.0\n",
      "loss: 0.00038465 train accuracy: 50.0\n",
      "validation accuracy 100.0\n",
      "loss: 0.00259706 train accuracy: 52.0\n",
      "validation accuracy 100.0\n",
      "loss: 0.00144226 train accuracy: 50.8889\n",
      "validation accuracy 100.0\n",
      "loss: 0.000311846 train accuracy: 50.2222\n",
      "validation accuracy 100.0\n",
      "loss: 0.00356083 train accuracy: 52.7222\n",
      "validation accuracy 100.0\n",
      "loss: 0.000248363 train accuracy: 50.0\n",
      "validation accuracy 100.0\n",
      "epoch 8\n",
      "loss: 0.000447137 train accuracy: 52.7222\n",
      "validation accuracy 100.0\n",
      "loss: 0.000357972 train accuracy: 51.3889\n",
      "validation accuracy 100.0\n",
      "loss: 0.00173224 train accuracy: 54.5\n",
      "validation accuracy 100.0\n",
      "loss: 0.000699262 train accuracy: 52.7222\n",
      "validation accuracy 100.0\n",
      "loss: 0.000232548 train accuracy: 52.0\n",
      "validation accuracy 100.0\n",
      "loss: 0.000444712 train accuracy: 52.0\n",
      "validation accuracy 100.0\n",
      "loss: 0.00021345 train accuracy: 50.5\n",
      "validation accuracy 100.0\n",
      "loss: 0.000200345 train accuracy: 50.5\n",
      "validation accuracy 100.0\n",
      "loss: 0.00023448 train accuracy: 50.8889\n",
      "validation accuracy 100.0\n",
      "loss: 0.000502188 train accuracy: 52.7222\n",
      "validation accuracy 100.0\n",
      "loss: 0.000372272 train accuracy: 50.0\n",
      "validation accuracy 100.0\n",
      "loss: 0.000125701 train accuracy: 50.5\n",
      "validation accuracy 100.0\n",
      "loss: 0.000231311 train accuracy: 50.5\n",
      "validation accuracy 100.0\n",
      "loss: 0.000191551 train accuracy: 51.3889\n",
      "validation accuracy 100.0\n",
      "loss: 0.000142573 train accuracy: 50.0556\n",
      "validation accuracy 100.0\n",
      "loss: 0.000403815 train accuracy: 51.3889\n",
      "validation accuracy 100.0\n",
      "loss: 0.000128135 train accuracy: 50.5\n",
      "validation accuracy 100.0\n",
      "epoch 9\n",
      "loss: 0.000137722 train accuracy: 53.5556\n",
      "validation accuracy 100.0\n",
      "loss: 0.000121442 train accuracy: 50.0\n",
      "validation accuracy 100.0\n",
      "loss: 0.000198838 train accuracy: 50.2222\n",
      "validation accuracy 100.0\n",
      "loss: 0.000186381 train accuracy: 50.0\n",
      "validation accuracy 100.0\n",
      "loss: 9.17439e-05 train accuracy: 50.0556\n",
      "validation accuracy 100.0\n",
      "loss: 8.21362e-05 train accuracy: 50.2222\n",
      "validation accuracy 100.0\n",
      "loss: 0.00012442 train accuracy: 52.0\n",
      "validation accuracy 100.0\n",
      "Stopping early during epoch 9 with best validation accuracy 100.0\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/reber_rnn_model_early_stopping.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "rnn_classifier = RnnClassifier(max_sequence_length, learning_rate=0.01, n_neurons=40)\n",
    "\n",
    "rnn_classifier.fit(train_X, train_seq_lengths, train_y, validation_X, validation_seq_lengths, validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> First 20 training instances accuracy 100.0\n",
      ">>>> Test dataset accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "subset_train_acc = rnn_classifier.prediction_accuracy(train_X[:20], train_seq_lengths[:20], train_y[:20])\n",
    "print(\">>>> First 20 training instances accuracy\", subset_train_acc)\n",
    "\n",
    "test_acc = rnn_classifier.prediction_accuracy(test_X, test_seq_lengths, test_y)\n",
    "print(\">>>> Test dataset accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN correctly understands the embedded reber grammar by correctly identifying valid sentences and invalid sentences with **100% test accuracy**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8\n",
    "===\n",
    "Try to estimate how much it rained based polarimetric radar measurements: https://www.kaggle.com/c/how-much-did-it-rain-ii/\n",
    "\n",
    "First, let's clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13765201\n",
      "7481030\n",
      "7293935\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "cleaned_training_data_path = \"./data/rain/train_clean.csv\"\n",
    "\n",
    "train_df = pd.read_csv(\"./data/rain/train.csv\")\n",
    "print(len(train_df))\n",
    "\n",
    "# There are a lot of records that don't have any polarimetric radar measurements. Let's remove those most of those.\n",
    "train_df2 = train_df[train_df.Ref_5x5_90th > 0]\n",
    "print(len(train_df2))\n",
    "\n",
    "# Some of the expected rainfall values are too big. Let's remove these outliers\n",
    "expected_outlier_value_start_in_mm = 45\n",
    "train_df3 = train_df2[train_df2.Expected < expected_outlier_value_start_in_mm]\n",
    "\n",
    "print(len(train_df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df3.to_csv(cleaned_training_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84181, 19, 22)\n",
      "(21046, 19, 22)\n",
      "[[  1.           2.           9.           5.           7.5         10.5\n",
      "   15.          10.5         16.5         23.5          0.99833333\n",
      "    0.99833333   0.99833333   0.99833333   0.375       -0.125        0.3125\n",
      "    0.875        1.0599976   -1.4100037   -0.3500061    1.0599976 ]\n",
      " [  6.           2.          26.5         22.5         25.5         31.5\n",
      "   26.5         26.5         28.5         32.           1.0016667\n",
      "    0.9816667    0.99833333   1.005        0.0625      -0.1875       0.25\n",
      "    0.6875       0.           0.           0.           1.4099884 ]\n",
      " [ 11.           2.          21.5         15.5         20.5         25.\n",
      "   26.5         23.5         25.          27.           1.0016667    0.995\n",
      "    0.99833333   1.0016667    0.3125      -0.0625       0.3125       0.625\n",
      "    0.34999084   0.          -0.3500061    1.7599945 ]\n",
      " [ 16.           2.          18.          14.          17.5         21.\n",
      "   20.5         18.          20.5         23.           0.995        0.995\n",
      "    0.99833333   1.0016667    0.25         0.125        0.375        0.6875\n",
      "    0.34999084  -1.0599976    0.           1.0599976 ]\n",
      " [ 21.           2.          24.5         16.5         21.          24.5\n",
      "   24.5         21.          24.          28.           0.99833333   0.995\n",
      "    0.99833333   0.99833333   0.25         0.0625       0.1875       0.5625\n",
      "   -0.3500061   -1.0599976   -0.3500061    1.7599945 ]\n",
      " [ 26.           2.          12.          12.          16.          20.\n",
      "   16.5         17.          19.          21.           0.99833333   0.995\n",
      "    0.99833333   0.99833333   0.5625       0.25         0.4375       0.6875\n",
      "   -1.7600098   -1.7600098   -0.3500061    0.70999146]\n",
      " [ 31.           2.          22.5         19.          22.          25.\n",
      "   26.          23.5         25.5         27.5          0.99833333   0.995\n",
      "    0.99833333   1.0016667    0.          -0.1875       0.25         0.625\n",
      "   -1.0599976   -2.1200104   -0.7100067    0.34999084]\n",
      " [ 37.           2.          14.          14.          18.5         21.\n",
      "   19.5         20.          21.          23.           0.99833333\n",
      "    0.9916667    0.99833333   0.99833333   0.5          0.1875       0.4375\n",
      "    0.8125       0.          -1.7600098   -0.3500061    1.0599976 ]\n",
      " [ 42.           2.          12.          11.          12.5         17.\n",
      "   19.5         18.          21.          23.           0.99833333   0.995\n",
      "    0.99833333   0.99833333   0.625        0.375        0.625        0.875\n",
      "   -0.3500061   -0.3500061    0.           0.34999084]\n",
      " [ 47.           2.           1.5          3.5          7.          10.5\n",
      "   18.          16.5         18.5         21.5          0.99833333   0.995\n",
      "    0.99833333   0.99833333   0.375        0.1875       0.5          0.6875\n",
      "    0.34999084  -2.1100006   -0.3500061    1.0599976 ]\n",
      " [ 53.           2.          16.          14.5         18.          23.5\n",
      "   28.          23.5         26.5         29.5          0.99833333\n",
      "    0.9916667    0.99833333   0.99833333   0.875        0.625        0.9375\n",
      "    1.375       -0.3500061   -1.4100037   -0.3500061    2.119995  ]\n",
      " [ 58.           2.          22.          16.5         22.5         26.5\n",
      "   31.5         26.5         29.          32.           0.99833333   0.995\n",
      "    0.99833333   1.0016667    0.375        0.1875       0.375        0.875\n",
      "   -1.4100037    0.          -0.3500061    0.69999695]\n",
      " [  0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.        ]]\n",
      "1.0160005\n",
      "1 19\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def prep_field(field):\n",
    "    if math.isnan(field):\n",
    "        return 0.\n",
    "    else:\n",
    "        return float(field)\n",
    "\n",
    "def zero_pad_sequences(data):\n",
    "    max_sequence_length = max(list(map(lambda a: len(a[0]), data)))\n",
    "    n_features = len(data[0][0][0])\n",
    "    new_data = []\n",
    "    for sequence, seq_len, y in data:\n",
    "        if len(sequence) < max_sequence_length:\n",
    "            sequence = np.lib.pad(sequence, ((0, max_sequence_length - len(sequence)), (0, 0)), 'constant', constant_values=(0, 0))    \n",
    "        new_data.append((sequence, seq_len, y))\n",
    "    return new_data\n",
    "    \n",
    "def prep_data(df):\n",
    "    data = []\n",
    "    for id, group_df in df.groupby(['Id']):\n",
    "        features = group_df.drop([\"Id\", \"Expected\"], axis=1).to_records(index=False)\n",
    "        # Assume all expected values are the same for the group\n",
    "        y = group_df.Expected.iloc[0]\n",
    "        data.append((np.array([tuple([prep_field(y) for y in x]) for x in features]), len(features), y))\n",
    "    return zip(*zero_pad_sequences(data))\n",
    "\n",
    "X, seq_lengths, y = prep_data(train_df3[:1000000])\n",
    "split_indices = [int(len(X) * 0.8)] # Give the training dataset 80% of the size\n",
    "train_X, valid_X = np.split(X, split_indices)\n",
    "train_seq_lengths, valid_seq_lengths = np.split(seq_lengths, split_indices)\n",
    "train_y, valid_y = np.split(y, split_indices)\n",
    "\n",
    "print(train_X.shape)\n",
    "print(valid_X.shape)\n",
    "print(train_X[0])\n",
    "print(train_y[0])\n",
    "print(min(seq_lengths), max(seq_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import tensorflow as tf\n",
    "\n",
    "def he_normal_initialisation(n_inputs, n_outputs):\n",
    "    stddev = np.power(2 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.truncated_normal((n_inputs, n_outputs), stddev=stddev)\n",
    "\n",
    "def he_uniform_initialisation(n_inputs, n_outputs):\n",
    "    r = np.power(6 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.random_uniform((n_inputs, n_outputs), -r, r)\n",
    "\n",
    "def create_next_batch_fn(data, sequence_lengths, targets, batch_size):\n",
    "    assert len(data) == len(sequence_lengths) and len(data) == len(targets)\n",
    "    current_batch = 0\n",
    "    def next_batch():\n",
    "        nonlocal current_batch\n",
    "        i = current_batch\n",
    "        #print(current_batch)\n",
    "        current_batch = (current_batch + batch_size) % len(data)\n",
    "        return data[i:i+batch_size], sequence_lengths[i:i+batch_size], targets[i:i+batch_size]\n",
    "    return next_batch\n",
    "\n",
    "class RnnClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_steps, n_inputs, learning_rate=0.001, n_neurons=2):\n",
    "        self.n_steps = n_steps\n",
    "        self.n_inputs = n_inputs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_neurons = n_neurons\n",
    "        self._build_graph()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        self.n_output = 1\n",
    "        self.batch_size = 30\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, shape=(None, self.n_steps, self.n_inputs), name=\"input\")\n",
    "        self.sequence_length = tf.placeholder(tf.int32, shape=(None), name=\"sequence_length\")\n",
    "        self.y = tf.placeholder(tf.float32, shape=(None), name=\"y\")\n",
    "\n",
    "        with tf.name_scope(\"rnn\"):\n",
    "            self.rnn_activation_midpoint = 0.0\n",
    "            #cell = tf.contrib.rnn.GRUCell(num_units=self.n_neurons, activation=None)\n",
    "            cell = tf.contrib.rnn.GRUCell(num_units=self.n_neurons, activation=tf.nn.tanh)\n",
    "            #cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(num_units=self.n_neurons, activation=tf.nn.tanh), tf.contrib.rnn.GRUCell(num_units=self.n_neurons, activation=tf.nn.tanh)])\n",
    "            #cell = tf.nn.rnn_cell.BasicRNNCell(num_units=self.n_neurons)\n",
    "            outputs, last_outputs = tf.nn.dynamic_rnn(cell, self.x, dtype=tf.float32, sequence_length=self.sequence_length)\n",
    "            #print(\"last_outputs.shape\", last_outputs.shape)\n",
    "            with tf.name_scope(\"fc\"):\n",
    "                W = tf.Variable(tf.truncated_normal((self.n_neurons, self.n_output), stddev=10.0), name=\"weights\")\n",
    "                b = tf.Variable(tf.zeros([self.n_output]), name=\"biases\")\n",
    "                self.rainfall = tf.matmul(last_outputs, W) + b\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.abs(self.rainfall - self.y))\n",
    "\n",
    "        with tf.name_scope(\"training\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.training_op = optimizer.minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            epsilon = 0.1\n",
    "            correctness = tf.abs(self.rainfall - self.y) < epsilon\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correctness, tf.float32)) * 100.0\n",
    "            \n",
    "        self.init = tf.global_variables_initializer()\n",
    "\n",
    "    def fit(self, X, sequence_lengths, y, valid_X, valid_sequence_length, valid_y, epochs = 50):\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        interim_checkpoint_path = \"./checkpoints/reber_rnn_model.ckpt\"\n",
    "        early_stopping_checkpoint_path = \"./checkpoints/reber_rnn_model_early_stopping.ckpt\"\n",
    "\n",
    "        from datetime import datetime\n",
    "\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        root_logdir = \"tf_logs\"\n",
    "        log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "        loss_summary = tf.summary.scalar('loss', self.loss)\n",
    "        accuracy_summary = tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "        summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "        file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "        \n",
    "        n_batches = int(np.ceil(len(X) // self.batch_size))\n",
    "        next_batch = create_next_batch_fn(X, sequence_lengths, y, self.batch_size)\n",
    "            \n",
    "        early_stopping_check_frequency = n_batches // 16\n",
    "        early_stopping_check_limit = n_batches\n",
    "\n",
    "        sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        self.session = sess\n",
    "        sess.run(self.init)\n",
    "        #saver.restore(sess, interim_checkpoint_path)\n",
    "\n",
    "        best_validation_acc = 0.0\n",
    "        best_validation_step = 0\n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch\", epoch)\n",
    "            for batch_index in range(n_batches):\n",
    "                step = epoch * n_batches + batch_index\n",
    "                X_batch, seq_length_batch, y_batch = next_batch()\n",
    "                #print(\"training batch\", X_batch.shape, seq_length_batch.shape, y_batch.shape)\n",
    "                #print(seq_length_batch)\n",
    "                if batch_index % 10 == 0:\n",
    "                    summary_str = summary_op.eval(session=sess, feed_dict={self.x: X_batch, self.sequence_length: seq_length_batch, self.y: y_batch})\n",
    "                    file_writer.add_summary(summary_str, step)\n",
    "                t, l, a = sess.run([self.training_op, self.loss, self.accuracy], feed_dict={self.x: X_batch, self.sequence_length: seq_length_batch, self.y: y_batch})\n",
    "                #a = self.prediction_accuracy(X_batch, seq_length_batch, y_batch)\n",
    "                #print(\"y_proba\", proba)\n",
    "                if batch_index % 10 == 0: print(\"loss:\", l, \"train accuracy:\", a)\n",
    "                # Early stopping check\n",
    "                if batch_index % early_stopping_check_frequency == 0:\n",
    "                    validation_acc = self.prediction_accuracy(valid_X, valid_sequence_length, valid_y)\n",
    "                    print(\"validation accuracy\", validation_acc)\n",
    "                    if validation_acc > best_validation_acc:\n",
    "                        saver.save(sess, early_stopping_checkpoint_path)\n",
    "                        best_validation_acc = validation_acc\n",
    "                        best_validation_step = step\n",
    "                    elif step >= (best_validation_step + early_stopping_check_limit):\n",
    "                        print(\"Stopping early during epoch\", epoch, \"with best validation accuracy\", best_validation_acc)\n",
    "                        break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            save_path = saver.save(sess, interim_checkpoint_path)\n",
    "        saver.restore(sess, early_stopping_checkpoint_path)\n",
    "        save_path = saver.save(sess, \"./checkpoints/reber_rnn_model_final.ckpt\")\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        raise Exception(\"Not Implemented\")\n",
    "\n",
    "    def predict(self, X, sequence_lengths):\n",
    "        return self.session.run([self.rainfall], feed_dict={self.x: X, self.sequence_length: sequence_lengths})\n",
    "    \n",
    "    def prediction_accuracy(self, X, sequence_lengths, y):\n",
    "        return self.session.run([self.accuracy], feed_dict={self.x: X, self.sequence_length: sequence_lengths, self.y: y})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "loss: 88.6418 train accuracy: 0.0\n",
      "validation accuracy 0.00164494\n",
      "loss: 14.0066 train accuracy: 0.0\n",
      "loss: 14.6535 train accuracy: 0.0\n",
      "loss: 8.66533 train accuracy: 0.777778\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "_, n_steps, n_inputs = train_X.shape\n",
    "rnn_classifier = RnnClassifier(n_steps, n_inputs, learning_rate=0.011, n_neurons=20)\n",
    "\n",
    "rnn_classifier.fit(train_X, train_seq_lengths, train_y, valid_X, valid_seq_lengths, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.56413883],\n",
      "       [ 1.5448885 ],\n",
      "       [-0.39569986],\n",
      "       [ 0.22859526],\n",
      "       [ 0.82138163]], dtype=float32)]\n",
      "[  1.0160005   26.162014     4.064002     0.50800025   3.2250018 ]\n"
     ]
    }
   ],
   "source": [
    "print(rnn_classifier.predict(train_X[:5], train_seq_lengths[:5]))\n",
    "print(train_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
