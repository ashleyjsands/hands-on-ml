{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 7\n",
    "===\n",
    "Create a RNN that can learn a Reber grammer (http://www.willamette.edu/~gorr/classes/cs449/reber.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "\n",
    "start_symbol = \"b\"\n",
    "end_symbol = \"e\"\n",
    "\n",
    "symbols = [start_symbol, \"t\", \"p\", \"s\", \"x\", \"v\", end_symbol]\n",
    "\n",
    "def get_next_symbols(previous_symbol, current_symbol):\n",
    "    if current_symbol == None and previous_symbol == None:\n",
    "        return [start_symbol]\n",
    "    elif current_symbol == start_symbol:\n",
    "        return [\"t\", \"p\"]\n",
    "    elif current_symbol == \"t\":\n",
    "        if previous_symbol == start_symbol:\n",
    "            return [\"s\", \"x\"]\n",
    "        elif previous_symbol in [\"p\", \"x\", \"t\"]:\n",
    "            return [\"t\", \"v\"]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == \"p\":\n",
    "        if previous_symbol == start_symbol:\n",
    "            return [\"t\", \"v\"]\n",
    "        elif previous_symbol == \"v\":\n",
    "            return [\"x\", \"s\"]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == \"s\":\n",
    "        if previous_symbol in [\"t\", \"s\"]:\n",
    "            return [\"x\", \"s\"]\n",
    "        elif previous_symbol in [\"x\", \"p\"]:\n",
    "            return [end_symbol]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == \"x\":\n",
    "        if previous_symbol in [\"t\", \"s\"]:\n",
    "            return [\"x\", \"s\"]\n",
    "        elif previous_symbol in [\"x\", \"p\"]:\n",
    "            return [\"t\", \"v\"]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == \"v\":\n",
    "        if previous_symbol in [\"t\", \"x\", \"p\"]:\n",
    "            return [\"p\", \"v\"]\n",
    "        elif previous_symbol == \"v\":\n",
    "            return [ end_symbol ]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == end_symbol:\n",
    "        return []\n",
    "    else:\n",
    "        raise Exception(\"Invalid symbols: %s and %s.\" % (previous_symbol, current_symbol))\n",
    "\n",
    "def get_next_symbols_for_string(reber_str):\n",
    "    previous_symbol = reber_str[-2] if len(reber_str) >= 2 else None\n",
    "    current_symbol = reber_str[-1] if len(reber_str) >= 1 else None\n",
    "    return get_next_symbols(previous_symbol, current_symbol)\n",
    "\n",
    "def create_reber_string():\n",
    "    reber_str = \"\"\n",
    "    while not reber_str.endswith(end_symbol):\n",
    "        reber_str += random.choice(get_next_symbols_for_string(reber_str))\n",
    "    return reber_str\n",
    "\n",
    "def is_valid_reber_string(value):\n",
    "    index = 0\n",
    "    while index < len(value):\n",
    "        current = value[index]\n",
    "        next_symbols = get_next_symbols_for_string(value[:index] if index != 0 else \"\")\n",
    "        if current not in next_symbols:\n",
    "            return False\n",
    "        index += 1\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 10000 validation size 2000 test size 2000\n",
      "all_strings.shape (14000, 49, 7)\n",
      "Example sequence: [[1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]] True\n",
      "The max sequence length is 49\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "def create_reber_strings(size):\n",
    "    strings = []\n",
    "    while len(strings) < size:\n",
    "        new_string = create_reber_string()\n",
    "        if new_string not in strings:\n",
    "            strings.append(new_string)\n",
    "    return strings\n",
    "\n",
    "def invalidate_reber_string(value):\n",
    "    while is_valid_reber_string(value):\n",
    "        char_list = list(value)\n",
    "        char_list[random.randint(0, len(value) - 1)] = random.choice(symbols)\n",
    "        value = \"\".join(char_list)\n",
    "    return value\n",
    "\n",
    "def invalidate_reber_strings(strings):\n",
    "    return [invalidate_reber_string(x) for x in strings]\n",
    "\n",
    "def symbol_to_one_hot_encoding(symbol):\n",
    "    one_hot_encoding = np.zeros((len(symbols)), dtype=np.int8)\n",
    "    if symbol != '':\n",
    "        one_hot_encoding[symbols.index(symbol)] = 1\n",
    "    return one_hot_encoding\n",
    "\n",
    "def prep_strings_for_model(strings, sequence_length):\n",
    "    padded_strings = np.array([np.pad(list(x), (0, (sequence_length - len(x)) % sequence_length), 'constant') for x in strings])\n",
    "    sequences = np.array([np.array(list(map(symbol_to_one_hot_encoding, x))) for x in padded_strings])\n",
    "    return sequences\n",
    "\n",
    "def generate_dataset(size, error_ratio = 0.5):\n",
    "    if size % 2 != 0:\n",
    "        raise Exception(\"size must be a multiple of 2.\")\n",
    "    correct_strings = create_reber_strings(int(size * error_ratio))\n",
    "    incorrect_strings = invalidate_reber_strings(correct_strings)\n",
    "    correct_val = True\n",
    "    incorrect_val = False\n",
    "    targets = np.array(([ correct_val ] * len(correct_strings)) + ([ incorrect_val ] * len(incorrect_strings)))\n",
    "    indices = np.random.permutation(size)\n",
    "    strings = correct_strings + incorrect_strings\n",
    "    sequence_lengths = np.array([len(x) for x in strings])\n",
    "    max_sequence_length = max(sequence_lengths)\n",
    "    strings = prep_strings_for_model(strings, max_sequence_length)\n",
    "    #print(strings[0])\n",
    "    return strings[indices], sequence_lengths, targets[indices]\n",
    "\n",
    "train_size = 10000\n",
    "validation_size = int(train_size * 0.2)\n",
    "test_size = int(train_size * 0.2)\n",
    "\n",
    "all_strings, sequence_lengths, all_targets = generate_dataset(train_size + validation_size + test_size)\n",
    "train_X = all_strings[:train_size]\n",
    "train_seq_lengths = sequence_lengths[:train_size]\n",
    "train_y = all_targets[:train_size]\n",
    "validation_X = all_strings[train_size:train_size+validation_size]\n",
    "validation_seq_lengths = sequence_lengths[train_size:train_size+validation_size]\n",
    "validation_y = all_targets[train_size:train_size+validation_size]\n",
    "test_X = all_strings[train_size+validation_size:train_size+validation_size+test_size]\n",
    "test_seq_lengths = sequence_lengths[train_size+validation_size:train_size+validation_size+test_size]\n",
    "test_y = all_targets[train_size+validation_size:train_size+validation_size+test_size]\n",
    "\n",
    "#print(list(map(symbol_to_one_hot_encoding, create_reber_string())))\n",
    "print(\"train size\", len(train_X), \"validation size\", len(validation_X), \"test size\", len(test_X))\n",
    "print(\"all_strings.shape\", all_strings.shape)\n",
    "#print(\"string shapes\", list(map(lambda a: a.shape, all_strings)))\n",
    "print(\"Example sequence:\", all_strings[1], all_targets[1])\n",
    "#print(\"sequence_lengths\", sequence_lengths)\n",
    "\n",
    "max_sequence_length = train_X[0].shape[0]\n",
    "print(\"The max sequence length is\", max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "def he_normal_initialisation(n_inputs, n_outputs):\n",
    "    stddev = np.power(2 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.truncated_normal((n_inputs, n_outputs), stddev=stddev)\n",
    "\n",
    "def he_uniform_initialisation(n_inputs, n_outputs):\n",
    "    r = np.power(6 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.random_uniform((n_inputs, n_outputs), -r, r)\n",
    "\n",
    "def create_next_batch_fn(data, sequence_lengths, targets, batch_size):\n",
    "    assert len(data) == len(sequence_lengths) and len(data) == len(targets)\n",
    "    current_batch = 0\n",
    "    def next_batch():\n",
    "        nonlocal current_batch\n",
    "        i = current_batch\n",
    "        #print(current_batch)\n",
    "        current_batch = (current_batch + batch_size) % len(data)\n",
    "        return data[i:i+batch_size], sequence_lengths[i:i+batch_size], targets[i:i+batch_size]\n",
    "    return next_batch\n",
    "\n",
    "class RnnClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_steps, learning_rate=0.001, n_neurons=2):\n",
    "        self.n_steps = n_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_neurons = n_neurons\n",
    "        self._build_graph()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        n_inputs = 1\n",
    "        self.n_output = 1\n",
    "        self.batch_size = 50\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, shape=(None, self.n_steps, len(symbols)), name=\"input\")\n",
    "        self.sequence_length = tf.placeholder(tf.int32, shape=(None), name=\"sequence_length\")\n",
    "        self.y = tf.placeholder(tf.bool, shape=(None), name=\"y\")\n",
    "\n",
    "        with tf.name_scope(\"rnn\"):\n",
    "            #cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "            #    tf.contrib.rnn.BasicRNNCell(num_units=self.n_neurons, activation=tf.nn.relu),\n",
    "            #    output_size=self.n_output)\n",
    "            self.rnn_activation_midpoint = 0.0\n",
    "            cell = tf.contrib.rnn.GRUCell(num_units=self.n_neurons, activation=tf.nn.tanh)\n",
    "            outputs, last_outputs = tf.nn.dynamic_rnn(cell, self.x, dtype=tf.float32, sequence_length=self.sequence_length)\n",
    "            #print(\"last_outputs.shape\", last_outputs.shape)\n",
    "            with tf.name_scope(\"fc\"):\n",
    "                W = tf.Variable(tf.truncated_normal((self.n_neurons, self.n_output), stddev=10.1), name=\"weights\")\n",
    "                b = tf.Variable(tf.zeros([self.n_output]), name=\"biases\")\n",
    "                logits = tf.matmul(last_outputs, W) + b\n",
    "                self.y_proba = tf.nn.sigmoid(logits)\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            y_float = tf.reshape(tf.cast(self.y, tf.float32), (-1, 1))\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_float, logits=logits))\n",
    "\n",
    "        with tf.name_scope(\"training\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.training_op = optimizer.minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            self.y_pred = logits > self.rnn_activation_midpoint\n",
    "            correctness = tf.equal(self.y_pred, self.y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correctness, tf.float32)) * 100\n",
    "            \n",
    "        self.init = tf.global_variables_initializer()\n",
    "\n",
    "    def fit(self, X, sequence_lengths, y, valid_X, valid_sequence_length, valid_y, epochs = 20):\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        interim_checkpoint_path = \"./checkpoints/reber_rnn_model.ckpt\"\n",
    "        early_stopping_checkpoint_path = \"./checkpoints/reber_rnn_model_early_stopping.ckpt\"\n",
    "\n",
    "        from datetime import datetime\n",
    "\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        root_logdir = \"tf_logs\"\n",
    "        log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "        loss_summary = tf.summary.scalar('loss', self.loss)\n",
    "        accuracy_summary = tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "        summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "        file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "        \n",
    "        n_batches = int(np.ceil(len(X) // self.batch_size))\n",
    "        next_batch = create_next_batch_fn(X, sequence_lengths, y, self.batch_size)\n",
    "            \n",
    "        early_stopping_check_frequency = n_batches // 16\n",
    "        early_stopping_check_limit = n_batches * 2\n",
    "\n",
    "        sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        self.session = sess\n",
    "        sess.run(self.init)\n",
    "        #saver.restore(sess, interim_checkpoint_path)\n",
    "\n",
    "        best_validation_acc = 0.0\n",
    "        best_validation_step = 0\n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch\", epoch)\n",
    "            for batch_index in range(n_batches):\n",
    "                step = epoch * n_batches + batch_index\n",
    "                X_batch, seq_length_batch, y_batch = next_batch()\n",
    "                #print(\"training batch\", X_batch.shape, seq_length_batch.shape, y_batch.shape)\n",
    "                #print(seq_length_batch)\n",
    "                if batch_index % 10 == 0:\n",
    "                    summary_str = summary_op.eval(session=sess, feed_dict={self.x: X_batch, self.sequence_length: seq_length_batch, self.y: y_batch})\n",
    "                    file_writer.add_summary(summary_str, step)\n",
    "                t, l, a, proba = sess.run([self.training_op, self.loss, self.accuracy, self.y_proba], feed_dict={self.x: X_batch, self.sequence_length: seq_length_batch, self.y: y_batch})\n",
    "                #print(\"y_proba\", proba)\n",
    "                if batch_index % 10 == 0: print(\"loss:\", l, \"train accuracy:\", a)\n",
    "                # Early stopping check\n",
    "                if batch_index % early_stopping_check_frequency == 0:\n",
    "                    validation_acc = self.prediction_accuracy(valid_X, valid_sequence_length, valid_y)\n",
    "                    print(\"validation accuracy\", validation_acc)\n",
    "                    if validation_acc > best_validation_acc:\n",
    "                        saver.save(sess, early_stopping_checkpoint_path)\n",
    "                        best_validation_acc = validation_acc\n",
    "                        best_validation_step = step\n",
    "                    elif step >= (best_validation_step + early_stopping_check_limit):\n",
    "                        print(\"Stopping early during epoch\", epoch)\n",
    "                        break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            save_path = saver.save(sess, interim_checkpoint_path)\n",
    "        saver.restore(sess, early_stopping_checkpoint_path)\n",
    "        save_path = saver.save(sess, \"./checkpoints/reber_rnn_model_final.ckpt\")\n",
    "            \n",
    "    def predict_proba(self, X, sequence_lengths):\n",
    "        dataset_size = X.shape[0]\n",
    "        #print \"dataset_size: \", dataset_size, \" batch_size: \", batch_size\n",
    "        predictions = np.ndarray(shape=(dataset_size, self.n_output), dtype=np.float32)\n",
    "        steps = int(math.ceil(dataset_size / self.batch_size))\n",
    "        #print \"steps: \", steps\n",
    "        for step in range(steps):\n",
    "            offset = (step * self.batch_size)\n",
    "            #print \"offset \", offset\n",
    "            data_end_index = min(offset + self.batch_size, dataset_size)\n",
    "            batch_data = X[offset:data_end_index, :]\n",
    "            feed_dict = {\n",
    "                self.x: batch_data,\n",
    "                self.sequence_length: sequence_lengths[offset:data_end_index]\n",
    "            }\n",
    "            predictions[offset:data_end_index, :] = self.y_proba.eval(session=self.session, feed_dict=feed_dict)\n",
    "        #print(\"predict_proba\", predictions)\n",
    "        return predictions\n",
    "\n",
    "    def predict(self, X, sequence_lengths):\n",
    "        return np.argmax(self.predict_proba(X, sequence_lengths), axis=1)\n",
    "    \n",
    "    def _prediction_accuracy(self, predictions, y):\n",
    "        probability_midpoint = 0.5\n",
    "        return (np.sum(((predictions.reshape((-1)) > probability_midpoint) == y).astype(float))\n",
    "              / predictions.shape[0]) * 100\n",
    "    \n",
    "    def prediction_accuracy(self, X, sequence_lengths, y):\n",
    "        predictions = self.predict_proba(X, sequence_lengths)\n",
    "        return self._prediction_accuracy(predictions, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "loss: 3.99558 train accuracy: 58.0\n",
      "validation accuracy 52.1\n",
      "loss: 0.730568 train accuracy: 50.8\n",
      "validation accuracy 48.7\n",
      "loss: 0.71854 train accuracy: 51.6\n",
      "validation accuracy 49.15\n",
      "loss: 0.712743 train accuracy: 51.6\n",
      "validation accuracy 53.95\n",
      "loss: 0.684097 train accuracy: 51.92\n",
      "validation accuracy 53.95\n",
      "loss: 0.69673 train accuracy: 48.32\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "rnn_classifier = RnnClassifier(max_sequence_length, learning_rate=0.1, n_neurons=10)\n",
    "\n",
    "rnn_classifier.fit(train_X, train_seq_lengths, train_y, validation_X, validation_seq_lengths, validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_train_acc = rnn_classifier.prediction_accuracy(train_X[:20], train_seq_lengths[:20], train_y[:20])\n",
    "print(\">>>> First 20 training instances accuracy\", subset_train_acc)\n",
    "\n",
    "\n",
    "test_acc = rnn_classifier.prediction_accuracy(test_X, test_seq_lengths, test_y)\n",
    "print(\">>>> Test dataset accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
