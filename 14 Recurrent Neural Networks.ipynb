{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 7\n",
    "===\n",
    "Create a RNN that can learn a Reber grammer (http://www.willamette.edu/~gorr/classes/cs449/reber.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "start_symbol = \"b\"\n",
    "end_symbol = \"e\"\n",
    "\n",
    "symbols = [start_symbol, \"t\", \"p\", \"s\", \"x\", \"v\", end_symbol]\n",
    "\n",
    "def get_next_symbols(previous_symbol, current_symbol):\n",
    "    if current_symbol == None and previous_symbol == None:\n",
    "        return [start_symbol]\n",
    "    elif current_symbol == start_symbol:\n",
    "        return [\"t\", \"p\"]\n",
    "    elif current_symbol == \"t\":\n",
    "        if previous_symbol == start_symbol:\n",
    "            return [\"s\", \"x\"]\n",
    "        elif previous_symbol in [\"p\", \"x\", \"t\"]:\n",
    "            return [\"t\", \"v\"]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == \"p\":\n",
    "        if previous_symbol == start_symbol:\n",
    "            return [\"t\", \"v\"]\n",
    "        elif previous_symbol == \"v\":\n",
    "            return [\"x\", \"s\"]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == \"s\":\n",
    "        if previous_symbol in [\"t\", \"s\"]:\n",
    "            return [\"x\", \"s\"]\n",
    "        elif previous_symbol in [\"x\", \"p\"]:\n",
    "            return [end_symbol]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == \"x\":\n",
    "        if previous_symbol in [\"t\", \"s\"]:\n",
    "            return [\"x\", \"s\"]\n",
    "        elif previous_symbol in [\"x\", \"p\"]:\n",
    "            return [\"t\", \"v\"]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == \"v\":\n",
    "        if previous_symbol in [\"t\", \"x\", \"p\"]:\n",
    "            return [\"p\", \"v\"]\n",
    "        elif previous_symbol == \"v\":\n",
    "            return [ end_symbol ]\n",
    "        else:\n",
    "            raise Exception(\"Invalid grammar.\")\n",
    "    elif current_symbol == end_symbol:\n",
    "        return []\n",
    "    else:\n",
    "        raise Exception(\"Invalid symbols: %s and %s.\" % (previous_symbol, current_symbol))\n",
    "\n",
    "def get_next_symbols_for_string(reber_str):\n",
    "    previous_symbol = reber_str[-2] if len(reber_str) >= 2 else None\n",
    "    current_symbol = reber_str[-1] if len(reber_str) >= 1 else None\n",
    "    return get_next_symbols(previous_symbol, current_symbol)\n",
    "\n",
    "def create_reber_string():\n",
    "    reber_str = \"\"\n",
    "    while not reber_str.endswith(end_symbol):\n",
    "        reber_str += random.choice(get_next_symbols_for_string(reber_str))\n",
    "    return reber_str\n",
    "\n",
    "def is_valid_reber_string(value):\n",
    "    index = 0\n",
    "    while index < len(value):\n",
    "        current = value[index]\n",
    "        next_symbols = get_next_symbols_for_string(value[:index] if index != 0 else \"\")\n",
    "        if current not in next_symbols:\n",
    "            return False\n",
    "        index += 1\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 100 validation size 20 test size 20\n",
      "The max sequence length is 29\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "def create_reber_strings(size):\n",
    "    strings = []\n",
    "    while len(strings) < size:\n",
    "        new_string = create_reber_string()\n",
    "        if new_string not in strings:\n",
    "            strings.append(new_string)\n",
    "    return strings\n",
    "\n",
    "def invalidate_reber_string(value):\n",
    "    while is_valid_reber_string(value):\n",
    "        char_list = list(value)\n",
    "        char_list[random.randint(0, len(value) - 1)] = random.choice(symbols)\n",
    "        value = \"\".join(char_list)\n",
    "    return value\n",
    "\n",
    "def invalidate_reber_strings(strings):\n",
    "    return [invalidate_reber_string(x) for x in strings]\n",
    "\n",
    "\n",
    "def prep_strings_for_model(strings, sequence_length):\n",
    "    return np.array([np.pad(x, (0, sequence_length - len(x)%sequence_length), 'constant') for x in strings])\n",
    "\n",
    "def generate_dataset(size, error_ratio = 0.5):\n",
    "    if size % 2 != 0:\n",
    "        raise Exception(\"size must be a multiple of 2.\")\n",
    "    correct_strings = create_reber_strings(int(size * error_ratio))\n",
    "    incorrect_strings = invalidate_reber_strings(correct_strings)\n",
    "    strings = np.array([list(x) for x in correct_strings + incorrect_strings])\n",
    "    correct_val = 1.0\n",
    "    incorrect_val = 0.0\n",
    "    targets = np.array(([ correct_val ] * len(correct_strings)) + ([ incorrect_val ] * len(incorrect_strings)))\n",
    "    indices = np.random.permutation(size)\n",
    "    max_sequence_length = max([len(x) for x in all_strings])\n",
    "    strings = prep_strings_for_model(strings, max_sequence_length)\n",
    "    return strings[indices], targets[indices]\n",
    "\n",
    "train_size = 100\n",
    "validation_size = 20\n",
    "test_size = 20\n",
    "\n",
    "all_strings, all_targets = generate_dataset(train_size + validation_size + test_size)\n",
    "train_X = all_strings[:train_size]\n",
    "train_y = all_targets[:train_size]\n",
    "validation_X = all_strings[train_size:train_size+validation_size]\n",
    "validation_y = all_targets[train_size:train_size+validation_size]\n",
    "test_X = all_strings[train_size+validation_size:train_size+validation_size+test_size]\n",
    "test_y = all_targets[train_size+validation_size:train_size+validation_size+test_size]\n",
    "\n",
    "print(\"train size\", len(train_X), \"validation size\", len(validation_X), \"test size\", len(test_X))\n",
    "\n",
    "max_sequence_length = train_X.shape[1]\n",
    "print(\"The max sequence length is\", max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t' 'e' 's' 't']\n",
      "bptvve\n",
      "True\n",
      "[True, False, False, True, True, False, False, True, False, True]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "def create_next_batch_fn(images, labels, batch_size):\n",
    "    assert len(images) == len(labels)\n",
    "    current_batch = 0\n",
    "    def next_batch():\n",
    "        nonlocal current_batch\n",
    "        i = current_batch\n",
    "        #print(current_batch)\n",
    "        current_batch = (current_batch + batch_size) % len(images)\n",
    "        return images[i:i+batch_size], labels[i:i+batch_size]\n",
    "    return next_batch\n",
    "\n",
    "class RnnClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_steps, n_neurons=2):\n",
    "        self.n_steps = n_steps\n",
    "        self.n_neurons = n_neurons\n",
    "        self._build_graph()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        n_inputs = 1\n",
    "        self.n_output = 1\n",
    "        self.batch_size = 20\n",
    "        \n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            self.x = tf.placeholder(tf.int64, shape=(None, self.n_steps, n_inputs), name=\"input\")\n",
    "            self.y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "            with tf.name_scope(\"rnn\"):\n",
    "                cell = tf.nn.BasicRNNCell(self.n_neurons, activation=tf.nn.relu)\n",
    "                rnn = tf.nn.static_rnn(cell)\n",
    "                input_tensor = rotated_channels\n",
    "                n_input_filters = input_channels\n",
    "                for i in range(len(n_filters_per_layer)):\n",
    "                    input_tensor = cnn_layer(input_tensor, patch_size, n_input_filters, n_filters_per_layer[i], \"hidden\" + str(i + 1))\n",
    "                    n_input_filters = n_filters_per_layer[i]\n",
    "                #avg_pool_output = tf.nn.avg_pool(input_tensor, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "                # If more than one element of the shape list is -1, the tf.reshape operation below will have an error.\n",
    "                shape = list(map(lambda a: -1 if a == None else a, input_tensor.get_shape().as_list()))\n",
    "                #print(shape)\n",
    "                reshape = tf.reshape(input_tensor, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                fc = tf.nn.dropout(neuron_layer(reshape, self.fully_connected_neurons, \"fully_connected_one\"), keep_prob=0.4)\n",
    "                #print(reshape.get_shape())\n",
    "                logits = neuron_layer(fc, self.n_output, \"output\")\n",
    "                self.evaluation = tf.nn.softmax(logits)\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=logits)\n",
    "                self.loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "\n",
    "            with tf.name_scope(\"training\"):\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "                self.training_op = optimizer.minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            k = 1\n",
    "            correctness = tf.nn.in_top_k(logits, self.y, k)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correctness, tf.float32)) * 100\n",
    "            \n",
    "        self.init = tf.global_variables_initializer()\n",
    "\n",
    "    def fit(self, X, y, valid_X, valid_y, epochs = 20):\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        interim_checkpoint_path = \"./checkpoints/mnist_cnn_model.ckpt\"\n",
    "        early_stopping_checkpoint_path = \"./checkpoints/mnist_cnn_model_early_stopping.ckpt\"\n",
    "\n",
    "        from datetime import datetime\n",
    "\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        root_logdir = \"tf_logs\"\n",
    "        log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "        loss_summary = tf.summary.scalar('loss', self.loss)\n",
    "        accuracy_summary = tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "        summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "        file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "        \n",
    "        n_batches = int(np.ceil(len(X) // self.batch_size))\n",
    "        next_batch = create_next_batch_fn(X, y, self.batch_size)\n",
    "            \n",
    "        early_stopping_check_frequency = self.batch_size // 4\n",
    "        early_stopping_check_limit = self.batch_size * 2\n",
    "\n",
    "        sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        self.session = sess\n",
    "        sess.run(self.init)\n",
    "        #saver.restore(sess, interim_checkpoint_path)\n",
    "\n",
    "        best_validation_acc = 0.0\n",
    "        best_validation_step = 0\n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch\", epoch)\n",
    "            for batch_index in range(n_batches):\n",
    "                step = epoch * n_batches + batch_index\n",
    "                X_batch, y_batch = next_batch()\n",
    "                if batch_index % 10 == 0:\n",
    "                    summary_str = summary_op.eval(session=sess, feed_dict={self.x: X_batch, self.y: y_batch})\n",
    "                    file_writer.add_summary(summary_str, step)\n",
    "                t, l, a = sess.run([self.training_op, self.loss, self.accuracy], feed_dict={self.x: X_batch, self.y: y_batch})\n",
    "                if batch_index % 10 == 0: print(\"loss:\", l, \"train accuracy:\", a)\n",
    "                # Early stopping check\n",
    "                if batch_index % early_stopping_check_frequency == 0:\n",
    "                    validation_acc = self.prediction_accuracy(valid_X, valid_y)\n",
    "                    print(\"validation accuracy\", validation_acc)\n",
    "                    if validation_acc > best_validation_acc:\n",
    "                        saver.save(sess, early_stopping_checkpoint_path)\n",
    "                        best_validation_acc = validation_acc\n",
    "                        best_validation_step = step\n",
    "                    elif step >= (best_validation_step + early_stopping_check_limit):\n",
    "                        print(\"Stopping early during epoch\", epoch)\n",
    "                        break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            save_path = saver.save(sess, interim_checkpoint_path)\n",
    "        saver.restore(sess, early_stopping_checkpoint_path)\n",
    "        save_path = saver.save(sess, \"./checkpoints/mnist_cnn_model_final.ckpt\")\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        dataset_size = X.shape[0]\n",
    "        #print \"dataset_size: \", dataset_size, \" batch_size: \", batch_size\n",
    "        predictions = np.ndarray(shape=(dataset_size, self.n_output), dtype=np.float32)\n",
    "        steps = int(math.ceil(dataset_size / self.batch_size))\n",
    "        #print \"steps: \", steps\n",
    "        for step in range(steps):\n",
    "            offset = (step * self.batch_size)\n",
    "            #print \"offset \", offset\n",
    "            data_end_index = min(offset + self.batch_size, dataset_size)\n",
    "            batch_data = X[offset:data_end_index, :]\n",
    "            feed_dict = {\n",
    "                self.x: batch_data\n",
    "            }\n",
    "            predictions[offset:data_end_index, :] = self.evaluation.eval(session=self.session, feed_dict=feed_dict)\n",
    "        #print(\"predict_proba\", predictions)\n",
    "        return predictions\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "    \n",
    "    def _prediction_accuracy(self, predictions, labels):\n",
    "        return (100.0 * np.sum(np.argmax(predictions, 1) == labels)\n",
    "              / predictions.shape[0])\n",
    "    \n",
    "    def prediction_accuracy(self, X, y):\n",
    "        predictions = self.predict_proba(X)\n",
    "        return self._prediction_accuracy(predictions, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
