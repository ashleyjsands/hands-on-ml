{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.layers import batch_norm, dropout\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_normal_initialisation(n_inputs, n_outputs):\n",
    "    stddev = np.power(2 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.truncated_normal((n_inputs, n_outputs), stddev=stddev)\n",
    "\n",
    "def he_uniform_initialisation(n_inputs, n_outputs):\n",
    "    r = np.power(6 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.random_uniform((n_inputs, n_outputs), -r, r)\n",
    "\n",
    "def neuron_layer(X, n_neurons, name):\n",
    "    with tf.name_scope(name):\n",
    "        #print(X.get_shape()[1])\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        W = tf.Variable(he_normal_initialisation(n_inputs, n_neurons), name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        return tf.nn.elu(z)\n",
    "\n",
    "def cnn_layer(X, patch_size, n_input_filters, n_filters, name, initialised_weights_stddev = 0.1):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, n_input_filters, n_filters], stddev=initialised_weights_stddev))\n",
    "        b = tf.Variable(tf.zeros([n_filters]))\n",
    "        return tf.nn.elu(tf.nn.conv2d(X, w, strides=[1, 2, 2, 1], padding=\"SAME\") + b)\n",
    "\n",
    "input_spatial_size = 28\n",
    "input_channels = 1\n",
    "n_filters_per_layer = [10, 10]\n",
    "patch_size = 3\n",
    "n_output = 10\n",
    "batch_size = 200\n",
    "    \n",
    "with tf.device(\"/gpu:0\"):\n",
    "    x = tf.placeholder(tf.float32, shape=(batch_size, input_spatial_size ** 2), name=\"input\")\n",
    "    reshaped_x = tf.reshape(x, (tf.shape(x)[0], input_spatial_size, input_spatial_size, 1))\n",
    "    y = tf.placeholder(tf.int64, shape=(batch_size), name=\"y\")\n",
    "    \n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        input_tensor = reshaped_x\n",
    "        n_input_filters = input_channels\n",
    "        for i in range(len(n_filters_per_layer)):\n",
    "            input_tensor = cnn_layer(input_tensor, patch_size, n_input_filters, n_filters_per_layer[i], \"hidden\" + str(i + 1))\n",
    "            n_input_filters = n_filters_per_layer[i]\n",
    "        shape = input_tensor.get_shape().as_list()\n",
    "        #print(shape)\n",
    "        reshape = tf.reshape(input_tensor, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        #print(reshape.get_shape())\n",
    "        logits = neuron_layer(reshape, n_output, \"output\")\n",
    "        evaluation = tf.nn.softmax(logits)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "\n",
    "    with tf.name_scope(\"training\"):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    k = 1\n",
    "    correctness = tf.nn.in_top_k(logits, y, k)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctness, tf.float32)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "interim_checkpoint_path = \"./checkpoints/mnist_cnn_model.ckpt\"\n",
    "early_stopping_checkpoint_path = \"./checkpoints/mnist_cnn_model_early_stopping.ckpt\"\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "loss: 2.30844 train accuracy: 3.5\n",
      "validation accuracy 9.6\n",
      "loss: 2.22732 train accuracy: 51.0\n",
      "loss: 2.0181 train accuracy: 71.0\n",
      "loss: 1.69386 train accuracy: 73.0\n",
      "loss: 1.26418 train accuracy: 77.0\n",
      "loss: 0.95412 train accuracy: 77.0\n",
      "validation accuracy 80.82\n",
      "loss: 0.656738 train accuracy: 84.0\n",
      "loss: 0.576777 train accuracy: 85.0\n",
      "loss: 0.499835 train accuracy: 85.0\n",
      "loss: 0.407467 train accuracy: 88.0\n",
      "loss: 0.385032 train accuracy: 90.0\n",
      "validation accuracy 89.02\n",
      "loss: 0.427316 train accuracy: 89.5\n",
      "loss: 0.363884 train accuracy: 87.5\n",
      "loss: 0.44623 train accuracy: 87.0\n",
      "loss: 0.342677 train accuracy: 92.5\n",
      "loss: 0.345295 train accuracy: 88.0\n",
      "validation accuracy 90.42\n",
      "loss: 0.363489 train accuracy: 88.5\n",
      "loss: 0.312713 train accuracy: 91.0\n",
      "loss: 0.3171 train accuracy: 91.5\n",
      "loss: 0.250527 train accuracy: 93.0\n",
      "loss: 0.368013 train accuracy: 88.5\n",
      "validation accuracy 90.84\n",
      "loss: 0.4191 train accuracy: 91.5\n",
      "loss: 0.348346 train accuracy: 92.5\n",
      "loss: 0.46249 train accuracy: 85.5\n",
      "loss: 0.28662 train accuracy: 93.5\n",
      "loss: 0.275847 train accuracy: 91.0\n",
      "validation accuracy 91.64\n",
      "loss: 0.338498 train accuracy: 90.5\n",
      "loss: 0.216432 train accuracy: 92.0\n",
      "epoch 1\n",
      "loss: 0.297873 train accuracy: 90.5\n",
      "validation accuracy 92.08\n",
      "loss: 0.35662 train accuracy: 88.5\n",
      "loss: 0.286776 train accuracy: 92.0\n",
      "loss: 0.317218 train accuracy: 90.5\n",
      "loss: 0.297067 train accuracy: 91.5\n",
      "loss: 0.248182 train accuracy: 91.5\n",
      "validation accuracy 92.66\n",
      "loss: 0.328055 train accuracy: 93.5\n",
      "loss: 0.425942 train accuracy: 88.5\n",
      "loss: 0.286986 train accuracy: 92.0\n",
      "loss: 0.231478 train accuracy: 94.0\n",
      "loss: 0.258396 train accuracy: 93.0\n",
      "validation accuracy 92.74\n",
      "loss: 0.255646 train accuracy: 91.5\n",
      "loss: 0.248156 train accuracy: 93.5\n",
      "loss: 0.198346 train accuracy: 95.5\n",
      "loss: 0.283489 train accuracy: 93.0\n",
      "loss: 0.254809 train accuracy: 93.0\n",
      "validation accuracy 93.26\n",
      "loss: 0.299233 train accuracy: 89.5\n",
      "loss: 0.235322 train accuracy: 95.5\n",
      "loss: 0.281377 train accuracy: 90.5\n",
      "loss: 0.307674 train accuracy: 93.5\n",
      "loss: 0.23906 train accuracy: 92.0\n",
      "validation accuracy 93.56\n",
      "loss: 0.207392 train accuracy: 94.0\n",
      "loss: 0.231278 train accuracy: 94.0\n",
      "loss: 0.285775 train accuracy: 93.5\n",
      "loss: 0.24434 train accuracy: 91.5\n",
      "loss: 0.328863 train accuracy: 92.5\n",
      "validation accuracy 93.96\n",
      "loss: 0.185231 train accuracy: 94.0\n",
      "loss: 0.219538 train accuracy: 92.5\n",
      "epoch 2\n",
      "loss: 0.230997 train accuracy: 93.0\n",
      "validation accuracy 94.16\n",
      "loss: 0.248507 train accuracy: 95.0\n",
      "loss: 0.178284 train accuracy: 94.5\n",
      "loss: 0.255544 train accuracy: 91.5\n",
      "loss: 0.205052 train accuracy: 92.0\n",
      "loss: 0.318807 train accuracy: 91.5\n",
      "validation accuracy 94.52\n",
      "loss: 0.269193 train accuracy: 92.5\n",
      "loss: 0.226166 train accuracy: 94.0\n",
      "loss: 0.160432 train accuracy: 94.5\n",
      "loss: 0.209932 train accuracy: 92.5\n",
      "loss: 0.300088 train accuracy: 93.0\n",
      "validation accuracy 94.92\n",
      "loss: 0.206966 train accuracy: 94.0\n",
      "loss: 0.120547 train accuracy: 97.5\n",
      "loss: 0.153216 train accuracy: 94.0\n",
      "loss: 0.19689 train accuracy: 93.5\n",
      "loss: 0.148837 train accuracy: 94.5\n",
      "validation accuracy 95.14\n",
      "loss: 0.168877 train accuracy: 96.5\n",
      "loss: 0.171199 train accuracy: 95.0\n",
      "loss: 0.206183 train accuracy: 94.0\n",
      "loss: 0.225281 train accuracy: 94.0\n",
      "loss: 0.177569 train accuracy: 94.0\n",
      "validation accuracy 95.34\n",
      "loss: 0.178717 train accuracy: 93.0\n",
      "loss: 0.153808 train accuracy: 95.5\n",
      "loss: 0.132797 train accuracy: 96.5\n",
      "loss: 0.24148 train accuracy: 93.0\n",
      "loss: 0.235297 train accuracy: 94.5\n",
      "validation accuracy 95.54\n",
      "loss: 0.119603 train accuracy: 96.0\n",
      "loss: 0.165269 train accuracy: 95.0\n",
      "epoch 3\n",
      "loss: 0.224411 train accuracy: 94.5\n",
      "validation accuracy 95.68\n",
      "loss: 0.107907 train accuracy: 96.5\n",
      "loss: 0.139313 train accuracy: 94.5\n",
      "loss: 0.129121 train accuracy: 95.0\n",
      "loss: 0.167047 train accuracy: 96.0\n",
      "loss: 0.220274 train accuracy: 92.0\n",
      "validation accuracy 95.56\n",
      "loss: 0.231888 train accuracy: 94.5\n",
      "loss: 0.185063 train accuracy: 96.0\n",
      "loss: 0.134283 train accuracy: 96.0\n",
      "loss: 0.126986 train accuracy: 96.0\n",
      "loss: 0.118515 train accuracy: 96.0\n",
      "validation accuracy 95.96\n",
      "loss: 0.149111 train accuracy: 94.5\n",
      "loss: 0.210457 train accuracy: 94.5\n",
      "loss: 0.148027 train accuracy: 94.5\n",
      "loss: 0.146464 train accuracy: 94.5\n",
      "loss: 0.127407 train accuracy: 96.5\n",
      "validation accuracy 95.82\n",
      "loss: 0.141956 train accuracy: 96.5\n",
      "loss: 0.18291 train accuracy: 96.0\n",
      "loss: 0.29247 train accuracy: 92.0\n",
      "loss: 0.156714 train accuracy: 95.0\n",
      "loss: 0.133934 train accuracy: 98.0\n",
      "validation accuracy 95.98\n",
      "loss: 0.177523 train accuracy: 97.0\n",
      "loss: 0.177999 train accuracy: 94.5\n",
      "loss: 0.110464 train accuracy: 97.0\n",
      "loss: 0.168764 train accuracy: 94.5\n",
      "loss: 0.152836 train accuracy: 95.0\n",
      "validation accuracy 96.46\n",
      "loss: 0.170563 train accuracy: 96.0\n",
      "loss: 0.162942 train accuracy: 94.5\n",
      "epoch 4\n",
      "loss: 0.113549 train accuracy: 98.0\n",
      "validation accuracy 96.26\n",
      "loss: 0.0994803 train accuracy: 97.0\n",
      "loss: 0.109437 train accuracy: 97.0\n",
      "loss: 0.139368 train accuracy: 95.5\n",
      "loss: 0.186359 train accuracy: 95.5\n",
      "loss: 0.122867 train accuracy: 97.0\n",
      "validation accuracy 96.32\n",
      "loss: 0.187338 train accuracy: 95.0\n",
      "loss: 0.174729 train accuracy: 96.0\n",
      "loss: 0.142208 train accuracy: 97.5\n",
      "loss: 0.142062 train accuracy: 94.5\n",
      "loss: 0.141791 train accuracy: 96.0\n",
      "validation accuracy 96.34\n",
      "loss: 0.157912 train accuracy: 94.5\n",
      "loss: 0.132155 train accuracy: 97.0\n",
      "loss: 0.104978 train accuracy: 97.5\n",
      "loss: 0.125783 train accuracy: 96.5\n",
      "loss: 0.127865 train accuracy: 97.0\n",
      "validation accuracy 96.46\n",
      "loss: 0.101648 train accuracy: 95.0\n",
      "loss: 0.151227 train accuracy: 95.5\n",
      "loss: 0.153548 train accuracy: 96.5\n",
      "loss: 0.0968534 train accuracy: 97.0\n",
      "loss: 0.104551 train accuracy: 96.5\n",
      "validation accuracy 96.48\n",
      "loss: 0.0909465 train accuracy: 96.5\n",
      "loss: 0.140675 train accuracy: 94.5\n",
      "loss: 0.0793294 train accuracy: 96.5\n",
      "loss: 0.152391 train accuracy: 94.5\n",
      "loss: 0.12408 train accuracy: 97.0\n",
      "validation accuracy 96.72\n",
      "loss: 0.0840105 train accuracy: 97.0\n",
      "loss: 0.0982043 train accuracy: 98.0\n",
      "epoch 5\n",
      "loss: 0.107278 train accuracy: 97.0\n",
      "validation accuracy 96.96\n",
      "loss: 0.168974 train accuracy: 94.5\n",
      "loss: 0.121258 train accuracy: 97.0\n",
      "loss: 0.10075 train accuracy: 96.5\n",
      "loss: 0.172495 train accuracy: 95.5\n",
      "loss: 0.0948239 train accuracy: 97.5\n",
      "validation accuracy 96.76\n",
      "loss: 0.08003 train accuracy: 96.5\n",
      "loss: 0.162058 train accuracy: 96.0\n",
      "loss: 0.16103 train accuracy: 95.5\n",
      "loss: 0.259596 train accuracy: 93.5\n",
      "loss: 0.0730165 train accuracy: 97.5\n",
      "validation accuracy 96.92\n",
      "loss: 0.145908 train accuracy: 95.5\n",
      "loss: 0.103802 train accuracy: 96.5\n",
      "loss: 0.083032 train accuracy: 98.5\n",
      "loss: 0.126552 train accuracy: 96.5\n",
      "loss: 0.112299 train accuracy: 97.0\n",
      "validation accuracy 97.0\n",
      "loss: 0.164001 train accuracy: 95.0\n",
      "loss: 0.152115 train accuracy: 96.0\n",
      "loss: 0.035074 train accuracy: 98.5\n",
      "loss: 0.0673339 train accuracy: 98.0\n",
      "loss: 0.0695655 train accuracy: 97.5\n",
      "validation accuracy 97.02\n",
      "loss: 0.193784 train accuracy: 94.0\n",
      "loss: 0.075302 train accuracy: 97.0\n",
      "loss: 0.123306 train accuracy: 97.0\n",
      "loss: 0.100251 train accuracy: 97.5\n",
      "loss: 0.163903 train accuracy: 94.0\n",
      "validation accuracy 96.84\n",
      "loss: 0.077928 train accuracy: 97.5\n",
      "loss: 0.0619686 train accuracy: 97.5\n",
      "epoch 6\n",
      "loss: 0.109242 train accuracy: 96.0\n",
      "validation accuracy 97.0\n",
      "loss: 0.0528601 train accuracy: 99.0\n",
      "loss: 0.0753326 train accuracy: 97.0\n",
      "loss: 0.0550791 train accuracy: 98.0\n",
      "loss: 0.12057 train accuracy: 95.5\n",
      "loss: 0.0610875 train accuracy: 97.0\n",
      "validation accuracy 97.22\n",
      "loss: 0.0539393 train accuracy: 98.5\n",
      "loss: 0.0728973 train accuracy: 98.0\n",
      "loss: 0.097876 train accuracy: 95.5\n",
      "loss: 0.0769328 train accuracy: 97.5\n",
      "loss: 0.0934892 train accuracy: 98.0\n",
      "validation accuracy 97.08\n",
      "loss: 0.14439 train accuracy: 96.5\n",
      "loss: 0.105926 train accuracy: 95.0\n",
      "loss: 0.151939 train accuracy: 96.0\n",
      "loss: 0.12898 train accuracy: 97.0\n",
      "loss: 0.0659329 train accuracy: 98.0\n",
      "validation accuracy 97.4\n"
     ]
    }
   ],
   "source": [
    "def eval_predictions(session, data):\n",
    "    dataset_size = data.shape[0]\n",
    "    #print \"dataset_size: \", dataset_size, \" batch_size: \", batch_size\n",
    "    if dataset_size % batch_size != 0:\n",
    "        raise \"batch_size must be a multiple of dataset_size.\"\n",
    "    predictions = np.ndarray(shape=(dataset_size, n_output), dtype=np.float32)\n",
    "    steps = dataset_size // batch_size\n",
    "    #print \"steps: \", steps\n",
    "    for step in range(steps):\n",
    "        offset = (step * batch_size)\n",
    "        #print \"offset \", offset\n",
    "        batch_data = data[offset:(offset + batch_size), :]\n",
    "        feed_dict = {\n",
    "            x: batch_data\n",
    "        }\n",
    "        predictions[offset:offset+batch_size, :] = session.run(evaluation, feed_dict=feed_dict)\n",
    "    return predictions\n",
    "\n",
    "def prediction_accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == labels)\n",
    "          / predictions.shape[0])\n",
    "\n",
    "epochs = 20\n",
    "n_batches = int(np.ceil(mnist.train.num_examples // batch_size))\n",
    "\n",
    "early_stopping_check_frequency = batch_size // 4\n",
    "early_stopping_check_limit = batch_size * 2\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init)\n",
    "    #saver.restore(sess, interim_checkpoint_path)\n",
    "    \n",
    "    best_validation_acc = 0.0\n",
    "    best_validation_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch\", epoch)\n",
    "        for batch_index in range(n_batches):\n",
    "            step = epoch * n_batches + batch_index\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = summary_op.eval(feed_dict={x: X_batch, y: y_batch})\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            t, l, a = sess.run([training_op, loss, accuracy], feed_dict={x: X_batch, y: y_batch})\n",
    "            if batch_index % 10 == 0: print(\"loss:\", l, \"train accuracy:\", a)\n",
    "            # Early stopping check\n",
    "            if batch_index % early_stopping_check_frequency == 0:\n",
    "                predictions = eval_predictions(sess, mnist.validation.images)\n",
    "                validation_acc = prediction_accuracy(predictions, mnist.validation.labels)\n",
    "                print(\"validation accuracy\", validation_acc)\n",
    "                if validation_acc > best_validation_acc:\n",
    "                    saver.save(sess, early_stopping_checkpoint_path)\n",
    "                    best_validation_acc = validation_acc\n",
    "                    best_validation_step = step\n",
    "                elif step >= (best_validation_step + early_stopping_check_limit):\n",
    "                    print(\"Stopping early during epoch\", epoch)\n",
    "                    break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    save_path = saver.save(sess, interim_checkpoint_path)\n",
    "    test_predictions = eval_predictions(sess, mnist.test.images)\n",
    "    test_acc = prediction_accuracy(predictions, mnist.test.labels)\n",
    "    print(\">>>>>>>>>> test dataset accuracy:\", test_acc)\n",
    "\n",
    "    save_path = saver.save(sess, \"./checkpoints/mnist_cnn_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
