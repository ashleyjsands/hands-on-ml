{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.layers import batch_norm, dropout\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_normal_initialisation(n_inputs, n_outputs):\n",
    "    stddev = np.power(2 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.truncated_normal((n_inputs, n_outputs), stddev=stddev)\n",
    "\n",
    "def he_uniform_initialisation(n_inputs, n_outputs):\n",
    "    r = np.power(6 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.random_uniform((n_inputs, n_outputs), -r, r)\n",
    "\n",
    "def neuron_layer(X, n_neurons, name):\n",
    "    with tf.name_scope(name):\n",
    "        #print(X.get_shape()[1])\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        W = tf.Variable(he_normal_initialisation(n_inputs, n_neurons), name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        return tf.nn.elu(z)\n",
    "\n",
    "def cnn_layer(X, patch_size, n_input_filters, n_filters, name, initialised_weights_stddev = 0.05):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, n_input_filters, n_filters], stddev=initialised_weights_stddev))\n",
    "        b = tf.Variable(tf.zeros([n_filters]))\n",
    "        m = tf.nn.elu(tf.nn.conv2d(X, w, strides=[1, 2, 2, 1], padding=\"SAME\") + b)\n",
    "        return tf.nn.local_response_normalization(m, depth_radius=7, alpha=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class CnnClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_cnn_layers=3, first_cnn_n_neurons=200, ccn_layer_ratio=0.5, fully_connected_neurons = 1200,\n",
    "                learning_rate=0.01):\n",
    "        self.n_cnn_layers = n_cnn_layers\n",
    "        self.first_cnn_n_neurons = first_cnn_n_neurons\n",
    "        self.ccn_layer_ratio = ccn_layer_ratio\n",
    "        self.fully_connected_neurons = fully_connected_neurons\n",
    "        self.learning_rate = learning_rate\n",
    "        self._build_graph()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        input_spatial_size = 28\n",
    "        input_channels = 1\n",
    "        n_filters_per_layer = [int(self.first_cnn_n_neurons * (self.ccn_layer_ratio ** i)) for i in range(self.n_cnn_layers)]\n",
    "        print(\"n_filters_per_layer:\", n_filters_per_layer)\n",
    "        patch_size = 3\n",
    "        self.n_output = 10\n",
    "        self.batch_size = 200\n",
    "        \n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            self.x = tf.placeholder(tf.float32, shape=(self.batch_size, input_spatial_size ** 2), name=\"input\")\n",
    "            reshaped_x = tf.reshape(self.x, (tf.shape(self.x)[0], input_spatial_size, input_spatial_size, 1))\n",
    "            self.y = tf.placeholder(tf.int64, shape=(self.batch_size), name=\"y\")\n",
    "\n",
    "            with tf.name_scope(\"dnn\"):\n",
    "                input_tensor = reshaped_x\n",
    "                n_input_filters = input_channels\n",
    "                for i in range(len(n_filters_per_layer)):\n",
    "                    input_tensor = cnn_layer(input_tensor, patch_size, n_input_filters, n_filters_per_layer[i], \"hidden\" + str(i + 1))\n",
    "                    n_input_filters = n_filters_per_layer[i]\n",
    "                #avg_pool_output = tf.nn.avg_pool(input_tensor, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "                shape = input_tensor.get_shape().as_list()\n",
    "                #print(shape)\n",
    "                reshape = tf.reshape(input_tensor, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                fc = tf.nn.dropout(neuron_layer(reshape, self.fully_connected_neurons, \"fully_connected_one\"), keep_prob=0.4)\n",
    "                #print(reshape.get_shape())\n",
    "                logits = neuron_layer(fc, self.n_output, \"output\")\n",
    "                self.evaluation = tf.nn.softmax(logits)\n",
    "\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=logits)\n",
    "                self.loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "\n",
    "            with tf.name_scope(\"training\"):\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "                self.training_op = optimizer.minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            k = 1\n",
    "            correctness = tf.nn.in_top_k(logits, self.y, k)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correctness, tf.float32)) * 100\n",
    "            \n",
    "        self.init = tf.global_variables_initializer()\n",
    "\n",
    "    def fit(self, X, y, epochs = 20):\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        interim_checkpoint_path = \"./checkpoints/mnist_cnn_model.ckpt\"\n",
    "        early_stopping_checkpoint_path = \"./checkpoints/mnist_cnn_model_early_stopping.ckpt\"\n",
    "\n",
    "        from datetime import datetime\n",
    "\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        root_logdir = \"tf_logs\"\n",
    "        log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "        loss_summary = tf.summary.scalar('loss', self.loss)\n",
    "        accuracy_summary = tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "        summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "        file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "        \n",
    "        n_batches = int(np.ceil(len(X) // self.batch_size))\n",
    "\n",
    "        early_stopping_check_frequency = self.batch_size // 4\n",
    "        early_stopping_check_limit = self.batch_size * 2\n",
    "\n",
    "        sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        self.session = sess\n",
    "        sess.run(self.init)\n",
    "        #saver.restore(sess, interim_checkpoint_path)\n",
    "\n",
    "        best_validation_acc = 0.0\n",
    "        best_validation_step = 0\n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch\", epoch)\n",
    "            for batch_index in range(n_batches):\n",
    "                step = epoch * n_batches + batch_index\n",
    "                # TODO: replace this with code that gets a batch from X and y.\n",
    "                X_batch, y_batch = mnist.train.next_batch(self.batch_size)\n",
    "                if batch_index % 10 == 0:\n",
    "                    summary_str = summary_op.eval(session=sess, feed_dict={self.x: X_batch, self.y: y_batch})\n",
    "                    file_writer.add_summary(summary_str, step)\n",
    "                t, l, a = sess.run([self.training_op, self.loss, self.accuracy], feed_dict={self.x: X_batch, self.y: y_batch})\n",
    "                if batch_index % 10 == 0: print(\"loss:\", l, \"train accuracy:\", a)\n",
    "                # Early stopping check\n",
    "                if batch_index % early_stopping_check_frequency == 0:\n",
    "                    validation_acc = self.prediction_accuracy(mnist.validation.images, mnist.validation.labels)\n",
    "                    print(\"validation accuracy\", validation_acc)\n",
    "                    if validation_acc > best_validation_acc:\n",
    "                        saver.save(sess, early_stopping_checkpoint_path)\n",
    "                        best_validation_acc = validation_acc\n",
    "                        best_validation_step = step\n",
    "                    elif step >= (best_validation_step + early_stopping_check_limit):\n",
    "                        print(\"Stopping early during epoch\", epoch)\n",
    "                        break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "            save_path = saver.save(sess, interim_checkpoint_path)\n",
    "        saver.restore(sess, early_stopping_checkpoint_path)\n",
    "        save_path = saver.save(sess, \"./checkpoints/mnist_cnn_model_final.ckpt\")\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        raise Error(\"Not Implemented\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        dataset_size = X.shape[0]\n",
    "        #print \"dataset_size: \", dataset_size, \" batch_size: \", batch_size\n",
    "        if dataset_size % self.batch_size != 0:\n",
    "            raise \"batch_size must be a multiple of dataset_size.\"\n",
    "        predictions = np.ndarray(shape=(dataset_size, self.n_output), dtype=np.float32)\n",
    "        steps = dataset_size // self.batch_size\n",
    "        #print \"steps: \", steps\n",
    "        for step in range(steps):\n",
    "            offset = (step * self.batch_size)\n",
    "            #print \"offset \", offset\n",
    "            batch_data = X[offset:(offset + self.batch_size), :]\n",
    "            feed_dict = {\n",
    "                self.x: batch_data\n",
    "            }\n",
    "            predictions[offset:offset+self.batch_size, :] = self.evaluation.eval(session=self.session, feed_dict=feed_dict)\n",
    "        return predictions\n",
    "    \n",
    "    def _prediction_accuracy(self, predictions, labels):\n",
    "        return (100.0 * np.sum(np.argmax(predictions, 1) == labels)\n",
    "              / predictions.shape[0])\n",
    "    \n",
    "    def prediction_accuracy(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return self._prediction_accuracy(predictions, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_filters_per_layer: [200, 100, 50]\n",
      "epoch 0\n",
      "loss: 2.30233 train accuracy: 10.0\n",
      "validation accuracy 36.2\n",
      "loss: 0.988802 train accuracy: 75.5\n",
      "loss: 0.795613 train accuracy: 78.5\n",
      "loss: 0.511704 train accuracy: 86.0\n",
      "loss: 0.453489 train accuracy: 87.0\n",
      "loss: 0.594584 train accuracy: 86.0\n",
      "validation accuracy 89.9\n",
      "loss: 0.654343 train accuracy: 87.0\n",
      "loss: 0.180736 train accuracy: 94.5\n",
      "loss: 0.395703 train accuracy: 92.0\n",
      "loss: 0.273213 train accuracy: 93.0\n",
      "loss: 0.297605 train accuracy: 94.5\n",
      "validation accuracy 92.24\n",
      "loss: 0.430427 train accuracy: 88.5\n",
      "loss: 0.473437 train accuracy: 90.0\n",
      "loss: 0.406114 train accuracy: 91.5\n",
      "loss: 0.313312 train accuracy: 94.0\n",
      "loss: 0.155711 train accuracy: 97.0\n",
      "validation accuracy 92.42\n",
      "loss: 0.339202 train accuracy: 93.5\n",
      "loss: 0.475945 train accuracy: 91.0\n",
      "loss: 0.242563 train accuracy: 94.0\n",
      "loss: 0.326255 train accuracy: 93.0\n",
      "loss: 0.257056 train accuracy: 95.0\n",
      "validation accuracy 94.08\n",
      "loss: 0.52771 train accuracy: 91.5\n",
      "loss: 0.281026 train accuracy: 93.0\n",
      "loss: 0.573154 train accuracy: 91.0\n",
      "loss: 0.489486 train accuracy: 92.5\n",
      "loss: 0.450793 train accuracy: 91.5\n",
      "validation accuracy 93.38\n",
      "loss: 0.558341 train accuracy: 90.5\n",
      "loss: 0.519865 train accuracy: 87.0\n",
      "epoch 1\n",
      "loss: 0.241836 train accuracy: 95.0\n",
      "validation accuracy 94.46\n",
      "loss: 0.449736 train accuracy: 93.0\n",
      "loss: 0.480367 train accuracy: 95.0\n",
      "loss: 0.60337 train accuracy: 90.0\n",
      "loss: 0.281353 train accuracy: 95.5\n",
      "loss: 0.356277 train accuracy: 94.0\n",
      "validation accuracy 94.22\n",
      "loss: 0.648145 train accuracy: 92.5\n",
      "loss: 0.335947 train accuracy: 94.0\n",
      "loss: 0.546751 train accuracy: 96.5\n",
      "loss: 0.511535 train accuracy: 93.0\n",
      "loss: 0.603192 train accuracy: 94.5\n",
      "validation accuracy 93.18\n",
      "loss: 0.237858 train accuracy: 94.0\n",
      "loss: 0.304185 train accuracy: 95.0\n",
      "loss: 0.177591 train accuracy: 97.5\n",
      "loss: 0.376949 train accuracy: 94.5\n",
      "loss: 0.486432 train accuracy: 91.5\n",
      "validation accuracy 93.44\n",
      "loss: 0.433489 train accuracy: 94.5\n",
      "loss: 0.284033 train accuracy: 95.0\n",
      "loss: 0.223881 train accuracy: 95.0\n",
      "loss: 0.389573 train accuracy: 96.0\n",
      "loss: 0.641861 train accuracy: 95.0\n",
      "validation accuracy 95.88\n",
      "loss: 0.339389 train accuracy: 93.5\n",
      "loss: 0.514718 train accuracy: 93.0\n",
      "loss: 0.231196 train accuracy: 97.5\n",
      "loss: 0.7219 train accuracy: 92.0\n",
      "loss: 0.440174 train accuracy: 96.0\n",
      "validation accuracy 94.56\n",
      "loss: 0.162051 train accuracy: 96.5\n",
      "loss: 0.324668 train accuracy: 95.0\n",
      "epoch 2\n",
      "loss: 0.224643 train accuracy: 98.0\n",
      "validation accuracy 95.08\n",
      "loss: 0.351178 train accuracy: 97.0\n",
      "loss: 0.426473 train accuracy: 93.0\n",
      "loss: 0.202836 train accuracy: 96.5\n",
      "loss: 0.307534 train accuracy: 96.5\n",
      "loss: 0.410264 train accuracy: 94.0\n",
      "validation accuracy 94.28\n",
      "loss: 0.210415 train accuracy: 97.0\n",
      "loss: 0.377175 train accuracy: 95.5\n",
      "loss: 0.338903 train accuracy: 96.0\n",
      "loss: 0.528638 train accuracy: 95.5\n",
      "loss: 0.562903 train accuracy: 93.5\n",
      "validation accuracy 94.66\n",
      "loss: 0.467781 train accuracy: 95.5\n",
      "loss: 0.195366 train accuracy: 96.5\n",
      "loss: 0.438461 train accuracy: 97.0\n",
      "loss: 0.457666 train accuracy: 95.0\n",
      "loss: 0.35676 train accuracy: 94.5\n",
      "validation accuracy 95.88\n",
      "loss: 0.855847 train accuracy: 92.5\n",
      "loss: 0.360229 train accuracy: 96.5\n",
      "loss: 0.894453 train accuracy: 95.0\n",
      "loss: 0.705881 train accuracy: 93.5\n",
      "loss: 0.263249 train accuracy: 95.0\n",
      "validation accuracy 95.94\n",
      "loss: 0.303701 train accuracy: 95.0\n",
      "loss: 0.123209 train accuracy: 97.0\n",
      "loss: 0.18481 train accuracy: 96.5\n",
      "loss: 0.371066 train accuracy: 95.5\n",
      "loss: 0.310043 train accuracy: 94.5\n",
      "validation accuracy 95.64\n",
      "loss: 0.241795 train accuracy: 96.0\n",
      "loss: 0.158275 train accuracy: 97.5\n",
      "epoch 3\n",
      "loss: 0.413254 train accuracy: 97.0\n",
      "validation accuracy 95.96\n",
      "loss: 0.356481 train accuracy: 97.0\n",
      "loss: 0.163659 train accuracy: 98.5\n",
      "loss: 1.22031 train accuracy: 93.0\n",
      "loss: 0.478631 train accuracy: 95.5\n",
      "loss: 0.149056 train accuracy: 97.0\n",
      "validation accuracy 95.8\n",
      "loss: 0.710418 train accuracy: 95.0\n",
      "loss: 0.476898 train accuracy: 96.5\n",
      "loss: 0.235857 train accuracy: 98.0\n",
      "loss: 0.448375 train accuracy: 96.0\n",
      "loss: 0.541137 train accuracy: 92.5\n",
      "validation accuracy 96.24\n",
      "loss: 0.36549 train accuracy: 94.5\n",
      "loss: 0.383826 train accuracy: 95.5\n",
      "loss: 0.148965 train accuracy: 98.5\n",
      "loss: 0.261198 train accuracy: 97.5\n",
      "loss: 0.429753 train accuracy: 96.0\n",
      "validation accuracy 95.84\n",
      "loss: 0.418753 train accuracy: 96.0\n",
      "loss: 0.264852 train accuracy: 97.5\n",
      "loss: 0.168416 train accuracy: 98.0\n",
      "loss: 0.23088 train accuracy: 97.0\n",
      "loss: 0.222397 train accuracy: 95.5\n",
      "validation accuracy 95.84\n",
      "loss: 0.582185 train accuracy: 93.0\n",
      "loss: 0.169267 train accuracy: 98.5\n",
      "loss: 0.277875 train accuracy: 96.5\n",
      "loss: 0.171038 train accuracy: 96.5\n",
      "loss: 0.602536 train accuracy: 95.5\n",
      "validation accuracy 96.06\n",
      "loss: 0.302475 train accuracy: 97.0\n",
      "loss: 0.190205 train accuracy: 99.0\n",
      "epoch 4\n",
      "loss: 0.261482 train accuracy: 97.0\n",
      "validation accuracy 95.78\n",
      "loss: 0.164661 train accuracy: 99.0\n",
      "loss: 0.12564 train accuracy: 98.0\n",
      "loss: 0.422352 train accuracy: 95.0\n",
      "loss: 0.515839 train accuracy: 97.0\n",
      "loss: 0.327652 train accuracy: 95.0\n",
      "validation accuracy 95.76\n",
      "loss: 0.390661 train accuracy: 97.0\n",
      "loss: 0.213917 train accuracy: 98.5\n",
      "loss: 0.179567 train accuracy: 96.0\n",
      "loss: 0.299619 train accuracy: 97.5\n",
      "loss: 0.417406 train accuracy: 98.0\n",
      "validation accuracy 94.88\n",
      "loss: 0.0376622 train accuracy: 100.0\n",
      "loss: 0.245414 train accuracy: 96.5\n",
      "loss: 0.481488 train accuracy: 96.0\n",
      "loss: 0.112304 train accuracy: 98.5\n",
      "loss: 0.59759 train accuracy: 95.0\n",
      "validation accuracy 96.44\n",
      "loss: 0.362026 train accuracy: 94.5\n",
      "loss: 0.220192 train accuracy: 97.5\n",
      "loss: 0.317922 train accuracy: 98.0\n",
      "loss: 0.334965 train accuracy: 94.5\n",
      "loss: 0.508154 train accuracy: 96.0\n",
      "validation accuracy 96.16\n",
      "loss: 0.557664 train accuracy: 97.0\n",
      "loss: 0.282144 train accuracy: 96.5\n",
      "loss: 0.120778 train accuracy: 98.0\n",
      "loss: 0.744656 train accuracy: 97.5\n",
      "loss: 0.217338 train accuracy: 98.5\n",
      "validation accuracy 95.66\n",
      "loss: 0.358931 train accuracy: 97.5\n",
      "loss: 0.342712 train accuracy: 97.5\n",
      "epoch 5\n",
      "loss: 0.483012 train accuracy: 95.5\n",
      "validation accuracy 94.4\n",
      "loss: 0.370807 train accuracy: 98.0\n",
      "loss: 0.249683 train accuracy: 97.5\n",
      "loss: 0.153671 train accuracy: 98.0\n",
      "loss: 0.308759 train accuracy: 97.5\n",
      "loss: 0.139882 train accuracy: 97.0\n",
      "validation accuracy 95.22\n",
      "loss: 0.404242 train accuracy: 97.5\n",
      "loss: 0.253193 train accuracy: 96.5\n",
      "loss: 0.549375 train accuracy: 98.5\n",
      "loss: 0.350794 train accuracy: 97.5\n",
      "loss: 0.391263 train accuracy: 96.5\n",
      "validation accuracy 95.36\n",
      "loss: 0.188122 train accuracy: 98.0\n",
      "loss: 0.204066 train accuracy: 99.5\n",
      "loss: 0.352498 train accuracy: 97.5\n",
      "loss: 0.19838 train accuracy: 98.5\n",
      "loss: 0.251778 train accuracy: 96.0\n",
      "validation accuracy 95.04\n",
      "loss: 0.33847 train accuracy: 98.5\n",
      "loss: 0.362912 train accuracy: 98.5\n",
      "loss: 0.591088 train accuracy: 96.5\n",
      "loss: 0.296998 train accuracy: 97.5\n",
      "loss: 0.349852 train accuracy: 97.0\n",
      "validation accuracy 94.18\n",
      "loss: 0.325921 train accuracy: 98.5\n",
      "loss: 0.462271 train accuracy: 96.5\n",
      "loss: 0.139382 train accuracy: 99.0\n",
      "loss: 0.277285 train accuracy: 98.0\n",
      "loss: 0.171543 train accuracy: 97.5\n",
      "validation accuracy 94.94\n",
      "loss: 0.345307 train accuracy: 99.0\n",
      "loss: 0.337785 train accuracy: 98.0\n",
      "epoch 6\n",
      "loss: 0.52596 train accuracy: 98.5\n",
      "validation accuracy 96.16\n",
      "Stopping early during epoch 6\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/mnist_cnn_model_early_stopping.ckpt\n",
      ">>>>>>>>>> test dataset accuracy: 96.49\n"
     ]
    }
   ],
   "source": [
    "cnn_clf = CnnClassifier()\n",
    "cnn_clf.fit(mnist.train.images, mnist.train.labels)\n",
    "\n",
    "test_acc = cnn_clf.prediction_accuracy(mnist.test.images, mnist.test.labels)\n",
    "print(\">>>>>>>>>> test dataset accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_filters_per_layer: [200, 100, 50]\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "n_filters_per_layer: [200, 100, 50]\n",
      "n_filters_per_layer: [200, 100, 50]\n",
      "[CV] ccn_layer_ratio=0.9, n_cnn_layers=4, learning_rate=0.02, first_cnn_n_neurons=10, fully_connected_neurons=500 \n",
      "epoch 0\n",
      "loss: 2.30279 train accuracy: 6.0\n",
      "validation accuracy 42.12\n",
      "loss: 2.66131 train accuracy: 50.5\n",
      "loss: 1.09941 train accuracy: 70.5\n",
      "loss: 0.818534 train accuracy: 77.5\n",
      "loss: 0.787004 train accuracy: 82.5\n",
      "loss: 0.662137 train accuracy: 83.5\n",
      "validation accuracy 86.14\n",
      "loss: 0.495906 train accuracy: 87.5\n",
      "loss: 0.628606 train accuracy: 83.5\n",
      "loss: 0.40967 train accuracy: 89.0\n",
      "loss: 0.609841 train accuracy: 87.0\n",
      "loss: 0.292426 train accuracy: 92.5\n",
      "validation accuracy 92.12\n",
      "loss: 0.314274 train accuracy: 89.5\n",
      "loss: 0.485306 train accuracy: 87.5\n",
      "loss: 0.21564 train accuracy: 94.5\n",
      "loss: 0.456117 train accuracy: 91.5\n",
      "loss: 0.368529 train accuracy: 92.0\n",
      "validation accuracy 92.3\n",
      "loss: 0.205393 train accuracy: 93.0\n",
      "loss: 0.289285 train accuracy: 95.0\n",
      "loss: 0.373139 train accuracy: 94.0\n",
      "epoch 1\n",
      "loss: 0.367205 train accuracy: 93.0\n",
      "validation accuracy 92.86\n",
      "loss: 0.326077 train accuracy: 94.5\n",
      "loss: 0.493387 train accuracy: 92.0\n",
      "loss: 0.277496 train accuracy: 94.0\n",
      "loss: 0.263913 train accuracy: 93.5\n",
      "loss: 0.552001 train accuracy: 91.5\n",
      "validation accuracy 94.32\n",
      "loss: 0.394586 train accuracy: 91.5\n",
      "loss: 0.309121 train accuracy: 94.0\n",
      "loss: 0.467925 train accuracy: 91.5\n",
      "loss: 0.272117 train accuracy: 94.5\n",
      "loss: 0.47371 train accuracy: 94.5\n",
      "validation accuracy 95.18\n",
      "loss: 0.186101 train accuracy: 95.5\n",
      "loss: 0.415576 train accuracy: 93.5\n",
      "loss: 0.212273 train accuracy: 95.0\n",
      "loss: 0.434539 train accuracy: 93.0\n",
      "loss: 0.322224 train accuracy: 93.5\n",
      "validation accuracy 93.26\n",
      "loss: 0.363122 train accuracy: 93.0\n",
      "loss: 0.355156 train accuracy: 93.0\n",
      "loss: 0.257881 train accuracy: 97.0\n",
      "epoch 2\n",
      "loss: 0.2946 train accuracy: 95.5\n",
      "validation accuracy 94.58\n",
      "loss: 0.141587 train accuracy: 96.5\n",
      "loss: 0.528812 train accuracy: 93.5\n",
      "loss: 0.669593 train accuracy: 87.0\n",
      "loss: 0.141185 train accuracy: 97.0\n",
      "loss: 0.310916 train accuracy: 94.0\n",
      "validation accuracy 93.6\n",
      "loss: 0.374536 train accuracy: 93.0\n",
      "loss: 0.53067 train accuracy: 93.5\n",
      "loss: 0.447569 train accuracy: 90.5\n",
      "loss: 0.336984 train accuracy: 96.0\n",
      "loss: 0.367312 train accuracy: 94.0\n",
      "validation accuracy 95.5\n",
      "loss: 0.262702 train accuracy: 96.5\n",
      "loss: 0.212338 train accuracy: 97.5\n",
      "loss: 0.247507 train accuracy: 96.0\n",
      "loss: 0.470954 train accuracy: 94.5\n",
      "loss: 0.821734 train accuracy: 93.0\n",
      "validation accuracy 95.06\n",
      "loss: 0.422216 train accuracy: 95.0\n",
      "loss: 0.158172 train accuracy: 97.0\n",
      "loss: 0.388305 train accuracy: 94.5\n",
      "epoch 3\n",
      "loss: 0.274704 train accuracy: 95.5\n",
      "validation accuracy 94.82\n",
      "loss: 0.246934 train accuracy: 96.0\n",
      "loss: 0.190671 train accuracy: 96.0\n",
      "loss: 0.213705 train accuracy: 97.5\n",
      "loss: 0.318009 train accuracy: 95.0\n",
      "loss: 0.501598 train accuracy: 96.0\n",
      "validation accuracy 96.0\n",
      "loss: 0.367391 train accuracy: 96.0\n",
      "loss: 0.361955 train accuracy: 94.5\n",
      "loss: 0.274628 train accuracy: 95.5\n",
      "loss: 0.498042 train accuracy: 94.0\n",
      "loss: 0.330965 train accuracy: 96.5\n",
      "validation accuracy 94.3\n",
      "loss: 0.328442 train accuracy: 96.0\n",
      "loss: 0.477871 train accuracy: 95.0\n",
      "loss: 0.312691 train accuracy: 95.5\n",
      "loss: 0.377367 train accuracy: 94.5\n",
      "loss: 0.667741 train accuracy: 95.5\n",
      "validation accuracy 95.42\n",
      "loss: 0.274662 train accuracy: 94.5\n",
      "loss: 0.357264 train accuracy: 96.0\n",
      "loss: 0.232352 train accuracy: 95.0\n",
      "epoch 4\n",
      "loss: 0.489146 train accuracy: 91.0\n",
      "validation accuracy 95.0\n",
      "loss: 0.543344 train accuracy: 92.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_cnn_layers\": range(1, 15),\n",
    "    \"first_cnn_n_neurons\": [10, 100, 200, 400, 800, 1000, 1200, 2000],\n",
    "    \"ccn_layer_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    \"fully_connected_neurons\": [200, 500, 1000, 1200, 1500, 2000, 3000],\n",
    "    \"learning_rate\": [0.001, 0.005, 0.01, 0.02, 0.05],\n",
    "    #\"batch_size\": [10, 50, 100, 500],\n",
    "    #\"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(CnnClassifier(), param_distribs, n_iter=100,\n",
    "                                #fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000},\n",
    "                                random_state=42, verbose=2)\n",
    "rnd_search.fit(mnist.train.images, mnist.train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.system(\"beep -f 555 -l 460\")\n",
    "os.system(\"shutdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a FC layer doesn't seem to be improving the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
