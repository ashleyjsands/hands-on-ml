{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8\n",
    "===\n",
    "Let's create an autoencoder network that pretrains on MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.layers import batch_norm, dropout\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import tensorflow as tf\n",
    "\n",
    "def he_normal_initialisation(n_inputs, n_outputs):\n",
    "    stddev = np.power(2 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.truncated_normal((n_inputs, n_outputs), stddev=stddev)\n",
    "\n",
    "def he_uniform_initialisation(n_inputs, n_outputs):\n",
    "    r = np.power(6 / (n_inputs + n_outputs), 1 / np.sqrt(2))\n",
    "    # truncated normal distributions limit the size of the weights, speeding up the training time.\n",
    "    return tf.random_uniform((n_inputs, n_outputs), -r, r)\n",
    "\n",
    "def create_next_batch_fn(data, sequence_lengths, targets, batch_size):\n",
    "    assert len(data) == len(sequence_lengths) and len(data) == len(targets)\n",
    "    current_batch = 0\n",
    "    def next_batch():\n",
    "        nonlocal current_batch\n",
    "        i = current_batch\n",
    "        #print(current_batch)\n",
    "        current_batch = (current_batch + batch_size) % len(data)\n",
    "        return data[i:i+batch_size], sequence_lengths[i:i+batch_size], targets[i:i+batch_size]\n",
    "    return next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "loss: 1.73753\n",
      "loss: 0.226676\n",
      "loss: 0.153982\n",
      "loss: 0.0909388\n",
      "loss: 0.076949\n",
      "loss: 0.0661807\n",
      "loss: 0.0686202\n",
      "loss: 0.0615951\n",
      "loss: 0.0566764\n",
      "loss: 0.0799454\n",
      "loss: 0.0595585\n",
      "loss: 0.0545579\n",
      "loss: 0.0548278\n",
      "loss: 0.04802\n",
      "loss: 0.0483783\n",
      "loss: 0.0465307\n",
      "loss: 0.0588389\n",
      "loss: 0.0455309\n",
      "loss: 0.0439069\n",
      "loss: 0.0411751\n",
      "loss: 0.0371367\n",
      "loss: 0.0408954\n",
      "loss: 0.0382978\n",
      "loss: 0.0393784\n",
      "loss: 0.041052\n",
      "loss: 0.0407733\n",
      "loss: 0.0385869\n",
      "loss: 0.0367301\n",
      "loss: 0.0354173\n",
      "loss: 0.0354466\n",
      "loss: 0.0331125\n",
      "loss: 0.0352648\n",
      "loss: 0.0343549\n",
      "loss: 0.0327073\n",
      "loss: 0.0311494\n",
      "loss: 0.0316516\n",
      "loss: 0.0318462\n",
      "loss: 0.0287012\n",
      "loss: 0.0309217\n",
      "loss: 0.02783\n",
      "loss: 0.0284884\n",
      "loss: 0.0273773\n",
      "loss: 0.0292854\n",
      "loss: 0.0330542\n",
      "loss: 0.0321057\n",
      "loss: 0.0287997\n",
      "loss: 0.0272759\n",
      "loss: 0.026877\n",
      "loss: 0.0358\n",
      "loss: 0.0305628\n",
      "loss: 0.0272878\n",
      "loss: 0.0294595\n",
      "loss: 0.0267055\n",
      "loss: 0.0233107\n",
      "loss: 0.0296078\n",
      "loss: 0.0282039\n",
      "loss: 0.0246902\n",
      "loss: 0.0259125\n",
      "loss: 0.0245235\n",
      "loss: 0.0292486\n",
      "loss: 0.0262829\n",
      "loss: 0.0236271\n",
      "loss: 0.0256664\n",
      "loss: 0.0254805\n",
      "loss: 0.0250173\n",
      "loss: 0.0253485\n",
      "loss: 0.0205057\n",
      "loss: 0.0250261\n",
      "loss: 0.0228852\n",
      "epoch 1\n",
      "loss: 0.0372861\n",
      "loss: 0.0285693\n",
      "loss: 0.0294229\n",
      "loss: 0.0265964\n",
      "loss: 0.0232872\n",
      "loss: 0.024295\n",
      "loss: 0.0226305\n",
      "loss: 0.0236962\n",
      "loss: 0.0222823\n",
      "loss: 0.0226949\n",
      "loss: 0.0223966\n",
      "loss: 0.0245916\n",
      "loss: 0.0273614\n",
      "loss: 0.0220615\n",
      "loss: 0.0197863\n",
      "loss: 0.0203318\n",
      "loss: 0.0179328\n",
      "loss: 0.0234556\n",
      "loss: 0.0212748\n",
      "loss: 0.0189304\n",
      "loss: 0.0207081\n",
      "loss: 0.020768\n",
      "loss: 0.0252707\n",
      "loss: 0.0202993\n",
      "loss: 0.0207583\n",
      "loss: 0.0210997\n",
      "loss: 0.0187991\n",
      "loss: 0.0188957\n",
      "loss: 0.0204177\n",
      "loss: 0.0201719\n",
      "loss: 0.0186738\n",
      "loss: 0.0201512\n",
      "loss: 0.0190462\n",
      "loss: 0.0172828\n",
      "loss: 0.0176528\n",
      "loss: 0.0210085\n",
      "loss: 0.0181338\n",
      "loss: 0.020312\n",
      "loss: 0.0192056\n",
      "loss: 0.0191222\n",
      "loss: 0.0205046\n",
      "loss: 0.0180221\n",
      "loss: 0.0174652\n",
      "loss: 0.0164843\n",
      "loss: 0.0172837\n",
      "loss: 0.0184955\n",
      "loss: 0.0173013\n",
      "loss: 0.0169771\n",
      "loss: 0.0171773\n",
      "loss: 0.0178963\n",
      "loss: 0.0169871\n",
      "loss: 0.017478\n",
      "loss: 0.0173108\n",
      "loss: 0.0154602\n",
      "loss: 0.0168823\n",
      "loss: 0.0193936\n",
      "loss: 0.0171514\n",
      "loss: 0.0166233\n",
      "loss: 0.0189881\n",
      "loss: 0.0183272\n",
      "loss: 0.0168458\n",
      "loss: 0.0183852\n",
      "loss: 0.0172909\n",
      "loss: 0.0168452\n",
      "loss: 0.0159448\n",
      "loss: 0.0171021\n",
      "loss: 0.0158067\n",
      "loss: 0.0168272\n",
      "loss: 0.0187223\n",
      "epoch 2\n",
      "loss: 0.0159238\n",
      "loss: 0.0147075\n",
      "loss: 0.0142399\n",
      "loss: 0.0145913\n",
      "loss: 0.0158212\n",
      "loss: 0.0173071\n",
      "loss: 0.0155887\n",
      "loss: 0.0139586\n",
      "loss: 0.015211\n",
      "loss: 0.0155812\n",
      "loss: 0.0150177\n",
      "loss: 0.0163526\n",
      "loss: 0.0172291\n",
      "loss: 0.0142396\n",
      "loss: 0.0140045\n",
      "loss: 0.0148858\n",
      "loss: 0.0174134\n",
      "loss: 0.0157709\n",
      "loss: 0.0131622\n",
      "loss: 0.0164653\n",
      "loss: 0.0155896\n",
      "loss: 0.0127145\n",
      "loss: 0.0142077\n",
      "loss: 0.013441\n",
      "loss: 0.0127779\n",
      "loss: 0.0136385\n",
      "loss: 0.0207551\n",
      "loss: 0.0168061\n",
      "loss: 0.0184862\n",
      "loss: 0.0149117\n",
      "loss: 0.0149766\n",
      "loss: 0.0130321\n",
      "loss: 0.0126689\n",
      "loss: 0.0140909\n",
      "loss: 0.0127491\n",
      "loss: 0.0149101\n",
      "loss: 0.0124859\n",
      "loss: 0.0124108\n",
      "loss: 0.0124749\n",
      "loss: 0.0124324\n",
      "loss: 0.0141447\n",
      "loss: 0.0142338\n",
      "loss: 0.0131465\n",
      "loss: 0.0133928\n",
      "loss: 0.0143684\n",
      "loss: 0.0134201\n",
      "loss: 0.0120591\n",
      "loss: 0.0125331\n",
      "loss: 0.0114633\n",
      "loss: 0.0115607\n",
      "loss: 0.0166014\n",
      "loss: 0.0137617\n",
      "loss: 0.013691\n",
      "loss: 0.0129537\n",
      "loss: 0.0114201\n",
      "loss: 0.0132594\n",
      "loss: 0.0129894\n",
      "loss: 0.0114312\n",
      "loss: 0.0127078\n",
      "loss: 0.0120235\n",
      "loss: 0.0141787\n",
      "loss: 0.0126169\n",
      "loss: 0.0118167\n",
      "loss: 0.0116497\n",
      "loss: 0.0108944\n",
      "loss: 0.010748\n",
      "loss: 0.0121514\n",
      "loss: 0.0113761\n",
      "loss: 0.0108213\n",
      "epoch 3\n",
      "loss: 0.0119006\n",
      "loss: 0.0114958\n",
      "loss: 0.0123544\n",
      "loss: 0.0130545\n",
      "loss: 0.0125627\n",
      "loss: 0.013132\n",
      "loss: 0.0111531\n",
      "loss: 0.0112665\n",
      "loss: 0.0110568\n",
      "loss: 0.0125506\n",
      "loss: 0.012225\n",
      "loss: 0.0113147\n",
      "loss: 0.0109606\n",
      "loss: 0.0101265\n",
      "loss: 0.0116255\n",
      "loss: 0.0109051\n",
      "loss: 0.0112683\n",
      "loss: 0.0104744\n",
      "loss: 0.0101134\n",
      "loss: 0.0104542\n",
      "loss: 0.0159198\n",
      "loss: 0.0142266\n",
      "loss: 0.0126202\n",
      "loss: 0.0108703\n",
      "loss: 0.0103667\n",
      "loss: 0.00982351\n",
      "loss: 0.0102547\n",
      "loss: 0.0107456\n",
      "loss: 0.0103922\n",
      "loss: 0.0117743\n",
      "loss: 0.0112033\n",
      "loss: 0.0104911\n",
      "loss: 0.00929386\n",
      "loss: 0.0100021\n",
      "loss: 0.0109499\n",
      "loss: 0.010608\n",
      "loss: 0.00979463\n",
      "loss: 0.0114677\n",
      "loss: 0.0106292\n",
      "loss: 0.0103065\n",
      "loss: 0.00946562\n",
      "loss: 0.0108403\n",
      "loss: 0.0113063\n",
      "loss: 0.0100535\n",
      "loss: 0.0104729\n",
      "loss: 0.0129871\n",
      "loss: 0.0113659\n",
      "loss: 0.0105112\n",
      "loss: 0.011374\n",
      "loss: 0.0104135\n",
      "loss: 0.0096244\n",
      "loss: 0.0132379\n",
      "loss: 0.0104756\n",
      "loss: 0.0107131\n",
      "loss: 0.0105694\n",
      "loss: 0.00965063\n",
      "loss: 0.00989857\n",
      "loss: 0.00983998\n",
      "loss: 0.0106261\n",
      "loss: 0.00799044\n",
      "loss: 0.0105401\n",
      "loss: 0.0097432\n",
      "loss: 0.0110118\n",
      "loss: 0.0102523\n",
      "loss: 0.011469\n",
      "loss: 0.0105406\n",
      "loss: 0.0113948\n",
      "loss: 0.0100563\n",
      "loss: 0.0103448\n",
      "epoch 4\n",
      "loss: 0.010454\n",
      "loss: 0.0127375\n",
      "loss: 0.00974575\n",
      "loss: 0.0100883\n",
      "loss: 0.0088719\n",
      "loss: 0.00944252\n",
      "loss: 0.00953975\n",
      "loss: 0.00845821\n",
      "loss: 0.0132034\n",
      "loss: 0.0103789\n",
      "loss: 0.011701\n",
      "loss: 0.0109805\n",
      "loss: 0.0110735\n",
      "loss: 0.00987594\n",
      "loss: 0.00915623\n",
      "loss: 0.0118562\n",
      "loss: 0.0100778\n",
      "loss: 0.0113329\n",
      "loss: 0.0101151\n",
      "loss: 0.00963346\n",
      "loss: 0.00882455\n",
      "loss: 0.00997582\n",
      "loss: 0.0104655\n",
      "loss: 0.00930521\n",
      "loss: 0.0096801\n",
      "loss: 0.00984132\n",
      "loss: 0.00955144\n",
      "loss: 0.00866567\n",
      "loss: 0.00889642\n",
      "loss: 0.00927294\n",
      "loss: 0.00940337\n",
      "loss: 0.00805262\n",
      "loss: 0.00885241\n",
      "loss: 0.0082657\n",
      "loss: 0.00971478\n",
      "loss: 0.00863987\n",
      "loss: 0.008432\n",
      "loss: 0.00810877\n",
      "loss: 0.00925241\n",
      "loss: 0.00774463\n",
      "loss: 0.00892312\n",
      "loss: 0.00836128\n",
      "loss: 0.00829748\n",
      "loss: 0.00884821\n",
      "loss: 0.00864363\n",
      "loss: 0.00851644\n",
      "loss: 0.00876983\n",
      "loss: 0.00835498\n",
      "loss: 0.0079673\n",
      "loss: 0.00983518\n",
      "loss: 0.00916473\n",
      "loss: 0.00867461\n",
      "loss: 0.00810118\n",
      "loss: 0.00851499\n",
      "loss: 0.00850955\n",
      "loss: 0.00894991\n",
      "loss: 0.00777149\n",
      "loss: 0.00748032\n",
      "loss: 0.00743991\n",
      "loss: 0.00831651\n",
      "loss: 0.00848319\n",
      "loss: 0.00822285\n",
      "loss: 0.0084995\n",
      "loss: 0.00800344\n",
      "loss: 0.00723076\n",
      "loss: 0.0081565\n",
      "loss: 0.0152964\n",
      "loss: 0.211933\n",
      "loss: 336.046\n",
      "epoch 5\n",
      "loss: 93.7732\n",
      "loss: 154.402\n",
      "loss: 31.0818\n",
      "loss: 10.4193\n",
      "loss: 2.92071\n",
      "loss: 1.62521\n",
      "loss: 0.564693\n",
      "loss: 0.237158\n",
      "loss: 0.216687\n",
      "loss: 0.212391\n",
      "loss: 0.259053\n",
      "loss: 0.181909\n",
      "loss: 0.150892\n",
      "loss: 0.165833\n",
      "loss: 0.158637\n",
      "loss: 0.139977\n",
      "loss: 0.148886\n",
      "loss: 0.154368\n",
      "loss: 0.155983\n",
      "loss: 0.139954\n",
      "loss: 0.141689\n",
      "loss: 0.124975\n",
      "loss: 0.147242\n",
      "loss: 0.12055\n",
      "loss: 0.139293\n",
      "loss: 0.130579\n",
      "loss: 0.171299\n",
      "loss: 0.115643\n",
      "loss: 0.116261\n",
      "loss: 0.123659\n",
      "loss: 0.108361\n",
      "loss: 0.102633\n",
      "loss: 0.126205\n",
      "loss: 0.102123\n",
      "loss: 0.102398\n",
      "loss: 0.108274\n",
      "loss: 0.120751\n",
      "loss: 0.116324\n",
      "loss: 0.109663\n",
      "loss: 0.103208\n",
      "loss: 0.0894615\n",
      "loss: 0.100903\n",
      "loss: 0.108828\n",
      "loss: 0.0849911\n",
      "loss: 0.0885306\n",
      "loss: 0.0932746\n",
      "loss: 0.0976305\n",
      "loss: 0.0901788\n",
      "loss: 0.092531\n",
      "loss: 0.0888651\n",
      "loss: 0.0864777\n",
      "loss: 0.0929535\n",
      "loss: 0.0886298\n",
      "loss: 0.093252\n",
      "loss: 0.0815312\n",
      "loss: 0.0887578\n",
      "loss: 0.0802627\n",
      "loss: 0.0791072\n",
      "loss: 0.0838213\n",
      "loss: 0.0893269\n",
      "loss: 0.0833759\n",
      "loss: 0.0802488\n",
      "loss: 0.0794231\n",
      "loss: 0.0824385\n",
      "loss: 0.0840475\n",
      "loss: 0.0786463\n",
      "loss: 0.081166\n",
      "loss: 0.0783719\n",
      "loss: 0.0739885\n",
      "epoch 6\n",
      "loss: 0.0835462\n",
      "loss: 0.0761516\n",
      "loss: 0.0769179\n",
      "loss: 0.0732189\n",
      "loss: 0.10032\n",
      "loss: 0.0800875\n",
      "loss: 0.0810265\n",
      "loss: 0.0777785\n",
      "loss: 0.0779553\n",
      "loss: 0.0783641\n",
      "loss: 0.076939\n",
      "loss: 0.076346\n",
      "loss: 0.0744892\n",
      "loss: 0.0755391\n",
      "loss: 0.0792967\n",
      "loss: 0.0758164\n",
      "loss: 0.0745017\n",
      "loss: 0.0741484\n",
      "loss: 0.0761931\n",
      "loss: 0.0734829\n",
      "loss: 0.074523\n",
      "loss: 0.0766589\n",
      "loss: 0.0716449\n",
      "loss: 0.0704831\n",
      "loss: 0.0711294\n",
      "loss: 0.0707952\n",
      "loss: 0.0716738\n",
      "loss: 0.0745425\n",
      "loss: 0.0713353\n",
      "loss: 0.0741912\n",
      "loss: 0.0741856\n",
      "loss: 0.0732916\n",
      "loss: 0.0737173\n",
      "loss: 0.0721044\n",
      "loss: 0.0778524\n",
      "loss: 0.0718487\n",
      "loss: 0.0744207\n",
      "loss: 0.0786279\n",
      "loss: 0.0704333\n",
      "loss: 0.0735164\n",
      "loss: 0.0702671\n",
      "loss: 0.065928\n",
      "loss: 0.0718077\n",
      "loss: 0.072123\n",
      "loss: 0.0713946\n",
      "loss: 0.0734147\n",
      "loss: 0.069376\n",
      "loss: 0.0682721\n",
      "loss: 0.0704941\n",
      "loss: 0.0696128\n",
      "loss: 0.0743579\n",
      "loss: 0.0624785\n",
      "loss: 0.0677763\n",
      "loss: 0.0689458\n",
      "loss: 0.0718244\n",
      "loss: 0.0654263\n",
      "loss: 0.0676773\n",
      "loss: 0.0676517\n",
      "loss: 0.0652635\n",
      "loss: 0.0683702\n",
      "loss: 0.0692165\n",
      "loss: 0.0706721\n",
      "loss: 0.0688086\n",
      "loss: 0.0688228\n",
      "loss: 0.0681008\n",
      "loss: 0.0725045\n",
      "loss: 0.0684893\n",
      "loss: 0.071436\n",
      "loss: 0.0674979\n",
      "epoch 7\n",
      "loss: 0.0669238\n",
      "loss: 0.0637463\n",
      "loss: 0.0689968\n",
      "loss: 0.0699813\n",
      "loss: 0.0675512\n",
      "loss: 0.0674087\n",
      "loss: 0.0643751\n",
      "loss: 0.0666524\n",
      "loss: 0.0700211\n",
      "loss: 0.0669986\n",
      "loss: 0.0674995\n",
      "loss: 0.0714939\n",
      "loss: 0.0702331\n",
      "loss: 0.0672109\n",
      "loss: 0.067019\n",
      "loss: 0.0674225\n",
      "loss: 0.0691512\n",
      "loss: 0.0673741\n",
      "loss: 0.0684966\n",
      "loss: 0.0692792\n",
      "loss: 0.0697504\n",
      "loss: 0.0676987\n",
      "loss: 0.0666309\n",
      "loss: 0.079826\n",
      "loss: 0.0661033\n",
      "loss: 0.0703019\n",
      "loss: 0.0659403\n",
      "loss: 0.0648513\n",
      "loss: 0.0687217\n",
      "loss: 0.0636318\n",
      "loss: 0.0687797\n",
      "loss: 0.0618913\n",
      "loss: 0.065109\n",
      "loss: 0.0675572\n",
      "loss: 0.0703328\n",
      "Stopping early during epoch 7 with best loss: 0.00854163\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "input_spatial_size = 28\n",
    "input_channels = 1\n",
    "batch_size = 80\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_input_neurons = input_spatial_size ** 2\n",
    "n_hidden_neurons_layer1 = 250\n",
    "n_hidden_neurons_layer2 = 150\n",
    "n_hidden_neurons_layer3 = n_hidden_neurons_layer1\n",
    "n_output_neurons = n_input_neurons\n",
    "l2_reg = 0.0001\n",
    "\n",
    "#initializer = lambda a: he_uniform_initialisation(a[0], a[1])#tf.contrib.layers.variance_scaling_initializer()\n",
    "#activation = tf.nn.elu\n",
    "#regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_input_neurons), name=\"input\") \n",
    "    \"\"\"\n",
    "    weights1 = initializer([n_input_neurons, n_hidden_neurons_layer1])\n",
    "    biases1 = tf.Variable(tf.zeros(n_hidden_neurons_layer1))\n",
    "    \n",
    "    weights2 = initializer([n_hidden_neurons_layer1, n_hidden_neurons_layer2])\n",
    "    biases2 = tf.Variable(tf.zeros(n_hidden_neurons_layer2))\n",
    "    \n",
    "    weights3 = initializer([n_hidden_neurons_layer2, n_hidden_neurons_layer3])\n",
    "    biases3 = tf.Variable(tf.zeros(n_hidden_neurons_layer3))\n",
    "    \n",
    "    weights4 = initializer([n_hidden_neurons_layer3, n_output_neurons])\n",
    "    biases4 = tf.Variable(tf.zeros(n_output_neurons))\n",
    "    \n",
    "    hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "    hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "    hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)\n",
    "    outputs = tf.matmul(hidden3, weights4) + biases4\n",
    "    \"\"\"\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    l2_regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "    from functools import partial\n",
    "    my_dense_layer = partial(tf.layers.dense,\n",
    "                             activation=tf.nn.elu,\n",
    "                             kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden_neurons_layer1)\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden_neurons_layer2)\n",
    "    hidden3 = my_dense_layer(hidden2, n_hidden_neurons_layer3)\n",
    "    outputs = my_dense_layer(hidden3, n_output_neurons, activation=None)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(outputs - X), name=\"reconstruction_loss\")\n",
    "        #regularisation_loss = regularizer(weights1) + regularizer(weights2) \\\n",
    "        #    + regularizer(weights3) + regularizer(weights4)\n",
    "        #regularisation_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = reconstruction_loss# + regularisation_loss\n",
    "\n",
    "    with tf.name_scope(\"training\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "interim_checkpoint_path = \"./checkpoints/mnist_autoencoder_model.ckpt\"\n",
    "early_stopping_checkpoint_path = \"./checkpoints/mnist_autoencoder_model_early_stopping.ckpt\"\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "log_dir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "summary_op = tf.summary.merge([loss_summary])\n",
    "file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "epochs = 20\n",
    "n_batches = int(np.ceil(len(mnist.train.images) // batch_size))\n",
    "\n",
    "early_stopping_check_frequency = n_batches // 4\n",
    "early_stopping_check_limit = n_batches * 3\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "session = sess\n",
    "sess.run(init)\n",
    "#saver.restore(sess, interim_checkpoint_path)\n",
    "\n",
    "best_loss = 1000000000.0\n",
    "best_loss_step = 0\n",
    "for epoch in range(epochs):\n",
    "    print(\"epoch\", epoch)\n",
    "    for batch_index in range(n_batches):\n",
    "        step = epoch * n_batches + batch_index\n",
    "        # TODO: replace this with code that gets a batch from X and y.\n",
    "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        if batch_index % 10 == 0:\n",
    "            summary_str = summary_op.eval(session=sess, feed_dict={X: X_batch})\n",
    "            file_writer.add_summary(summary_str, step)\n",
    "        t = sess.run([training_op], feed_dict={X: X_batch})\n",
    "        l = sess.run(reconstruction_loss, feed_dict={X: X_batch})\n",
    "        if batch_index % 10 == 0: print(\"loss:\", l)\n",
    "        # Early stopping check\n",
    "        if batch_index % early_stopping_check_frequency == 0:\n",
    "            if l < best_loss:\n",
    "                saver.save(sess, early_stopping_checkpoint_path)\n",
    "                best_loss = l\n",
    "                best_loss_step = step\n",
    "            elif step >= (best_loss_step + early_stopping_check_limit):\n",
    "                print(\"Stopping early during epoch\", epoch, \"with best loss:\", best_loss)\n",
    "                break\n",
    "    else:\n",
    "        continue\n",
    "    break\n",
    "    save_path = saver.save(sess, interim_checkpoint_path)\n",
    "saver.restore(sess, early_stopping_checkpoint_path)\n",
    "save_path = saver.save(sess, \"./checkpoints/mnist_autoencoder_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABVZJREFUeJzt3SFvlFkUgOF2gwJEFQJRsCDQJITyC8CAriKQIACBJsGR\n4CpRYJAk8A8AgUGgQKAqwIAFgWDW7i6Z22bb+ab0fR57Oj2f6JsrbmdmdTabrQA9fy37AYDlED9E\niR+ixA9R4oco8UOU+CFK/BAlfog6MvE+/04Ii7e6mx9y8kOU+CFK/BAlfogSP0SJH6LED1Hihyjx\nQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogS\nP0SJH6Km/opuJra1tTWc37lzZ6H7NzY25s5evHgxfO3a2tp+Pw7/4OSHKPFDlPghSvwQJX6IEj9E\niR+i3PMfcqdPnx7Ojx8/Ppx///59T/tfv349d3b37t3ha588ebKn3Yw5+SFK/BAlfogSP0SJH6LE\nD1Hihyj3/IfclStXhvMHDx4M5/fu3dvPx/mXb9++Lex3szMnP0SJH6LED1HihyjxQ5T4IcpVX9yZ\nM2eW/QgsiZMfosQPUeKHKPFDlPghSvwQJX6Ics8fN/pobQ43Jz9EiR+ixA9R4oco8UOU+CFK/BC1\nOpvNptw36TJWVp4+fTqc37x5czj/+fPnnvaP/r7evn07fO358+f3tDtsdTc/5OSHKPFDlPghSvwQ\nJX6IEj9EiR+ivJ//EBjdpT969Gj42r3e4+9kbW1t7uzkyZML3c2Ykx+ixA9R4oco8UOU+CFK/BAl\nfohyz3/IffjwYan7Nzc3587W19cnfBL+y8kPUeKHKPFDlPghSvwQJX6IctV3CIy+Znvij2b/zcWL\nF5e6n/mc/BAlfogSP0SJH6LED1HihyjxQ5R7/kPg+fPnc2erq7v6tuaFuXbt2lL3M5+TH6LED1Hi\nhyjxQ5T4IUr8ECV+iHLP/wf4/PnzcP7y5cuJnuR3t27dWtpu9sbJD1HihyjxQ5T4IUr8ECV+iBI/\nRLnn/wP8+PFjON/e3l7Y7hMnTgznN27cWNhuFsvJD1HihyjxQ5T4IUr8ECV+iBI/RLnn/wO8evVq\nOJ/NZgvbffny5eH83LlzC9tddfv27eF8a2trX/Y4+SFK/BAlfogSP0SJH6LED1Gu+g6Ar1+/DueP\nHz8ezhf5NdxXr15d2O8uG13fvnnzZpJncPJDlPghSvwQJX6IEj9EiR+ixA9R7vkPgOvXrw/n7969\nW9juY8eODedHjx5d2O6y0cex7/RR7fvFyQ9R4oco8UOU+CFK/BAlfogSP0S55z8APn78uLTdly5d\nGs43NjYmehKm5uSHKPFDlPghSvwQJX6IEj9EiR+i3PNPYKevVP706dNET/K7X79+LW132alTp+bO\n1tfXJ3kGJz9EiR+ixA9R4oco8UOU+CHKVd8BsMiv2N7J/fv3l7a77OzZs3Nnz549m+QZnPwQJX6I\nEj9EiR+ixA9R4oco8UPU6mw2m3LfpMsOiu3t7eH8woULw/mXL1/+9+61tbXh/P3798P5VG8vZV/t\n6h9HnPwQJX6IEj9EiR+ixA9R4oco8UOU9/NPYPQxzSsrKyubm5vD+cOHD//37p1+t3v8Lic/RIkf\nosQPUeKHKPFDlPghSvwQ5f38cPh4Pz8wn/ghSvwQJX6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogS\nP0SJH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ijky8b1df\nHQwsnpMfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogSP0SJH6LED1Hi\nhyjxQ5T4IUr8ECV+iPob6mOS90DNo2kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4746211320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJNzv3o6EsAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f474ffb7d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABCdJREFUeJzt3V1uqlAYQNFy03mpI5OODBkZdwSikYCte63XU36Sduc8\nfECHZVm+gJ5/774B4D3ED1HihyjxQ5T4IUr8ECV+iBI/RIkfor4Pvp7HCWF/wzM/ZOeHKPFDlPgh\nSvwQJX6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkf\nosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4\nIUr8ECV+iBI/RH2/+wb4+rpcLqvrt9ttdf16vd5dG8fxhTuiwM4PUeKHKPFDlPghSvwQJX6IGpZl\nOfJ6h17srxiG4d238LLz+by6fjqd7q4ZQ+7mqT8oOz9EiR+ixA9R4oco8UOU+CFK/BBlzv8LPHpl\n9+fnZ9Pxv9WjZwSmaTrmRj6POT9wn/ghSvwQJX6IEj9EiR+ixA9R5vwfYG3Ov/czAPM8r65vuf6j\nOf+j5wTCzPmB+8QPUeKHKPFDlPghSvwQJX6I8i+6P8DavHvvWfijOf6WOf+jY835t7HzQ5T4IUr8\nECV+iBI/RIkfosQPUeb8bGLW/nfZ+SFK/BAlfogSP0SJH6LED1FGfWwyjuNu5zZG3JedH6LED1Hi\nhyjxQ5T4IUr8ECV+iDLn59cy59+XnR+ixA9R4oco8UOU+CFK/BAlfogy52eTeZ5fPtYc/73s/BAl\nfogSP0SJH6LED1HihyjxQ9SwLMuR1zv0YuxvGIaXj52maXXdcwAve+qXYueHKPFDlPghSvwQJX6I\nEj9EiR+ivM/Pqsvlstu5zfHfy84PUeKHKPFDlPghSvwQJX6I8kovq7a8svvIwX97JV7pBe4TP0SJ\nH6LED1HihyjxQ5T4Icorvezqer2++xa4w84PUeKHKPFDlPghSvwQJX6IEj9EmfPHjeP4p8/P6+z8\nECV+iBI/RIkfosQPUeKHKPFDlO/2x239Lv+jf7M9TdOm8/MS3+0H7hM/RIkfosQPUeKHKPFDlFd6\nP9ztdtv1/KfTadfzsx87P0SJH6LED1HihyjxQ5T4IUr8EGXO/+H2nvPzd9n5IUr8ECV+iBI/RIkf\nosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogSP0SJH6LED1Hihyjf7f9w\n5/N5dX2e503nH8dx0/G8j50fosQPUeKHKPFDlPghSvwQNSzLcuT1Dr0YRA3P/JCdH6LED1Hihyjx\nQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IOvrT3U+9Zwzsz84PUeKHKPFDlPghSvwQ\nJX6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4IUr8ECV+iBI/RP0HtMdo\neQf+3I4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f472415b358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46fdc6aba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxA+hxA+hxA+hfgOTxrn4DhDQQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4703bbb4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngaQofiApih9I6i/iK5H6lswa2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f470b53cac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_image(image):\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary,\n",
    "        interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "visualisation_batch = mnist.train.images[:3]\n",
    "o = sess.run([outputs], feed_dict={X: visualisation_batch})\n",
    "o = np.array(o).reshape((-1, n_input_neurons))\n",
    "\n",
    "image_shape = (input_spatial_size, input_spatial_size)\n",
    "for input_data, output_data in zip(visualisation_batch, o):\n",
    "    input_image = input_data.reshape(image_shape)\n",
    "    show_image(input_image)\n",
    "    output_image = output_data.reshape(image_shape)\n",
    "    show_image(output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
