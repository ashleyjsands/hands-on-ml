{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.layers import batch_norm, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8\n",
    "===\n",
    "\n",
    "Create a DQN to solve the \"BipedalWalker-v2\" gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.74708518e-03  2.86767026e-06 -3.73954531e-04 -1.60000181e-02\n",
      "  9.22417939e-02  8.68125702e-04  8.60060006e-01  7.13005662e-04\n",
      "  1.00000000e+00  3.26231159e-02  8.68074829e-04  8.53662491e-01\n",
      " -5.90644195e-04  1.00000000e+00  4.40813750e-01  4.45819855e-01\n",
      "  4.61422503e-01  4.89549905e-01  5.34102499e-01  6.02460682e-01\n",
      "  7.09148467e-01  8.85931313e-01  1.00000000e+00  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"BipedalWalker-v2\")\n",
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def render_env(env):\n",
    "    plt.imshow(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "#render_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "(4,)\n",
      "[-1 -1 -1 -1]\n",
      "[1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.action_space.shape)\n",
    "print(env.action_space.low)\n",
    "print(env.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_discretized_actions 3 n_actions 4 n_outputs 81\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "neurons_per_layer = [20]#[40, 40, 20]\n",
    "learning_rate = 1 #0.001\n",
    "n_discretized_actions = 3 # -1, -0.5, 0, +0.5, +1\n",
    "n_actions = env.action_space.shape[0]\n",
    "#n_outputs = env.action_space.shape[0]\n",
    "n_outputs = n_discretized_actions ** n_actions\n",
    "print(\"n_discretized_actions\", n_discretized_actions, \"n_actions\", n_actions, \"n_outputs\", n_outputs)\n",
    "\n",
    "def q_network(X_state, scope):\n",
    "    current_layer = X_state\n",
    "    layers = []\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        for neurons in neurons_per_layer:\n",
    "            current_layer = fully_connected(current_layer, neurons)#, activation_fn=tf.nn.sigmoid)\n",
    "            #print(\"current_layer shape\", current_layer.get_shape())\n",
    "        #print(\"input shape\", current_layer.get_shape())\n",
    "        outputs = fully_connected(current_layer, n_outputs, activation_fn=None)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name)]: var for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,)\n"
     ]
    }
   ],
   "source": [
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_state = tf.placeholder(tf.float32, shape=[None, obs.shape[0]])\n",
    "actor_q_values, actor_vars = q_network(X_state, \"q_networks/actor\")\n",
    "critic_q_values, critic_vars = q_network(X_state, \"q_networks/critic\")\n",
    "\n",
    "copy_ops = [actor_var.assign(critic_vars[var_name])\n",
    "            for var_name, actor_var in actor_vars.items()]\n",
    "copy_critic_to_actor = tf.group(*copy_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic_q_values.get_shape() (?, 81) q_value.get_shape() (?, 1)\n"
     ]
    }
   ],
   "source": [
    "X_action = tf.placeholder(tf.int32, shape=[None, env.action_space.shape[0]])\n",
    "# This contains the Q-value for the actor's chosen action.\n",
    "# q_value = tf.reduce_sum(critic_q_values * tf.one_hot(X_action, n_outputs), axis=1, keep_dims=True)\n",
    "q_value = tf.reduce_sum(critic_q_values * tf.one_hot(n_outputs, 1), axis=1, keep_dims=True)\n",
    "#q_value = critic_q_values\n",
    "print(\"critic_q_values.get_shape()\", critic_q_values.shape, \"q_value.get_shape()\", q_value.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Actor's reward + future discounted estimated Q-value.\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "cost = tf.reduce_mean(tf.square(y - q_value))\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory_size = 10000\n",
    "replay_memory = deque([], maxlen=replay_memory_size)\n",
    "\n",
    "def sample_memory(batch_size):\n",
    "    indices = np.random.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for idx in indices:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 4)\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "discretized_actions = [-1, 0, 1]\n",
    "action_space_universe = np.array(list(product(discretized_actions, discretized_actions, discretized_actions, discretized_actions)))\n",
    "\n",
    "print(action_space_universe.shape)\n",
    "#print(action_space_universe)\n",
    "\n",
    "def action_index_to_action(action_index):\n",
    "    return action_space_universe[action_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon greed algorithm to explore the state-action space of the environment at the beginning.\n",
    "def epsilon_greedy(q_values, step):\n",
    "    eps_min = 0.05 # At the start 5% of the time the actor will choose the greedy action.\n",
    "    eps_max = 1.0 # Once it has finished exploring, the actor will choose the greedy action 100% of the time.\n",
    "    eps_decay_steps = 200000\n",
    "    \n",
    "    epsilon = max(eps_min, eps_max - (eps_max - eps_min) * step / eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        #return np.random.uniform(env.action_space.low, env.action_space.high, size=n_actions)\n",
    "        return np.random.randint(len(action_space_universe))\n",
    "    else:\n",
    "        #return q_values.reshape(n_outputs)\n",
    "        return np.argmax(q_values) # the optimal/greedy action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent died: current_steps 116 total_reward -23.803945232568903\n",
      "Agent died: current_steps 113 total_reward -23.492575880540528\n",
      "Agent died: current_steps 66 total_reward -15.643291910674412\n",
      "Agent died: current_steps 90 total_reward -10.922980989407119\n",
      "Agent died: current_steps 103 total_reward -19.14144876760615\n",
      "Agent died: current_steps 76 total_reward -3.350739771795154\n",
      "Agent died: current_steps 37 total_reward -8.322276695397372\n",
      "Agent died: current_steps 45 total_reward -12.718892266169194\n",
      "Agent died: current_steps 65 total_reward -15.53690377385231\n",
      "Agent died: current_steps 63 total_reward -15.50765203927023\n",
      "Agent died: current_steps 45 total_reward -7.822595762593051\n",
      "Agent died: current_steps 1599 total_reward -110.34901866857302\n",
      "Agent died: current_steps 1599 total_reward -126.32463917083741\n",
      "Agent died: current_steps 78 total_reward -7.611501780005795\n",
      "Agent died: current_steps 98 total_reward -17.195385325001553\n",
      "Agent died: current_steps 55 total_reward -14.492614199799801\n",
      "Agent died: current_steps 52 total_reward -16.05609828114199\n",
      "Agent died: current_steps 51 total_reward -14.53266548592597\n",
      "Agent died: current_steps 1599 total_reward -106.88000778295368\n",
      "Agent died: current_steps 77 total_reward 1.1816071596002797\n",
      "Agent died: current_steps 49 total_reward -8.956365242702885\n",
      "Agent died: current_steps 1549 total_reward -121.62264127191449\n",
      "Agent died: current_steps 55 total_reward -14.37742049252366\n",
      "Agent died: current_steps 60 total_reward -3.474116225734974\n",
      "Agent died: current_steps 81 total_reward -12.213592499084132\n",
      "Agent died: current_steps 55 total_reward -6.828157514560345\n",
      "Agent died: current_steps 84 total_reward -9.236467063517608\n",
      "Agent died: current_steps 1599 total_reward -109.66361833866122\n",
      "Agent died: current_steps 1599 total_reward -108.83395724309376\n",
      "Agent died: current_steps 77 total_reward -20.593724213920527\n",
      "Agent died: current_steps 49 total_reward -13.529683342192943\n",
      "Agent died: current_steps 1599 total_reward -116.06278782004333\n",
      "Agent died: current_steps 1599 total_reward -107.50182536665531\n",
      "Agent died: current_steps 77 total_reward -16.747065609873573\n",
      "Agent died: current_steps 1599 total_reward -111.7644464778742\n",
      "Agent died: current_steps 58 total_reward -17.463806249042975\n",
      "Agent died: current_steps 86 total_reward -18.03143060489619\n",
      "Agent died: current_steps 100 total_reward -18.17255479932451\n",
      "Agent died: current_steps 35 total_reward -8.651343398257465\n",
      "Agent died: current_steps 55 total_reward -15.592506601930909\n",
      "Agent died: current_steps 75 total_reward -18.127254031656925\n",
      "Agent died: current_steps 72 total_reward -4.473055606018134\n",
      "Agent died: current_steps 1599 total_reward -116.17855777909196\n",
      "Agent died: current_steps 1599 total_reward -115.5799588462274\n",
      "Agent died: current_steps 127 total_reward -25.03865984134312\n",
      "Agent died: current_steps 73 total_reward -8.199102716056624\n",
      "Agent died: current_steps 1599 total_reward -107.26981163482023\n",
      "Agent died: current_steps 81 total_reward -11.222211701797937\n",
      "Agent died: current_steps 47 total_reward -11.675454016764336\n",
      "Agent died: current_steps 123 total_reward -16.062316247620284\n",
      "Agent died: current_steps 81 total_reward -16.5759638812399\n",
      "Agent died: current_steps 43 total_reward -5.195830736637117\n",
      "Agent died: current_steps 81 total_reward -25.248990639779098\n",
      "Agent died: current_steps 73 total_reward -11.59517578803747\n",
      "Agent died: current_steps 81 total_reward -21.379047033196304\n",
      "Agent died: current_steps 1599 total_reward -119.23587188351758\n",
      "Agent died: current_steps 1599 total_reward -122.47132752181747\n",
      "Agent died: current_steps 37 total_reward -7.1646323505795255\n",
      "Agent died: current_steps 62 total_reward -11.749660391718146\n",
      "Agent died: current_steps 77 total_reward -3.474138937494405\n",
      "Agent died: current_steps 77 total_reward -17.546767043555146\n",
      "Agent died: current_steps 1599 total_reward -116.89936483500794\n",
      "Agent died: current_steps 88 total_reward -19.680398740942884\n",
      "Agent died: current_steps 1599 total_reward -130.0498259396712\n",
      "Agent died: current_steps 67 total_reward -1.606880034092197\n",
      "Agent died: current_steps 1599 total_reward -124.87434654376978\n",
      "global_step 10000\n",
      "global_step 10000\n",
      "global_step 10000\n",
      "Agent died: current_steps 897 total_reward -80.1550735860121\n",
      "Agent died: current_steps 1599 total_reward -125.02949667241819\n",
      "Agent died: current_steps 79 total_reward -2.945193227192383\n",
      "Agent died: current_steps 1599 total_reward -122.75431006097696\n",
      "Agent died: current_steps 143 total_reward -27.410920220759397\n",
      "Agent died: current_steps 62 total_reward -14.380685480122146\n",
      "Agent died: current_steps 62 total_reward -13.250186263345181\n",
      "Agent died: current_steps 78 total_reward -22.879116371413698\n",
      "Agent died: current_steps 66 total_reward -23.546375874417524\n",
      "Agent died: current_steps 203 total_reward -33.29076183316171\n",
      "Agent died: current_steps 47 total_reward -11.317363284411526\n",
      "Agent died: current_steps 1226 total_reward -100.68972126800203\n",
      "Agent died: current_steps 84 total_reward -21.5806703659712\n",
      "Agent died: current_steps 694 total_reward -72.92451126724795\n",
      "Agent died: current_steps 39 total_reward -11.295317302379758\n",
      "Agent died: current_steps 42 total_reward -6.453583951136097\n",
      "Agent died: current_steps 1599 total_reward -91.97870235950336\n",
      "Agent died: current_steps 1419 total_reward -108.39216754314383\n",
      "Agent died: current_steps 1599 total_reward -120.71945370620054\n",
      "Agent died: current_steps 81 total_reward -0.34223887607889397\n",
      "Agent died: current_steps 86 total_reward -26.075634081811963\n",
      "Agent died: current_steps 1599 total_reward -117.96070590438472\n",
      "Agent died: current_steps 74 total_reward -3.3116134284120053\n",
      "Agent died: current_steps 1599 total_reward -108.66922197995221\n",
      "Agent died: current_steps 1599 total_reward -128.8424414978049\n",
      "Agent died: current_steps 53 total_reward -22.891231728823858\n",
      "Agent died: current_steps 62 total_reward -2.3917676226540148\n",
      "Agent died: current_steps 60 total_reward -16.98952712359652\n",
      "Agent died: current_steps 70 total_reward -15.026846327552565\n",
      "Agent died: current_steps 84 total_reward -1.0415024242643192\n",
      "Agent died: current_steps 84 total_reward -4.6869248354192825\n",
      "Agent died: current_steps 330 total_reward -45.71309279558371\n",
      "Agent died: current_steps 1511 total_reward -129.8923283070285\n",
      "Agent died: current_steps 1599 total_reward -100.37277447073343\n",
      "Agent died: current_steps 64 total_reward 1.5569617509208586\n",
      "Agent died: current_steps 92 total_reward -28.04808112222205\n",
      "Agent died: current_steps 88 total_reward -21.101535635131725\n",
      "Agent died: current_steps 68 total_reward -13.961705391063656\n",
      "Agent died: current_steps 75 total_reward -2.4098004002359996\n",
      "Agent died: current_steps 92 total_reward -0.07617147135982621\n",
      "Agent died: current_steps 1599 total_reward -112.89847276350528\n",
      "Agent died: current_steps 71 total_reward -4.152152461449309\n",
      "Agent died: current_steps 88 total_reward -23.27336378101631\n",
      "Agent died: current_steps 68 total_reward -19.68305341689289\n",
      "Agent died: current_steps 1599 total_reward -118.92410446586571\n",
      "Agent died: current_steps 59 total_reward -9.290850282746053\n",
      "Agent died: current_steps 74 total_reward -15.628096362916741\n",
      "Agent died: current_steps 1599 total_reward -118.64992854175692\n",
      "Agent died: current_steps 51 total_reward -11.038585354895641\n",
      "Agent died: current_steps 929 total_reward -92.26135047889453\n",
      "Agent died: current_steps 1599 total_reward -116.93033497291196\n",
      "Agent died: current_steps 687 total_reward -58.96627267714102\n",
      "global_step 20000\n",
      "global_step 20000\n",
      "global_step 20000\n",
      "Agent died: current_steps 1286 total_reward -126.07925489309251\n",
      "Agent died: current_steps 52 total_reward -11.151216290359072\n",
      "Agent died: current_steps 88 total_reward -4.621858112848672\n",
      "Agent died: current_steps 1352 total_reward -117.7381970232975\n",
      "Agent died: current_steps 1170 total_reward -102.89662342020014\n",
      "Agent died: current_steps 47 total_reward -7.199540677967168\n",
      "Agent died: current_steps 63 total_reward -2.8352637598657413\n",
      "Agent died: current_steps 108 total_reward -13.504363621434173\n",
      "Agent died: current_steps 1599 total_reward -109.22239814090891\n",
      "Agent died: current_steps 1599 total_reward -124.08436145093322\n",
      "Agent died: current_steps 68 total_reward -1.851515553021807\n",
      "Agent died: current_steps 65 total_reward -3.5071925916336513\n",
      "Agent died: current_steps 47 total_reward -9.172004888645683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent died: current_steps 75 total_reward -2.8807033831483424\n",
      "Agent died: current_steps 74 total_reward -16.350544048570093\n",
      "Agent died: current_steps 1599 total_reward -116.52148472407119\n",
      "Agent died: current_steps 133 total_reward -15.960650776656944\n",
      "Agent died: current_steps 100 total_reward -21.255030794510613\n",
      "Agent died: current_steps 118 total_reward -3.5943775407063265\n",
      "Agent died: current_steps 1599 total_reward -135.04876019959752\n",
      "Agent died: current_steps 69 total_reward -4.711659635302298\n",
      "Agent died: current_steps 1486 total_reward -133.30203322205944\n",
      "Agent died: current_steps 70 total_reward -0.4402937989868227\n",
      "Agent died: current_steps 96 total_reward -6.623934364743524\n",
      "Agent died: current_steps 61 total_reward -17.34821325808763\n",
      "Agent died: current_steps 99 total_reward -28.80870671799967\n",
      "Agent died: current_steps 1599 total_reward -119.44251458073352\n",
      "Agent died: current_steps 81 total_reward -1.3415354476794565\n",
      "Agent died: current_steps 61 total_reward -18.348439663466067\n",
      "Agent died: current_steps 42 total_reward -9.78714970316862\n",
      "Agent died: current_steps 1599 total_reward -125.3198112562977\n",
      "Agent died: current_steps 63 total_reward -17.984211178035785\n",
      "Agent died: current_steps 1599 total_reward -132.57196539842323\n",
      "Agent died: current_steps 101 total_reward -18.64738776947556\n",
      "Agent died: current_steps 1599 total_reward -101.68697064339273\n",
      "Agent died: current_steps 69 total_reward -1.6866639131537373\n",
      "Agent died: current_steps 61 total_reward -11.481611456871029\n",
      "Agent died: current_steps 96 total_reward -8.304958786402521\n",
      "Agent died: current_steps 1599 total_reward -127.89896737044192\n",
      "Agent died: current_steps 77 total_reward -7.471993380539117\n",
      "Agent died: current_steps 79 total_reward -2.329351858291149\n",
      "Agent died: current_steps 53 total_reward -2.322733400598171\n",
      "Agent died: current_steps 67 total_reward -18.82352863438552\n",
      "Agent died: current_steps 1599 total_reward -119.28351024100054\n",
      "Agent died: current_steps 63 total_reward -15.693579313170163\n",
      "Agent died: current_steps 63 total_reward -3.286986487551905\n",
      "Agent died: current_steps 99 total_reward -5.055133040123294\n",
      "Agent died: current_steps 92 total_reward -1.7870008761454401\n",
      "Agent died: current_steps 119 total_reward -23.10760641816506\n",
      "Agent died: current_steps 1599 total_reward -116.285548984618\n",
      "Agent died: current_steps 75 total_reward -3.441196255296473\n",
      "Agent died: current_steps 283 total_reward -39.9123373316973\n",
      "Agent died: current_steps 1599 total_reward -132.09474435745227\n",
      "Agent died: current_steps 113 total_reward -6.407781538346762\n",
      "Agent died: current_steps 61 total_reward -16.322742887039976\n",
      "Agent died: current_steps 86 total_reward -3.5730958825790626\n",
      "Agent died: current_steps 897 total_reward -72.77957119037958\n",
      "Agent died: current_steps 1599 total_reward -134.01398595173916\n",
      "Agent died: current_steps 92 total_reward -0.4174786578423023\n",
      "Agent died: current_steps 63 total_reward -18.04736131545156\n",
      "global_step 30000\n",
      "global_step 30000\n",
      "global_step 30000\n",
      "Agent died: current_steps 1599 total_reward -114.30713926431554\n",
      "Agent died: current_steps 108 total_reward -1.6312885907006722\n",
      "Agent died: current_steps 1599 total_reward -128.34722306389634\n",
      "Agent died: current_steps 71 total_reward -14.181562320866927\n",
      "Agent died: current_steps 1599 total_reward -115.22813588439003\n",
      "Agent died: current_steps 91 total_reward -5.432514002270995\n",
      "Agent died: current_steps 66 total_reward -14.812736803211886\n",
      "Agent died: current_steps 62 total_reward -19.60217573982105\n",
      "Agent died: current_steps 64 total_reward -21.66110139799242\n",
      "Agent died: current_steps 65 total_reward 0.05216593345316517\n",
      "Agent died: current_steps 1599 total_reward -105.71260112465451\n",
      "Agent died: current_steps 288 total_reward -35.00287038419825\n",
      "Agent died: current_steps 1599 total_reward -111.04973575767202\n",
      "Agent died: current_steps 83 total_reward -19.42048349278296\n",
      "Agent died: current_steps 47 total_reward -5.736362031400205\n",
      "Agent died: current_steps 98 total_reward -17.904953619786212\n",
      "Agent died: current_steps 52 total_reward -13.953595626887557\n",
      "Agent died: current_steps 66 total_reward -16.69972885820332\n",
      "Agent died: current_steps 71 total_reward -7.764814681880172\n",
      "Agent died: current_steps 1599 total_reward -127.40217621581984\n",
      "Agent died: current_steps 88 total_reward -2.0414573556923603\n",
      "Agent died: current_steps 49 total_reward -13.208145533304045\n",
      "Agent died: current_steps 66 total_reward -0.9951178407147545\n",
      "Agent died: current_steps 608 total_reward -61.50269371930753\n",
      "Agent died: current_steps 56 total_reward -6.030946285365772\n",
      "Agent died: current_steps 1599 total_reward -122.46167843184516\n",
      "Agent died: current_steps 63 total_reward -15.192893757058927\n",
      "Agent died: current_steps 107 total_reward -15.10112307886467\n",
      "Agent died: current_steps 74 total_reward -3.8582984126284705\n",
      "Agent died: current_steps 52 total_reward -15.940521582442646\n",
      "Agent died: current_steps 35 total_reward -13.376960332222286\n",
      "Agent died: current_steps 360 total_reward -42.88978547506772\n",
      "Agent died: current_steps 1599 total_reward -133.7073044715423\n",
      "Agent died: current_steps 77 total_reward -5.188767493493235\n",
      "Agent died: current_steps 82 total_reward -8.566463854381816\n",
      "Agent died: current_steps 274 total_reward -40.06578827020959\n",
      "Agent died: current_steps 84 total_reward -11.150030381112668\n",
      "Agent died: current_steps 65 total_reward -14.027480000987651\n",
      "Agent died: current_steps 1599 total_reward -113.91869865788837\n",
      "Agent died: current_steps 1599 total_reward -89.62656535114674\n",
      "Agent died: current_steps 104 total_reward -20.75697867233369\n",
      "Agent died: current_steps 1599 total_reward -137.79703755024303\n",
      "Agent died: current_steps 127 total_reward -27.511644532077618\n",
      "Agent died: current_steps 195 total_reward -32.608296471151185\n",
      "Agent died: current_steps 1599 total_reward -114.92738869242089\n",
      "Agent died: current_steps 53 total_reward -2.9933433844888007\n",
      "Agent died: current_steps 53 total_reward -10.03416928877247\n",
      "Agent died: current_steps 1599 total_reward -111.10769029502146\n",
      "Agent died: current_steps 68 total_reward -8.822311797649911\n",
      "Agent died: current_steps 35 total_reward -5.385579526712494\n",
      "Agent died: current_steps 416 total_reward -39.635326457060216\n",
      "Agent died: current_steps 59 total_reward -15.417167679484933\n",
      "Agent died: current_steps 91 total_reward 0.9915771400344836\n",
      "Agent died: current_steps 55 total_reward -0.5229156401548553\n",
      "Agent died: current_steps 1599 total_reward -105.17108721834614\n",
      "Agent died: current_steps 171 total_reward -23.582057099699938\n",
      "Agent died: current_steps 76 total_reward -21.34327244878746\n",
      "Agent died: current_steps 59 total_reward -11.159440737316997\n",
      "Agent died: current_steps 62 total_reward -12.320821177832777\n",
      "Agent died: current_steps 108 total_reward -5.902463404212153\n",
      "Agent died: current_steps 109 total_reward -5.90865319345084\n",
      "Agent died: current_steps 1599 total_reward -100.91907511187476\n",
      "Agent died: current_steps 58 total_reward -3.2348921722037103\n",
      "Agent died: current_steps 78 total_reward -19.783129890310263\n",
      "Agent died: current_steps 54 total_reward -3.9661247874014105\n",
      "global_step 40000\n",
      "global_step 40000\n",
      "global_step 40000\n",
      "Agent died: current_steps 1291 total_reward -121.28901380567707\n",
      "Agent died: current_steps 53 total_reward -8.400737919736033\n",
      "Agent died: current_steps 102 total_reward -7.529573228574045\n",
      "Agent died: current_steps 54 total_reward -1.2698466088349656\n",
      "Agent died: current_steps 81 total_reward -0.08318058870049849\n",
      "Agent died: current_steps 75 total_reward -23.559603454812112\n",
      "Agent died: current_steps 150 total_reward -19.2276880962178\n",
      "Agent died: current_steps 67 total_reward -1.3863273948077113\n",
      "Agent died: current_steps 83 total_reward 0.8922095482007883\n",
      "Agent died: current_steps 1599 total_reward -113.55830668075089\n",
      "Agent died: current_steps 1599 total_reward -124.44940544169538\n",
      "Agent died: current_steps 181 total_reward -21.23122218358021\n",
      "Agent died: current_steps 1599 total_reward -128.15232334808974\n",
      "Agent died: current_steps 80 total_reward -13.897815609980979\n",
      "Agent died: current_steps 40 total_reward -10.96125608852257\n",
      "Agent died: current_steps 78 total_reward -3.270133317947391\n",
      "Agent died: current_steps 80 total_reward -17.601551826619232\n",
      "Agent died: current_steps 148 total_reward -1.711456653856966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent died: current_steps 129 total_reward -6.485736214259002\n",
      "Agent died: current_steps 101 total_reward -4.77455969182029\n",
      "Agent died: current_steps 149 total_reward -20.456552527287943\n",
      "Agent died: current_steps 38 total_reward -4.183164329415189\n",
      "Agent died: current_steps 77 total_reward -2.845057701524971\n",
      "Agent died: current_steps 70 total_reward -19.688522764998172\n",
      "Agent died: current_steps 103 total_reward -23.99079502249135\n",
      "Agent died: current_steps 599 total_reward -71.98450719359084\n",
      "Agent died: current_steps 63 total_reward -6.571508155960589\n",
      "Agent died: current_steps 75 total_reward -2.611092600391559\n",
      "Agent died: current_steps 78 total_reward -3.5031095197672677\n",
      "Agent died: current_steps 62 total_reward -5.49128978076205\n",
      "Agent died: current_steps 84 total_reward -0.13360821805956377\n",
      "Agent died: current_steps 40 total_reward -11.062675286270679\n",
      "Agent died: current_steps 1599 total_reward -114.07260619094752\n",
      "Agent died: current_steps 44 total_reward -9.217416189596678\n",
      "Agent died: current_steps 43 total_reward -12.790903491002812\n",
      "Agent died: current_steps 42 total_reward -11.774741889633235\n",
      "Agent died: current_steps 1599 total_reward -116.64859119834283\n",
      "Agent died: current_steps 37 total_reward -7.904885797224322\n",
      "Agent died: current_steps 69 total_reward -7.9087685279734385\n",
      "Agent died: current_steps 1599 total_reward -115.70410860165909\n",
      "Agent died: current_steps 55 total_reward -4.096477911310895\n",
      "Agent died: current_steps 1599 total_reward -127.6737594430021\n",
      "Agent died: current_steps 79 total_reward 0.07224384399379274\n",
      "Agent died: current_steps 41 total_reward -4.108822109092651\n",
      "Agent died: current_steps 66 total_reward -16.040872813027352\n",
      "Agent died: current_steps 465 total_reward -57.57727559342612\n",
      "Agent died: current_steps 94 total_reward -0.5320364106160855\n",
      "Agent died: current_steps 317 total_reward -44.52349759107262\n",
      "Agent died: current_steps 65 total_reward -9.402003681796295\n",
      "Agent died: current_steps 71 total_reward -2.9506399537256662\n",
      "Agent died: current_steps 67 total_reward -19.32465780374532\n",
      "Agent died: current_steps 1599 total_reward -129.29626063712576\n",
      "Agent died: current_steps 58 total_reward -17.57024924302288\n",
      "Agent died: current_steps 1599 total_reward -101.32576102729584\n",
      "Agent died: current_steps 93 total_reward -24.98802741312856\n",
      "Agent died: current_steps 1599 total_reward -118.39068529307718\n",
      "Agent died: current_steps 1599 total_reward -127.21336386966256\n",
      "Agent died: current_steps 937 total_reward -89.9781823110622\n",
      "Agent died: current_steps 1599 total_reward -106.95655744798441\n",
      "Agent died: current_steps 64 total_reward -3.2465371363603817\n",
      "Agent died: current_steps 1599 total_reward -135.21035073623378\n",
      "Agent died: current_steps 1599 total_reward -115.94818183251618\n",
      "Agent died: current_steps 1176 total_reward -111.80069690875176\n",
      "global_step 50000\n",
      "global_step 50000\n",
      "global_step 50000\n",
      "Agent died: current_steps 845 total_reward -93.57173531188413\n",
      "Agent died: current_steps 59 total_reward -9.434036614109447\n",
      "Agent died: current_steps 66 total_reward -3.0620671978878486\n",
      "Agent died: current_steps 197 total_reward -16.69163428547912\n",
      "Agent died: current_steps 1032 total_reward -74.70524799181388\n",
      "Agent died: current_steps 58 total_reward -10.335756344650562\n",
      "Agent died: current_steps 1599 total_reward -126.00204610485048\n",
      "Agent died: current_steps 64 total_reward -19.00351037010053\n",
      "Agent died: current_steps 1599 total_reward -123.97058185237968\n",
      "Agent died: current_steps 75 total_reward -7.78152417521924\n",
      "Agent died: current_steps 1314 total_reward -106.60138119648893\n",
      "Agent died: current_steps 74 total_reward -18.398727328423405\n",
      "Agent died: current_steps 48 total_reward -14.756479228667292\n",
      "Agent died: current_steps 1599 total_reward -129.9676351287708\n",
      "Agent died: current_steps 65 total_reward -3.676547279752167\n",
      "Agent died: current_steps 69 total_reward -5.28286963623452\n",
      "Agent died: current_steps 70 total_reward -7.828954993685085\n",
      "Agent died: current_steps 107 total_reward -23.285065659951407\n",
      "Agent died: current_steps 1599 total_reward -128.36577098392635\n",
      "Agent died: current_steps 67 total_reward -8.294786495411763\n",
      "Agent died: current_steps 55 total_reward -4.015451052484415\n",
      "Agent died: current_steps 72 total_reward -8.0567396200827\n",
      "Agent died: current_steps 96 total_reward 0.6173428581661198\n",
      "Agent died: current_steps 1599 total_reward -102.67658213828079\n",
      "Agent died: current_steps 70 total_reward -5.501959033957997\n",
      "Agent died: current_steps 78 total_reward -8.965133439028632\n",
      "Agent died: current_steps 63 total_reward -17.924905903472872\n",
      "Agent died: current_steps 61 total_reward -0.44964218182427285\n",
      "Agent died: current_steps 787 total_reward -56.07949270406537\n",
      "Agent died: current_steps 75 total_reward -4.336242004945251\n",
      "Agent died: current_steps 98 total_reward -0.9134089072470855\n",
      "Agent died: current_steps 68 total_reward -19.95165269569432\n",
      "Agent died: current_steps 133 total_reward -24.181066546937426\n",
      "Agent died: current_steps 96 total_reward -16.03828245827864\n",
      "Agent died: current_steps 95 total_reward -5.376836499943709\n",
      "Agent died: current_steps 86 total_reward -10.834068110420052\n",
      "Agent died: current_steps 64 total_reward -14.121634727496648\n",
      "Agent died: current_steps 80 total_reward -18.75659764340644\n",
      "Agent died: current_steps 70 total_reward -15.728112154112505\n",
      "Agent died: current_steps 383 total_reward -32.531816767807356\n",
      "Agent died: current_steps 1599 total_reward -122.65402121716843\n",
      "Agent died: current_steps 62 total_reward -2.8365983737744425\n",
      "Agent died: current_steps 58 total_reward -12.823065191772448\n",
      "Agent died: current_steps 113 total_reward 1.868722183261065\n",
      "Agent died: current_steps 35 total_reward -9.555545766801885\n",
      "Agent died: current_steps 1354 total_reward -116.36132366649042\n",
      "Agent died: current_steps 67 total_reward -8.655960916317378\n",
      "Agent died: current_steps 87 total_reward -10.503977793814613\n",
      "Agent died: current_steps 60 total_reward -4.117645030884697\n",
      "Agent died: current_steps 254 total_reward -33.6861466385225\n",
      "Agent died: current_steps 94 total_reward -18.899862365405376\n",
      "Agent died: current_steps 72 total_reward -20.213150467841576\n",
      "Agent died: current_steps 51 total_reward -7.989884006801992\n",
      "Agent died: current_steps 90 total_reward -1.6946289103273346\n",
      "Agent died: current_steps 1599 total_reward -122.55380903005158\n",
      "Agent died: current_steps 123 total_reward -2.927483455463615\n",
      "Agent died: current_steps 59 total_reward -2.6836910330957413\n",
      "Agent died: current_steps 1599 total_reward -114.18583643871438\n",
      "Agent died: current_steps 97 total_reward -11.686802284171188\n",
      "Agent died: current_steps 64 total_reward -10.980643815213815\n",
      "Agent died: current_steps 56 total_reward -1.3141322061146334\n",
      "Agent died: current_steps 54 total_reward -15.22150065363633\n",
      "Agent died: current_steps 846 total_reward -73.20349184670808\n",
      "Agent died: current_steps 1508 total_reward -126.8739194966727\n",
      "Agent died: current_steps 1599 total_reward -112.91376072348925\n",
      "Agent died: current_steps 70 total_reward -13.074239060817277\n",
      "Agent died: current_steps 63 total_reward -17.956983396156385\n",
      "Agent died: current_steps 1599 total_reward -115.00113006010125\n",
      "Agent died: current_steps 75 total_reward -5.705903450209021\n",
      "Agent died: current_steps 102 total_reward -12.369766774401059\n",
      "Agent died: current_steps 77 total_reward -7.9842660514178325\n",
      "Agent died: current_steps 71 total_reward -16.18586403637876\n",
      "Agent died: current_steps 87 total_reward -7.902328044507031\n",
      "Agent died: current_steps 60 total_reward -10.458949777630473\n",
      "Agent died: current_steps 1599 total_reward -115.14161961476552\n",
      "Agent died: current_steps 70 total_reward -3.298570297000315\n",
      "Agent died: current_steps 55 total_reward -17.217345967282235\n",
      "global_step 60000\n",
      "global_step 60000\n",
      "global_step 60000\n",
      "Agent died: current_steps 1599 total_reward -112.48664358659775\n",
      "Agent died: current_steps 54 total_reward -4.90771969976152\n",
      "Agent died: current_steps 303 total_reward -33.7942499929039\n",
      "Agent died: current_steps 103 total_reward -4.079784606863425\n",
      "Agent died: current_steps 1599 total_reward -113.22225101879636\n",
      "Agent died: current_steps 1599 total_reward -116.13984251358947\n",
      "Agent died: current_steps 1599 total_reward -104.0637875058132\n",
      "Agent died: current_steps 1599 total_reward -119.0910228421514\n",
      "Agent died: current_steps 1599 total_reward -119.66266670058646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent died: current_steps 105 total_reward -11.398429974893098\n",
      "Agent died: current_steps 45 total_reward -4.119123979094127\n",
      "Agent died: current_steps 47 total_reward -11.728435773423564\n",
      "Agent died: current_steps 302 total_reward -31.495016551648504\n",
      "Agent died: current_steps 57 total_reward -7.693122034346688\n",
      "Agent died: current_steps 68 total_reward -19.5083536234113\n",
      "Agent died: current_steps 81 total_reward -7.918883517909178\n",
      "Agent died: current_steps 64 total_reward -17.763916383607928\n",
      "Agent died: current_steps 76 total_reward -1.672378878562405\n",
      "Agent died: current_steps 60 total_reward -2.669707521930961\n",
      "Agent died: current_steps 67 total_reward -7.970459649636718\n",
      "Agent died: current_steps 88 total_reward -23.29064004951094\n",
      "Agent died: current_steps 1599 total_reward -109.95860336804232\n",
      "Agent died: current_steps 1599 total_reward -110.76446184033249\n",
      "Agent died: current_steps 1599 total_reward -131.84874814641378\n",
      "Agent died: current_steps 163 total_reward -28.273433755559203\n",
      "Agent died: current_steps 76 total_reward -7.461057468142977\n",
      "Agent died: current_steps 51 total_reward -5.877606197010105\n",
      "Agent died: current_steps 78 total_reward -4.519858467614283\n",
      "Agent died: current_steps 45 total_reward -7.572361803979923\n",
      "Agent died: current_steps 106 total_reward -11.276973959696782\n",
      "Agent died: current_steps 61 total_reward -2.6517864276990326\n",
      "Agent died: current_steps 65 total_reward -6.06077222162485\n",
      "Agent died: current_steps 77 total_reward -19.177863191011056\n",
      "Agent died: current_steps 79 total_reward -0.9451601681864523\n",
      "Agent died: current_steps 48 total_reward -12.96332318352721\n",
      "Agent died: current_steps 965 total_reward -90.43085954237101\n",
      "Agent died: current_steps 122 total_reward 1.2774711251482285\n",
      "Agent died: current_steps 136 total_reward -25.11802803992291\n",
      "Agent died: current_steps 72 total_reward -2.4721638947905413\n",
      "Agent died: current_steps 54 total_reward -17.864852830972524\n",
      "Agent died: current_steps 75 total_reward -9.580893768945087\n",
      "Agent died: current_steps 82 total_reward -18.687780693948266\n",
      "Agent died: current_steps 1599 total_reward -110.8239026878646\n",
      "Agent died: current_steps 60 total_reward -7.157723410027723\n",
      "Agent died: current_steps 87 total_reward -27.41646550432035\n",
      "Agent died: current_steps 34 total_reward -12.024352169157314\n",
      "Agent died: current_steps 72 total_reward -20.907945117029048\n",
      "Agent died: current_steps 1599 total_reward -115.39263737295221\n",
      "Agent died: current_steps 104 total_reward -0.2746273375780609\n",
      "Agent died: current_steps 63 total_reward -15.360002740685523\n",
      "Agent died: current_steps 66 total_reward -4.737277753775935\n",
      "Agent died: current_steps 77 total_reward -4.656775154711681\n",
      "Agent died: current_steps 523 total_reward -49.60803772619113\n",
      "Agent died: current_steps 73 total_reward -20.19187824514694\n",
      "Agent died: current_steps 60 total_reward -2.62201544970957\n",
      "Agent died: current_steps 1457 total_reward -120.43076940914108\n",
      "Agent died: current_steps 54 total_reward -5.265068673606964\n",
      "Agent died: current_steps 84 total_reward -19.35018578218544\n",
      "Agent died: current_steps 1599 total_reward -120.290398206527\n",
      "Agent died: current_steps 75 total_reward -6.821945718452335\n",
      "Agent died: current_steps 63 total_reward -4.450855383093785\n",
      "Agent died: current_steps 1599 total_reward -119.21223783710724\n",
      "Agent died: current_steps 1599 total_reward -112.45015924500738\n",
      "Agent died: current_steps 102 total_reward -15.342948867629577\n",
      "global_step 70000\n",
      "global_step 70000\n",
      "global_step 70000\n",
      "Agent died: current_steps 1599 total_reward -92.77558760323642\n",
      "Agent died: current_steps 79 total_reward 1.449684993559491\n",
      "Agent died: current_steps 69 total_reward -5.278684277191139\n",
      "Agent died: current_steps 77 total_reward 0.9905594365087597\n",
      "Agent died: current_steps 1599 total_reward -111.48419673032954\n",
      "Agent died: current_steps 53 total_reward -15.459130845202132\n",
      "Agent died: current_steps 1599 total_reward -123.09952151582422\n",
      "Agent died: current_steps 66 total_reward -4.646230017425492\n",
      "Agent died: current_steps 115 total_reward -5.925464145688957\n",
      "Agent died: current_steps 66 total_reward -6.506186878320441\n",
      "Agent died: current_steps 36 total_reward -12.680860252767799\n",
      "Agent died: current_steps 51 total_reward -17.941101110141723\n",
      "Agent died: current_steps 93 total_reward -2.0924621996724366\n",
      "Agent died: current_steps 71 total_reward -13.653667068717372\n",
      "Agent died: current_steps 66 total_reward -16.840835087013115\n",
      "Agent died: current_steps 1599 total_reward -119.73213092004221\n",
      "Agent died: current_steps 1599 total_reward -105.51527359786807\n",
      "Agent died: current_steps 1599 total_reward -130.4972388463726\n",
      "Agent died: current_steps 57 total_reward -15.020225260361403\n",
      "Agent died: current_steps 496 total_reward -54.27562569668949\n",
      "Agent died: current_steps 1599 total_reward -106.2405110306859\n",
      "Agent died: current_steps 201 total_reward -35.480629909618415\n",
      "Agent died: current_steps 172 total_reward -16.808634101767694\n",
      "Agent died: current_steps 46 total_reward -11.844120895419273\n",
      "Agent died: current_steps 386 total_reward -49.928630101466496\n",
      "Agent died: current_steps 720 total_reward -76.0151323943237\n",
      "Agent died: current_steps 1599 total_reward -114.92906595823915\n",
      "Agent died: current_steps 58 total_reward -18.80898171549415\n",
      "Agent died: current_steps 153 total_reward -24.199107635968655\n",
      "Agent died: current_steps 47 total_reward -14.068178788349657\n",
      "Agent died: current_steps 89 total_reward -21.271041633031\n",
      "Agent died: current_steps 1599 total_reward -99.986717445191\n",
      "Agent died: current_steps 87 total_reward 0.055118043540047026\n",
      "Agent died: current_steps 85 total_reward -1.0326040905894118\n",
      "Agent died: current_steps 80 total_reward -7.6374799702124\n",
      "Agent died: current_steps 78 total_reward -9.00720324270862\n",
      "Agent died: current_steps 48 total_reward -13.06455188713781\n",
      "Agent died: current_steps 1599 total_reward -109.03207343196621\n",
      "Agent died: current_steps 53 total_reward -1.9487966691205898\n",
      "Agent died: current_steps 60 total_reward 0.4317435710399093\n",
      "Agent died: current_steps 84 total_reward -6.498816347933677\n",
      "Agent died: current_steps 1599 total_reward -100.93213256211834\n",
      "Agent died: current_steps 77 total_reward -3.0495923882952596\n",
      "Agent died: current_steps 107 total_reward -22.052269804746523\n",
      "Agent died: current_steps 1599 total_reward -107.73480031037471\n",
      "Agent died: current_steps 64 total_reward -4.956422160973776\n",
      "Agent died: current_steps 1599 total_reward -120.75679816703128\n",
      "Agent died: current_steps 557 total_reward -61.49977368329837\n",
      "Agent died: current_steps 74 total_reward -4.2383000956885555\n",
      "Agent died: current_steps 77 total_reward -22.93990153742643\n",
      "Agent died: current_steps 69 total_reward -20.49122559973971\n",
      "Agent died: current_steps 1599 total_reward -112.49683630312657\n",
      "Agent died: current_steps 1599 total_reward -107.53559556634093\n",
      "Agent died: current_steps 60 total_reward -17.095406763782094\n",
      "Agent died: current_steps 1599 total_reward -108.03736862303826\n",
      "Agent died: current_steps 59 total_reward -17.366760847138234\n",
      "global_step 80000\n",
      "global_step 80000\n",
      "global_step 80000\n",
      "Agent died: current_steps 99 total_reward -23.421985575389108\n",
      "Agent died: current_steps 234 total_reward -28.571102618687437\n",
      "Agent died: current_steps 1356 total_reward -113.04948959507988\n",
      "Agent died: current_steps 60 total_reward -5.962479665022347\n",
      "Agent died: current_steps 1519 total_reward -123.46680854351571\n",
      "Agent died: current_steps 118 total_reward -11.905756333929173\n",
      "Agent died: current_steps 68 total_reward -5.6977102399331825\n",
      "Agent died: current_steps 66 total_reward -19.867548746269822\n",
      "Agent died: current_steps 95 total_reward -6.4293322940071365\n",
      "Agent died: current_steps 52 total_reward -13.519831672384711\n",
      "Agent died: current_steps 69 total_reward -5.374253367703407\n",
      "Agent died: current_steps 73 total_reward -1.745592600830526\n",
      "Agent died: current_steps 72 total_reward -5.027790550912412\n",
      "Agent died: current_steps 84 total_reward -2.704535903630159\n",
      "Agent died: current_steps 1599 total_reward -119.94028269562189\n",
      "Agent died: current_steps 109 total_reward -19.037261855369422\n",
      "Agent died: current_steps 1599 total_reward -120.94766238544386\n",
      "Agent died: current_steps 1599 total_reward -120.15059042941314\n",
      "Agent died: current_steps 64 total_reward -1.8833602790410358\n",
      "Agent died: current_steps 71 total_reward 1.397025945485257\n",
      "Agent died: current_steps 79 total_reward -23.37992621858977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent died: current_steps 92 total_reward -0.5173514868362464\n",
      "Agent died: current_steps 1599 total_reward -128.34174589222903\n",
      "Agent died: current_steps 59 total_reward -5.011976726893338\n",
      "Agent died: current_steps 68 total_reward -15.733899857864406\n",
      "Agent died: current_steps 56 total_reward -2.783477669229734\n",
      "Agent died: current_steps 1599 total_reward -119.9460639490416\n",
      "Agent died: current_steps 100 total_reward -23.53707064264516\n",
      "Agent died: current_steps 50 total_reward -5.351259150027607\n",
      "Agent died: current_steps 57 total_reward -3.2782764983984367\n",
      "Agent died: current_steps 47 total_reward -3.7931750111151508\n",
      "Agent died: current_steps 34 total_reward -12.689976281870159\n",
      "Agent died: current_steps 94 total_reward -9.423848137706509\n",
      "Agent died: current_steps 1599 total_reward -115.69352260065662\n",
      "Agent died: current_steps 1599 total_reward -105.02359652569214\n",
      "Agent died: current_steps 1599 total_reward -109.37818228451277\n",
      "Agent died: current_steps 1383 total_reward -101.01731727027031\n",
      "Agent died: current_steps 256 total_reward -32.0225631456629\n",
      "Agent died: current_steps 45 total_reward -16.539839713851613\n",
      "Agent died: current_steps 59 total_reward -20.717597495129958\n",
      "Agent died: current_steps 74 total_reward -7.663668359853951\n",
      "Agent died: current_steps 38 total_reward -13.7361378359745\n",
      "Agent died: current_steps 55 total_reward -4.517012232561907\n",
      "Agent died: current_steps 43 total_reward -14.102338041686139\n",
      "Agent died: current_steps 326 total_reward -48.03077936140634\n",
      "Agent died: current_steps 1599 total_reward -101.42869404972548\n",
      "Agent died: current_steps 79 total_reward -2.6064240586310667\n",
      "Agent died: current_steps 69 total_reward -5.017881521537282\n",
      "Agent died: current_steps 164 total_reward -32.70126155707487\n",
      "Agent died: current_steps 129 total_reward -20.674764747203596\n",
      "Agent died: current_steps 71 total_reward -7.072191597027704\n",
      "Agent died: current_steps 46 total_reward -15.880501244110988\n",
      "Agent died: current_steps 71 total_reward -20.963141021588807\n",
      "Agent died: current_steps 395 total_reward -55.58849798528849\n",
      "Agent died: current_steps 81 total_reward -18.867563697695726\n",
      "Agent died: current_steps 54 total_reward -10.142002431213852\n",
      "Agent died: current_steps 99 total_reward -21.978562212143697\n",
      "Agent died: current_steps 1599 total_reward -109.43140726547024\n",
      "Agent died: current_steps 49 total_reward -5.505198788808037\n",
      "Agent died: current_steps 76 total_reward -9.68769404956574\n",
      "Agent died: current_steps 90 total_reward -8.883010729628307\n",
      "Agent died: current_steps 1378 total_reward -119.52143580242634\n",
      "Agent died: current_steps 1599 total_reward -108.02484138733656\n",
      "Agent died: current_steps 116 total_reward -35.84061243911584\n",
      "Agent died: current_steps 74 total_reward -5.698397856239231\n",
      "Agent died: current_steps 56 total_reward -4.752843764353546\n",
      "Agent died: current_steps 1599 total_reward -86.6728750883434\n",
      "Agent died: current_steps 53 total_reward -3.862496245487287\n",
      "Agent died: current_steps 79 total_reward 1.2882616558677187\n",
      "Agent died: current_steps 52 total_reward -2.0723858128103156\n",
      "global_step 90000\n",
      "global_step 90000\n",
      "global_step 90000\n",
      "Agent died: current_steps 1599 total_reward -111.27864657070063\n",
      "Agent died: current_steps 201 total_reward -32.32046932866112\n",
      "Agent died: current_steps 75 total_reward 2.2319467481958344\n",
      "Agent died: current_steps 55 total_reward -11.671981735656662\n",
      "Agent died: current_steps 1599 total_reward -121.6031466405238\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    # I don't think I need this method.\n",
    "    return obs\n",
    "\n",
    "n_steps = 210000\n",
    "# Start training the critic DQN after 1000 game iterations.\n",
    "# This has to be a lot bigger than the batch_size defined below.\n",
    "training_start = 1000\n",
    "training_interval = 3 # Run a training step every 3 game iterations start training_start.\n",
    "save_steps = 50\n",
    "copy_steps = 25 # Copy the critic to the actor every 25 training steps.\n",
    "discount_rate = 0.99999 # 0.95\n",
    "batch_size = 50\n",
    "iteration = 0\n",
    "checkpoint_path = \"./BipedalWalker-v2.ckpt\"\n",
    "done = True # Environment needs to be reset\n",
    "total_reward = 0\n",
    "current_steps = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        print(\"Restoring checkpoint\")\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        elif step != 0and step % 10000 == 0:\n",
    "            print(\"global_step\", step)\n",
    "        iteration += 1\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            state = preprocess_observation(obs)\n",
    "            total_reward = 0\n",
    "            current_steps = 0\n",
    "        \n",
    "        # Actor evaluates what to do\n",
    "        q_values = actor_q_values.eval(feed_dict={X_state: [state]})\n",
    "        #print(q_values)\n",
    "        action_index = epsilon_greedy(q_values, step)\n",
    "        action = action_index_to_action(action_index)\n",
    "        #print(action)\n",
    "        \n",
    "        #render_env(env)\n",
    "        # Actor takes action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Agent died: current_steps\", current_steps, \"total_reward\", total_reward)\n",
    "            render_env(env)\n",
    "        elif (150 < current_steps and current_steps % 10 == 0) or step > 90000:\n",
    "            render_env(env)\n",
    "        total_reward += reward\n",
    "        current_steps += 1\n",
    "        next_state = preprocess_observation(obs)\n",
    "        \n",
    "        # Memorise action.\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "        \n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue\n",
    "        \n",
    "        # Train the critic.\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "            sample_memory(batch_size))\n",
    "        next_q_values = actor_q_values.eval(feed_dict={X_state: X_next_state_val})\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        # Calculate the Actor's reward + future discounted estimated Q-value.\n",
    "        #print(\"rewards\", rewards.shape, \"continues\", continues.shape, \"max_next_q_values\", max_next_q_values.shape)\n",
    "        y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "        #print(X_state, X_action, y)\n",
    "        #print(\"X_state_val\", X_state_val.shape, \"X_action\", X_action_val.shape, \"y_val\", y_val.shape)\n",
    "        training_op.run(feed_dict={X_state: X_state_val,\n",
    "                                  X_action: X_action_val,\n",
    "                                  y: y_val})\n",
    "        \n",
    "        if step % copy_steps == 0:\n",
    "            copy_critic_to_actor.run()\n",
    "        \n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For:\n",
    "neurons_per_layer = [40, 10]\n",
    "learning_rate = 0.001\n",
    "\n",
    "The RL agent learnt how to get an average reward of 3 +- 2. I assume it is learning how to fall over. This is good progress. Running the agent with these hyper paremeters again, did not produce these same results unfortunately.\n",
    "\n",
    "Rendering the environment has shown that a reward of 3-5 is probably just the agent falling as far forward as possible. Dumb robot.\n",
    "\n",
    "After rendering the environment every 50 frames, I can see that it either falls over quickly or it splits it legs and very slowly inches forward and then gets killed after 1599 steps.\n",
    "\n",
    "I couldn't get the agent to learn how to walk and when I checked my code, I realised that I was using the Q-values from the agent as the torque values for it's action, which basically made it stateless (I think).\n",
    "\n",
    "Watching the agent, I could see that it was using too much force and flipping itself when it wanted to take a step. Let's add some granularity to the torque to see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
