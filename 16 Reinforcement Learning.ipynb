{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.layers import batch_norm, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8\n",
    "===\n",
    "\n",
    "Create a DQN to solve the \"BipedalWalker-v2\" gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.74735666e-03 -1.91480014e-05  1.48950197e-03 -1.59998703e-02\n",
      "  9.18869674e-02 -1.96562964e-03  8.60310689e-01  2.85527421e-03\n",
      "  1.00000000e+00  3.22923772e-02 -1.96549902e-03  8.53861332e-01\n",
      "  1.39011598e-03  1.00000000e+00  4.40814108e-01  4.45820212e-01\n",
      "  4.61422890e-01  4.89550292e-01  5.34102917e-01  6.02461159e-01\n",
      "  7.09149063e-01  8.85932028e-01  1.00000000e+00  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"BipedalWalker-v2\")\n",
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb389b39c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def render_env(env):\n",
    "    plt.imshow(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "render_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'contains', 'from_jsonable', 'high', 'low', 'sample', 'shape', 'to_jsonable']\n",
      "(4,)\n",
      "[-1 -1 -1 -1]\n",
      "[1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(dir(env.action_space))\n",
    "print(env.action_space.shape)\n",
    "print(env.action_space.low)\n",
    "print(env.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "neurons_per_layer = [50, 50]\n",
    "n_outputs = env.action_space.shape[0]\n",
    "\n",
    "def q_network(X_state, scope):\n",
    "    current_layer = X_state\n",
    "    layers = []\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        for neurons in neurons_per_layer:\n",
    "            current_layer = fully_connected(current_layer, neurons)\n",
    "        # We use the tanh function because the output ranges from -1 to 1.\n",
    "        outputs = fully_connected(current_layer, n_outputs, activation_fn=tf.nn.tanh)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name)]: var for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,)\n"
     ]
    }
   ],
   "source": [
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_state = tf.placeholder(tf.float32, shape=[None, obs.shape[0]])\n",
    "actor_q_values, actor_vars = q_network(X_state, \"q_networks/actor\")\n",
    "critic_q_values, critic_vars = q_network(X_state, \"q_networks/critic\")\n",
    "\n",
    "copy_ops = [actor_var.assign(critic_vars[var_name])\n",
    "            for var_name, actor_var in actor_vars.items()]\n",
    "copy_critic_to_actor = tf.group(*copy_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "# This contains the Q-value for the actor's chosen action.\n",
    "q_value = tf.reduce_sum(critic_q_values * tf.one_hot(X_action, n_outputs), axis=1, keep_dims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Actor's reward + future discounted estimated Q-value.\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "cost = tf.reduce_mean(tf.square(y - q_value))\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "training_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory_size = 10000\n",
    "replay_memory = deque([], maxlen=replay_memory_size)\n",
    "\n",
    "def sample_memory(batch_size):\n",
    "    indices = rnd.permuation(len(replay_memory))[:batch_size]\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for idx in indices:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0], cosl[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon greed algorithm to explore the state-action space of the environment at the beginning.\n",
    "def epsilon_greedy(q_values, step):\n",
    "    eps_min = 0.05 # At the start 5% of the time the actor will choose the greedy action.\n",
    "    eps_max = 1.0 # Once it has finished exploring, the actor will choose the greedy action 100% of the time.\n",
    "    eps_decay_steps = 50000\n",
    "    \n",
    "    epsilon = max(eps_min, eps_max - (eps_max - eps_min) * step / eps_decay_steps)\n",
    "    if rnd.rand() < epsilon:\n",
    "        return rnd.randin(n_output)\n",
    "    else:\n",
    "        return np.argmax(q_values) # The greedy/optimal action is the one with the highest estimate Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_observation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-181b59b0ac45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Actor evaluates what to do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess_observation' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "n_steps = 100000\n",
    "# Start training the critic DQN after 1000 game iterations.\n",
    "# This has to be a lot bigger than the batch_size defined below.\n",
    "training_start = 1000\n",
    "training_interval = 3 # Run a training step every 3 game iterations start training_start.\n",
    "save_steps = 50\n",
    "copy_steps = 25 # Copy the critic to the actor every 25 training steps.\n",
    "discount_rate = 0.95\n",
    "batch_size = 50\n",
    "iteration = 0\n",
    "checkpoint_path = \"./BipedalWalker-v2.ckpt\"\n",
    "done = True # Environment needs to be reset\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            state = preprocess_observation(obs)\n",
    "        \n",
    "        # Actor evaluates what to do\n",
    "        q_values = actor_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = epsilon_greed(q_values, step)\n",
    "        \n",
    "        # Actor takes action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observation(obs)\n",
    "        \n",
    "        # Memorise action.\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "        \n",
    "        if iteration < training_start or iteration % training_iterval != 0:\n",
    "            continue\n",
    "        \n",
    "        # Train the critic.\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "            sample_memories(batch_size))\n",
    "        next_q_values = actor_q_values.eval(feed_dict={X_state: X_next_state_val})\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        # Calculate the Actor's reward + future discounted estimated Q-value.\n",
    "        y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "        training_op.run(feed_dict={X_state: X_state_val,\n",
    "                                  X_action: X_action_val,\n",
    "                                  y: y_val})\n",
    "        \n",
    "        if step % copy_steps == 0:\n",
    "            copy_critic_to_actor.run()\n",
    "        \n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
