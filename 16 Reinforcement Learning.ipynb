{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.layers import batch_norm, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8\n",
    "===\n",
    "\n",
    "Create a DQN to solve the \"BipedalWalker-v2\" gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.74735712e-03 -2.07262044e-05  1.61228981e-03 -1.59998643e-02\n",
      "  9.18657258e-02 -2.12767534e-03  8.60322699e-01  2.97495971e-03\n",
      "  1.00000000e+00  3.22708003e-02 -2.12752772e-03  8.53874087e-01\n",
      "  1.50471103e-03  1.00000000e+00  4.40814108e-01  4.45820212e-01\n",
      "  4.61422890e-01  4.89550292e-01  5.34102917e-01  6.02461159e-01\n",
      "  7.09149063e-01  8.85932028e-01  1.00000000e+00  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"BipedalWalker-v2\")\n",
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def render_env(env):\n",
    "    plt.imshow(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "#render_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "(4,)\n",
      "[-1 -1 -1 -1]\n",
      "[1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.action_space.shape)\n",
    "print(env.action_space.low)\n",
    "print(env.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_discretized_actions 3 n_actions 4 n_outputs 81\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "neurons_per_layer = [40]#[40, 40, 20]\n",
    "learning_rate = 0.001 #0.001\n",
    "n_discretized_actions = 3 # -1, 0, +1\n",
    "n_actions = env.action_space.shape[0]\n",
    "#n_outputs = env.action_space.shape[0]\n",
    "n_outputs = n_discretized_actions ** n_actions\n",
    "print(\"n_discretized_actions\", n_discretized_actions, \"n_actions\", n_actions, \"n_outputs\", n_outputs)\n",
    "\n",
    "def q_network(X_state, scope):\n",
    "    current_layer = X_state\n",
    "    layers = []\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        for neurons in neurons_per_layer:\n",
    "            current_layer = fully_connected(current_layer, neurons)#, activation_fn=tf.nn.sigmoid)\n",
    "            #print(\"current_layer shape\", current_layer.get_shape())\n",
    "        #print(\"input shape\", current_layer.get_shape())\n",
    "        # We use the tanh function because the output ranges from -1 to 1.\n",
    "        outputs = fully_connected(current_layer, n_outputs, activation_fn=tf.nn.tanh)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name)]: var for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,)\n"
     ]
    }
   ],
   "source": [
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_state = tf.placeholder(tf.float32, shape=[None, obs.shape[0]])\n",
    "actor_q_values, actor_vars = q_network(X_state, \"q_networks/actor\")\n",
    "critic_q_values, critic_vars = q_network(X_state, \"q_networks/critic\")\n",
    "\n",
    "copy_ops = [actor_var.assign(critic_vars[var_name])\n",
    "            for var_name, actor_var in actor_vars.items()]\n",
    "copy_critic_to_actor = tf.group(*copy_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic_q_values.get_shape() (?, 81) q_value.get_shape() (?, 1)\n"
     ]
    }
   ],
   "source": [
    "X_action = tf.placeholder(tf.int32, shape=[None, env.action_space.shape[0]])\n",
    "# This contains the Q-value for the actor's chosen action.\n",
    "# q_value = tf.reduce_sum(critic_q_values * tf.one_hot(X_action, n_outputs), axis=1, keep_dims=True)\n",
    "q_value = tf.reduce_sum(critic_q_values * tf.one_hot(n_outputs, 1), axis=1, keep_dims=True)\n",
    "#q_value = critic_q_values\n",
    "print(\"critic_q_values.get_shape()\", critic_q_values.shape, \"q_value.get_shape()\", q_value.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Actor's reward + future discounted estimated Q-value.\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "cost = tf.reduce_mean(tf.square(y - q_value))\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory_size = 10000\n",
    "replay_memory = deque([], maxlen=replay_memory_size)\n",
    "\n",
    "def sample_memory(batch_size):\n",
    "    indices = np.random.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for idx in indices:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 4)\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "discretized_actions = [-1, 0, 1]\n",
    "action_space_universe = np.array(list(product(discretized_actions, discretized_actions, discretized_actions, discretized_actions)))\n",
    "\n",
    "print(action_space_universe.shape)\n",
    "#print(action_space_universe)\n",
    "\n",
    "def action_index_to_action(action_index):\n",
    "    return action_space_universe[action_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon greed algorithm to explore the state-action space of the environment at the beginning.\n",
    "def epsilon_greedy(q_values, step):\n",
    "    eps_min = 0.05 # At the start 5% of the time the actor will choose the greedy action.\n",
    "    eps_max = 1.0 # Once it has finished exploring, the actor will choose the greedy action 100% of the time.\n",
    "    eps_decay_steps = 100000\n",
    "    \n",
    "    epsilon = max(eps_min, eps_max - (eps_max - eps_min) * step / eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        #return np.random.uniform(env.action_space.low, env.action_space.high, size=n_actions)\n",
    "        return np.random.randint(len(action_space_universe))\n",
    "    else:\n",
    "        #return q_values.reshape(n_outputs)\n",
    "        return np.argmax(q_values) # the optimal/greedy action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent died: current_steps 88 total_reward -15.973937595615778\n",
      "Agent died: current_steps 72 total_reward -13.45541904788898\n",
      "Agent died: current_steps 53 total_reward -4.201687112902602\n",
      "Agent died: current_steps 59 total_reward -2.7168254013545843\n",
      "Agent died: current_steps 137 total_reward -14.938071741284773\n",
      "Agent died: current_steps 72 total_reward 0.08735586924540233\n",
      "Agent died: current_steps 1599 total_reward -122.87969524368668\n",
      "Agent died: current_steps 213 total_reward -26.68893238030558\n",
      "Agent died: current_steps 1599 total_reward -119.85622503653025\n",
      "Agent died: current_steps 52 total_reward -14.391935125932097\n",
      "Agent died: current_steps 71 total_reward -7.664384290754172\n",
      "Agent died: current_steps 1599 total_reward -94.91008623886334\n",
      "Agent died: current_steps 91 total_reward -20.73060168629947\n",
      "Agent died: current_steps 1599 total_reward -108.66638557585858\n",
      "Agent died: current_steps 49 total_reward -7.636111337793987\n",
      "Agent died: current_steps 1599 total_reward -130.41477990458156\n",
      "Agent died: current_steps 1599 total_reward -116.5903945704551\n",
      "Agent died: current_steps 1599 total_reward -123.07764832487855\n",
      "Agent died: current_steps 89 total_reward -6.5770813218330275\n",
      "Agent died: current_steps 118 total_reward -17.768946949937668\n",
      "Agent died: current_steps 1599 total_reward -102.23344270234146\n",
      "Agent died: current_steps 52 total_reward -12.77174383625202\n",
      "Agent died: current_steps 80 total_reward -17.837851244634013\n",
      "Agent died: current_steps 84 total_reward -9.001787829434493\n",
      "Agent died: current_steps 112 total_reward -13.31972565094193\n",
      "Agent died: current_steps 125 total_reward 1.2098300059717202\n",
      "Agent died: current_steps 80 total_reward -19.23306172365383\n",
      "Agent died: current_steps 83 total_reward -6.633844734845686\n",
      "Agent died: current_steps 37 total_reward -8.3667283930319\n",
      "Agent died: current_steps 66 total_reward -0.7755885435299328\n",
      "Agent died: current_steps 50 total_reward -14.813913606865949\n",
      "Agent died: current_steps 64 total_reward -8.192161475697535\n",
      "Agent died: current_steps 154 total_reward -24.96673731899631\n",
      "Agent died: current_steps 90 total_reward 0.4002389802218665\n",
      "Agent died: current_steps 681 total_reward -75.8838375300982\n",
      "Agent died: current_steps 1599 total_reward -124.7284894950072\n",
      "Agent died: current_steps 1599 total_reward -105.26380006880838\n",
      "Agent died: current_steps 109 total_reward -33.18193358220408\n",
      "Agent died: current_steps 141 total_reward -26.626712162083003\n",
      "Agent died: current_steps 1599 total_reward -123.13703137472598\n",
      "Agent died: current_steps 1599 total_reward -122.1251991014453\n",
      "Agent died: current_steps 112 total_reward -5.371336542300877\n",
      "Agent died: current_steps 54 total_reward -15.273176998150849\n",
      "Agent died: current_steps 64 total_reward -15.711162372818213\n",
      "Agent died: current_steps 70 total_reward -1.8002906420926257\n",
      "Agent died: current_steps 52 total_reward -16.48621528538565\n",
      "Agent died: current_steps 99 total_reward -2.3543826559825494\n",
      "Agent died: current_steps 100 total_reward -9.099673465963438\n",
      "Agent died: current_steps 61 total_reward -8.377903429235023\n",
      "Agent died: current_steps 61 total_reward -12.361300862536451\n",
      "Agent died: current_steps 66 total_reward -2.4452739598695192\n",
      "Agent died: current_steps 62 total_reward -15.904509886267904\n",
      "Agent died: current_steps 1599 total_reward -118.98926043058304\n",
      "Agent died: current_steps 65 total_reward -14.394997976566355\n",
      "Agent died: current_steps 55 total_reward -4.80872641210692\n",
      "Agent died: current_steps 54 total_reward -14.921094814305505\n",
      "Agent died: current_steps 74 total_reward -20.22004596423917\n",
      "Agent died: current_steps 92 total_reward 1.2298508886968111\n",
      "Agent died: current_steps 369 total_reward -39.608815399202776\n",
      "Agent died: current_steps 38 total_reward -8.560140479376539\n",
      "Agent died: current_steps 59 total_reward -16.878227986449374\n",
      "Agent died: current_steps 72 total_reward -3.6306234725881406\n",
      "Agent died: current_steps 65 total_reward -3.5497988278033845\n",
      "Agent died: current_steps 72 total_reward -15.184216309252506\n",
      "Agent died: current_steps 768 total_reward -76.25958655938763\n",
      "Agent died: current_steps 89 total_reward -19.032340070648736\n",
      "Agent died: current_steps 48 total_reward -15.663138471187402\n",
      "Agent died: current_steps 1599 total_reward -123.58083523661419\n",
      "Agent died: current_steps 46 total_reward -7.231284475782884\n",
      "Agent died: current_steps 1599 total_reward -117.74229594604256\n",
      "Agent died: current_steps 58 total_reward -12.992193029972409\n",
      "Agent died: current_steps 72 total_reward -3.0917789031962517\n",
      "Agent died: current_steps 87 total_reward -21.22345699809367\n",
      "global_step 10000\n",
      "global_step 10000\n",
      "global_step 10000\n",
      "Agent died: current_steps 1599 total_reward -118.26474681783507\n",
      "Agent died: current_steps 1599 total_reward -114.72722394775872\n",
      "Agent died: current_steps 63 total_reward -17.484584717490396\n",
      "Agent died: current_steps 53 total_reward -4.591497501585013\n",
      "Agent died: current_steps 1599 total_reward -119.66099465663655\n",
      "Agent died: current_steps 255 total_reward -35.03553074122899\n",
      "Agent died: current_steps 66 total_reward -3.17372249584521\n",
      "Agent died: current_steps 57 total_reward -13.667969063898543\n",
      "Agent died: current_steps 1599 total_reward -126.20589573185158\n",
      "Agent died: current_steps 1599 total_reward -122.1455579528502\n",
      "Agent died: current_steps 1599 total_reward -105.31132371375982\n",
      "Agent died: current_steps 1599 total_reward -121.6036086600945\n",
      "Agent died: current_steps 52 total_reward -1.133922105387477\n",
      "Agent died: current_steps 1599 total_reward -107.90550589693002\n",
      "Agent died: current_steps 1599 total_reward -117.25654429137978\n",
      "Agent died: current_steps 1599 total_reward -111.73321363566588\n",
      "Agent died: current_steps 1599 total_reward -114.40220067036054\n",
      "Agent died: current_steps 83 total_reward -4.441064294391618\n",
      "Agent died: current_steps 62 total_reward -13.634910931671032\n",
      "Agent died: current_steps 1599 total_reward -112.60545723145805\n",
      "Agent died: current_steps 988 total_reward -99.02235752585919\n",
      "Agent died: current_steps 652 total_reward -73.54316588428117\n",
      "Agent died: current_steps 76 total_reward -2.1448294691816017\n",
      "Agent died: current_steps 1599 total_reward -130.85367173118243\n",
      "Agent died: current_steps 57 total_reward -18.432691626487923\n",
      "Agent died: current_steps 1540 total_reward -135.2498876753621\n",
      "Agent died: current_steps 64 total_reward -14.887193741217864\n",
      "Agent died: current_steps 1599 total_reward -101.82493711978702\n",
      "Agent died: current_steps 68 total_reward 0.21234562061229784\n",
      "Agent died: current_steps 1599 total_reward -124.654190834349\n",
      "Agent died: current_steps 1599 total_reward -124.32747344812296\n",
      "Agent died: current_steps 69 total_reward -4.301416935248926\n",
      "global_step 20000\n",
      "global_step 20000\n",
      "global_step 20000\n",
      "Agent died: current_steps 1599 total_reward -118.52287935542286\n",
      "Agent died: current_steps 58 total_reward -10.959066608547541\n",
      "Agent died: current_steps 81 total_reward -13.273171744671334\n",
      "Agent died: current_steps 61 total_reward -4.950204251356425\n",
      "Agent died: current_steps 1599 total_reward -112.64979017333525\n",
      "Agent died: current_steps 1599 total_reward -127.63613700882932\n",
      "Agent died: current_steps 1599 total_reward -122.63313668868048\n",
      "Agent died: current_steps 107 total_reward -3.405102506408463\n",
      "Agent died: current_steps 53 total_reward -11.286112706509106\n",
      "Agent died: current_steps 108 total_reward -23.8553298612833\n",
      "Agent died: current_steps 69 total_reward -10.23251119528897\n",
      "Agent died: current_steps 44 total_reward -10.439035410093142\n",
      "Agent died: current_steps 43 total_reward -10.55551701135747\n",
      "Agent died: current_steps 60 total_reward -15.367001553560916\n",
      "Agent died: current_steps 1599 total_reward -116.38698496669068\n",
      "Agent died: current_steps 58 total_reward -12.030140942746769\n",
      "Agent died: current_steps 112 total_reward -16.592397822499255\n",
      "Agent died: current_steps 51 total_reward -4.933360204580547\n",
      "Agent died: current_steps 1599 total_reward -112.04660056976685\n",
      "Agent died: current_steps 1599 total_reward -123.53297448696487\n",
      "Agent died: current_steps 85 total_reward -20.805986488992346\n",
      "Agent died: current_steps 57 total_reward -12.231344147526343\n",
      "Agent died: current_steps 95 total_reward -17.029043177072555\n",
      "Agent died: current_steps 337 total_reward -41.26042318248559\n",
      "Agent died: current_steps 1599 total_reward -105.61868218403652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent died: current_steps 1599 total_reward -108.14392995958421\n",
      "Agent died: current_steps 61 total_reward -12.955021409107363\n",
      "Agent died: current_steps 1599 total_reward -103.8742836629783\n",
      "Agent died: current_steps 84 total_reward -3.8199603649936598\n",
      "Agent died: current_steps 1599 total_reward -133.94618840732545\n",
      "Agent died: current_steps 220 total_reward -38.22319666688885\n",
      "Agent died: current_steps 77 total_reward -12.654846983921402\n",
      "Agent died: current_steps 44 total_reward -12.848543182131522\n",
      "Agent died: current_steps 66 total_reward -19.5180803723118\n",
      "Agent died: current_steps 1599 total_reward -120.39897695316584\n",
      "Agent died: current_steps 57 total_reward -13.711500725942354\n",
      "Agent died: current_steps 65 total_reward -17.92399236278484\n",
      "Agent died: current_steps 1599 total_reward -122.34376900870085\n",
      "Agent died: current_steps 1599 total_reward -122.09429701803217\n",
      "Agent died: current_steps 63 total_reward -2.327600962860512\n",
      "Agent died: current_steps 80 total_reward -2.040906349827226\n",
      "Agent died: current_steps 1599 total_reward -83.82749596927292\n",
      "Agent died: current_steps 56 total_reward -3.8425436120585896\n",
      "Agent died: current_steps 68 total_reward -3.8037945397080257\n",
      "Agent died: current_steps 35 total_reward -7.274265653780972\n",
      "Agent died: current_steps 1599 total_reward -120.88693994492884\n",
      "Agent died: current_steps 748 total_reward -81.05565715128297\n",
      "Agent died: current_steps 1599 total_reward -136.7190490861019\n",
      "Agent died: current_steps 68 total_reward -0.9907356785082546\n",
      "Agent died: current_steps 115 total_reward -26.349082931694873\n",
      "Agent died: current_steps 99 total_reward -1.3590451832581276\n",
      "Agent died: current_steps 46 total_reward -12.600109854455415\n",
      "Agent died: current_steps 57 total_reward -12.705934723921121\n",
      "global_step 30000\n",
      "global_step 30000\n",
      "global_step 30000\n",
      "Agent died: current_steps 1599 total_reward -120.40943426558793\n",
      "Agent died: current_steps 1599 total_reward -98.31416099051788\n",
      "Agent died: current_steps 1599 total_reward -97.58129680637285\n",
      "Agent died: current_steps 1599 total_reward -129.86542477025188\n",
      "Agent died: current_steps 222 total_reward -32.86542864786892\n",
      "Agent died: current_steps 1599 total_reward -118.87112684121968\n",
      "Agent died: current_steps 147 total_reward -28.103844151829037\n",
      "Agent died: current_steps 62 total_reward -1.1098878767080638\n",
      "Agent died: current_steps 1599 total_reward -84.1348046127785\n",
      "Agent died: current_steps 1599 total_reward -112.51783536273956\n",
      "Agent died: current_steps 1599 total_reward -101.77623851894234\n",
      "Agent died: current_steps 1599 total_reward -88.14707472470427\n",
      "Agent died: current_steps 63 total_reward -24.2070566031908\n",
      "Agent died: current_steps 1599 total_reward -117.00308552492442\n",
      "Agent died: current_steps 69 total_reward -0.9690176545108309\n",
      "Agent died: current_steps 1599 total_reward -106.65408352373915\n",
      "Agent died: current_steps 1599 total_reward -107.64531787272826\n",
      "Agent died: current_steps 60 total_reward -0.5229581948239386\n",
      "Agent died: current_steps 1599 total_reward -118.09160786567611\n",
      "Agent died: current_steps 1599 total_reward -112.10740891373881\n",
      "Agent died: current_steps 67 total_reward -2.0492299310453266\n",
      "Agent died: current_steps 1599 total_reward -98.44203922652974\n",
      "Agent died: current_steps 55 total_reward -12.747352406767506\n",
      "Agent died: current_steps 70 total_reward -22.39044068400065\n",
      "Agent died: current_steps 1599 total_reward -124.47639092813843\n",
      "Agent died: current_steps 1599 total_reward -118.50909178233367\n",
      "Agent died: current_steps 1599 total_reward -98.67738747833506\n",
      "global_step 40000\n",
      "global_step 40000\n",
      "global_step 40000\n",
      "Agent died: current_steps 1599 total_reward -115.91017298526954\n",
      "Agent died: current_steps 1599 total_reward -109.28798617855546\n",
      "Agent died: current_steps 1599 total_reward -94.50359427316026\n",
      "Agent died: current_steps 1599 total_reward -97.91416890106436\n",
      "Agent died: current_steps 1599 total_reward -127.47530284422257\n",
      "Agent died: current_steps 1599 total_reward -111.7861308600011\n",
      "Agent died: current_steps 1599 total_reward -109.14243705989732\n",
      "Agent died: current_steps 58 total_reward -13.884960054562109\n",
      "Agent died: current_steps 71 total_reward 0.19934310430163926\n",
      "Agent died: current_steps 1599 total_reward -108.73058384418623\n",
      "Agent died: current_steps 1599 total_reward -114.01729082544732\n",
      "Agent died: current_steps 94 total_reward -24.34387164374254\n",
      "Agent died: current_steps 86 total_reward -5.626959305062263\n",
      "Agent died: current_steps 67 total_reward -12.117164736244819\n",
      "Agent died: current_steps 1599 total_reward -112.97923828072722\n",
      "Agent died: current_steps 1599 total_reward -109.98777377871883\n",
      "Agent died: current_steps 1599 total_reward -116.2439836698288\n",
      "Agent died: current_steps 1599 total_reward -95.51149229071129\n",
      "Agent died: current_steps 1599 total_reward -109.59017515729863\n",
      "Agent died: current_steps 57 total_reward -12.925602765638375\n",
      "Agent died: current_steps 433 total_reward -17.137935640192147\n",
      "Agent died: current_steps 1599 total_reward -113.40624844970398\n",
      "Agent died: current_steps 1599 total_reward -96.18493744811332\n",
      "Agent died: current_steps 726 total_reward -55.14056300140054\n",
      "Agent died: current_steps 1599 total_reward -107.16499506966468\n",
      "Agent died: current_steps 57 total_reward -15.848006606861325\n",
      "Agent died: current_steps 1599 total_reward -112.13268781662208\n",
      "global_step 50000\n",
      "global_step 50000\n",
      "global_step 50000\n",
      "Agent died: current_steps 82 total_reward -21.890946244474502\n",
      "Agent died: current_steps 1599 total_reward -96.06388521874902\n",
      "Agent died: current_steps 1599 total_reward -110.61116479198215\n",
      "Agent died: current_steps 1599 total_reward -92.69722675290527\n",
      "Agent died: current_steps 54 total_reward -17.064266505838685\n",
      "Agent died: current_steps 379 total_reward -28.086309697043077\n",
      "Agent died: current_steps 1599 total_reward -98.90833658272525\n",
      "Agent died: current_steps 73 total_reward -8.351027001819256\n",
      "Agent died: current_steps 1599 total_reward -103.67370216256764\n",
      "Agent died: current_steps 1599 total_reward -90.68945378043593\n",
      "Agent died: current_steps 1599 total_reward -86.37420553305209\n",
      "Agent died: current_steps 593 total_reward -37.1718529200268\n",
      "Agent died: current_steps 1599 total_reward -108.46935597786326\n",
      "Agent died: current_steps 1599 total_reward -86.49655651436117\n",
      "Agent died: current_steps 53 total_reward -11.808225110431508\n",
      "Agent died: current_steps 1599 total_reward -97.44669735050677\n",
      "Agent died: current_steps 66 total_reward -20.50183524569\n",
      "Agent died: current_steps 1599 total_reward -100.13395231372128\n",
      "Agent died: current_steps 1599 total_reward -94.6933329447385\n",
      "Agent died: current_steps 1599 total_reward -102.26564580867085\n",
      "Agent died: current_steps 1599 total_reward -82.03468788084098\n",
      "Agent died: current_steps 74 total_reward -17.656476743517437\n",
      "Agent died: current_steps 1599 total_reward -102.71150842222724\n",
      "Agent died: current_steps 1599 total_reward -89.05881683921146\n",
      "Agent died: current_steps 1137 total_reward -57.24304494009143\n",
      "Agent died: current_steps 435 total_reward -20.212978814088363\n",
      "Agent died: current_steps 42 total_reward -7.960215948722636\n",
      "global_step 60000\n",
      "global_step 60000\n",
      "global_step 60000\n",
      "Agent died: current_steps 1599 total_reward -109.29425691265881\n",
      "Agent died: current_steps 1599 total_reward -90.1811096729571\n",
      "Agent died: current_steps 1599 total_reward -81.95876804469017\n",
      "Agent died: current_steps 66 total_reward -9.231084608770908\n",
      "Agent died: current_steps 1599 total_reward -84.20481747485988\n",
      "Agent died: current_steps 1118 total_reward -57.837620416869925\n",
      "Agent died: current_steps 70 total_reward -21.676462320577354\n",
      "Agent died: current_steps 74 total_reward -7.978582514977698\n",
      "Agent died: current_steps 112 total_reward -21.763811171401898\n",
      "Agent died: current_steps 1599 total_reward -82.20171571407187\n",
      "Agent died: current_steps 1599 total_reward -69.66167810689441\n",
      "Agent died: current_steps 56 total_reward -9.071004599739487\n",
      "Agent died: current_steps 1599 total_reward -88.62905960587638\n",
      "Agent died: current_steps 1599 total_reward -86.9025055697363\n",
      "Agent died: current_steps 744 total_reward -35.677875598869456\n",
      "Agent died: current_steps 472 total_reward -14.693646500039165\n",
      "Agent died: current_steps 1599 total_reward -95.5418757281948\n",
      "Agent died: current_steps 1599 total_reward -88.110683977601\n",
      "Agent died: current_steps 1599 total_reward -84.10208569195886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent died: current_steps 68 total_reward -16.03703583888958\n",
      "Agent died: current_steps 334 total_reward -10.14673841541504\n",
      "Agent died: current_steps 1599 total_reward -87.04916050532623\n",
      "Agent died: current_steps 1599 total_reward -87.00842457813675\n",
      "Agent died: current_steps 1345 total_reward -80.41504716888464\n",
      "Agent died: current_steps 71 total_reward -16.331749296011395\n",
      "Agent died: current_steps 1599 total_reward -81.01149494732157\n",
      "Agent died: current_steps 917 total_reward -40.00318152027601\n",
      "Agent died: current_steps 574 total_reward -23.822030239215625\n",
      "Agent died: current_steps 1309 total_reward -51.78304150195118\n",
      "Agent died: current_steps 622 total_reward -36.95572718939063\n",
      "global_step 70000\n",
      "global_step 70000\n",
      "global_step 70000\n",
      "Agent died: current_steps 1599 total_reward -93.72269632899726\n",
      "Agent died: current_steps 511 total_reward -34.36231517360293\n",
      "Agent died: current_steps 141 total_reward -4.933669305431325\n",
      "Agent died: current_steps 888 total_reward -61.30025427493161\n",
      "Agent died: current_steps 240 total_reward -16.191096713075964\n",
      "Agent died: current_steps 53 total_reward -11.817832239522913\n",
      "Agent died: current_steps 1599 total_reward -87.01103255507681\n",
      "Agent died: current_steps 161 total_reward -15.948895256760206\n",
      "Agent died: current_steps 1550 total_reward -83.50409346933124\n",
      "Agent died: current_steps 773 total_reward -36.999554902498836\n",
      "Agent died: current_steps 161 total_reward -0.3860463829139902\n",
      "Agent died: current_steps 1599 total_reward -84.29501384842932\n",
      "Agent died: current_steps 258 total_reward -1.5126833518824059\n",
      "Agent died: current_steps 1599 total_reward -93.29309866345399\n",
      "Agent died: current_steps 1599 total_reward -83.8183622945079\n",
      "Agent died: current_steps 1599 total_reward -89.71382579927987\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    # I don't think I need this method.\n",
    "    return obs\n",
    "\n",
    "n_steps = 110000\n",
    "# Start training the critic DQN after 1000 game iterations.\n",
    "# This has to be a lot bigger than the batch_size defined below.\n",
    "training_start = 1000\n",
    "training_interval = 3 # Run a training step every 3 game iterations start training_start.\n",
    "save_steps = 50\n",
    "copy_steps = 25 # Copy the critic to the actor every 25 training steps.\n",
    "discount_rate = 0.95 # 0.95\n",
    "batch_size = 50\n",
    "iteration = 0\n",
    "checkpoint_path = \"./BipedalWalker-v2.ckpt\"\n",
    "done = True # Environment needs to be reset\n",
    "total_reward = 0\n",
    "current_steps = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        print(\"Restoring checkpoint\")\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        elif step != 0and step % 10000 == 0:\n",
    "            print(\"global_step\", step)\n",
    "        iteration += 1\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            state = preprocess_observation(obs)\n",
    "            total_reward = 0\n",
    "            current_steps = 0\n",
    "        \n",
    "        # Actor evaluates what to do\n",
    "        q_values = actor_q_values.eval(feed_dict={X_state: [state]})\n",
    "        #print(q_values)\n",
    "        action_index = epsilon_greedy(q_values, step)\n",
    "        action = action_index_to_action(action_index)\n",
    "        #print(action)\n",
    "        \n",
    "        #render_env(env)\n",
    "        # Actor takes action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Agent died: current_steps\", current_steps, \"total_reward\", total_reward)\n",
    "            render_env(env)\n",
    "        elif (150 < current_steps and current_steps % 10 == 0):\n",
    "            render_env(env)\n",
    "        total_reward += reward\n",
    "        current_steps += 1\n",
    "        next_state = preprocess_observation(obs)\n",
    "        \n",
    "        # Memorise action.\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "        \n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue\n",
    "        \n",
    "        # Train the critic.\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "            sample_memory(batch_size))\n",
    "        next_q_values = actor_q_values.eval(feed_dict={X_state: X_next_state_val})\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        # Calculate the Actor's reward + future discounted estimated Q-value.\n",
    "        #print(\"rewards\", rewards.shape, \"continues\", continues.shape, \"max_next_q_values\", max_next_q_values.shape)\n",
    "        y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "        #print(X_state, X_action, y)\n",
    "        #print(\"X_state_val\", X_state_val.shape, \"X_action\", X_action_val.shape, \"y_val\", y_val.shape)\n",
    "        training_op.run(feed_dict={X_state: X_state_val,\n",
    "                                  X_action: X_action_val,\n",
    "                                  y: y_val})\n",
    "        \n",
    "        if step % copy_steps == 0:\n",
    "            copy_critic_to_actor.run()\n",
    "        \n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For:\n",
    "neurons_per_layer = [40, 10]\n",
    "learning_rate = 0.001\n",
    "\n",
    "The RL agent learnt how to get an average reward of 3 +- 2. I assume it is learning how to fall over. This is good progress. Running the agent with these hyper paremeters again, did not produce these same results unfortunately.\n",
    "\n",
    "Rendering the environment has shown that a reward of 3-5 is probably just the agent falling as far forward as possible. Dumb robot.\n",
    "\n",
    "After rendering the environment every 50 frames, I can see that it either falls over quickly or it splits it legs and very slowly inches forward and then gets killed after 1599 steps.\n",
    "\n",
    "I couldn't get the agent to learn how to walk and when I checked my code, I realised that I was using the Q-values from the agent as the torque values for it's action, which basically made it stateless (I think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
