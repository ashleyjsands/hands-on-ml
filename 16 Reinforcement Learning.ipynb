{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.layers import batch_norm, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8\n",
    "===\n",
    "\n",
    "Create a DQN to solve the \"BipedalWalker-v2\" gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.74738157e-03 -1.68065156e-05  1.30737595e-03 -1.59998858e-02\n",
      "  9.19198319e-02 -1.72529754e-03  8.60291928e-01  2.67775295e-03\n",
      "  1.00000000e+00  3.23249958e-02 -1.72517262e-03  8.53842065e-01\n",
      "  1.22014224e-03  1.00000000e+00  4.40814197e-01  4.45820302e-01\n",
      "  4.61422980e-01  4.89550382e-01  5.34103036e-01  6.02461278e-01\n",
      "  7.09149182e-01  8.85932207e-01  1.00000000e+00  1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"BipedalWalker-v2\")\n",
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def render_env(env):\n",
    "    plt.imshow(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "#render_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "(4,)\n",
      "[-1 -1 -1 -1]\n",
      "[1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.action_space.shape)\n",
    "print(env.action_space.low)\n",
    "print(env.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_discretized_actions 3 n_actions 4 n_outputs 81\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "neurons_per_layer = [20]#[40, 40, 20]\n",
    "learning_rate = 1 #0.001\n",
    "n_discretized_actions = 3 # -1, -0.5, 0, +0.5, +1\n",
    "n_actions = env.action_space.shape[0]\n",
    "#n_outputs = env.action_space.shape[0]\n",
    "n_outputs = n_discretized_actions ** n_actions\n",
    "print(\"n_discretized_actions\", n_discretized_actions, \"n_actions\", n_actions, \"n_outputs\", n_outputs)\n",
    "\n",
    "def q_network(X_state, scope):\n",
    "    current_layer = X_state\n",
    "    layers = []\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        for neurons in neurons_per_layer:\n",
    "            current_layer = fully_connected(current_layer, neurons)#, activation_fn=tf.nn.sigmoid)\n",
    "            #print(\"current_layer shape\", current_layer.get_shape())\n",
    "        #print(\"input shape\", current_layer.get_shape())\n",
    "        outputs = fully_connected(current_layer, n_outputs, activation_fn=None)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name)]: var for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,)\n"
     ]
    }
   ],
   "source": [
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name actor greedy q value is illegal; using actor_greedy_q_value instead.\n"
     ]
    }
   ],
   "source": [
    "X_state = tf.placeholder(tf.float32, shape=[None, obs.shape[0]])\n",
    "actor_q_values, actor_vars = q_network(X_state, \"q_networks/actor\")\n",
    "#actor_greedy_action_index_summary = tf.summary.scalar(\"actor greedy action index\", tf.argmax(actor_q_values))\n",
    "actor_greedy_q_value_summary = tf.summary.scalar(\"actor greedy q value\", tf.reduce_max(actor_q_values))\n",
    "actor_summary = tf.summary.merge_all()\n",
    "critic_q_values, critic_vars = q_network(X_state, \"q_networks/critic\")\n",
    "\n",
    "copy_ops = [actor_var.assign(critic_vars[var_name])\n",
    "            for var_name, actor_var in actor_vars.items()]\n",
    "copy_critic_to_actor = tf.group(*copy_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic_q_values.get_shape() (?, 81) q_value.get_shape() (?, 1)\n"
     ]
    }
   ],
   "source": [
    "#X_action = tf.placeholder(tf.int32, shape=[None, env.action_space.shape[0]])\n",
    "X_action_index = tf.placeholder(tf.int32, shape=[None])\n",
    "# This contains the Q-value for the actor's chosen action.\n",
    "# q_value = tf.reduce_sum(critic_q_values * tf.one_hot(X_action, n_outputs), axis=1, keep_dims=True)\n",
    "#q_value = tf.reduce_sum(critic_q_values * tf.one_hot(n_outputs, 1), axis=1, keep_dims=True)\n",
    "q_value = tf.reduce_sum(critic_q_values * tf.one_hot(X_action_index, 1), axis=1, keep_dims=True)\n",
    "#q_value = critic_q_values\n",
    "print(\"critic_q_values.get_shape()\", critic_q_values.shape, \"q_value.get_shape()\", q_value.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Actor's reward + future discounted estimated Q-value.\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "cost = tf.reduce_mean(tf.square(y - q_value))\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory_size = 10000\n",
    "replay_memory = deque([], maxlen=replay_memory_size)\n",
    "\n",
    "def sample_memory(batch_size):\n",
    "    indices = np.random.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[], [], [], [], []] # state, action_index, reward, next_state, continue\n",
    "    for idx in indices:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 4)\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "discretized_actions = [-1, 0, 1]\n",
    "action_space_universe = np.array(list(product(discretized_actions, discretized_actions, discretized_actions, discretized_actions)))\n",
    "\n",
    "print(action_space_universe.shape)\n",
    "#print(action_space_universe)\n",
    "\n",
    "def action_index_to_action(action_index):\n",
    "    return action_space_universe[action_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon greed algorithm to explore the state-action space of the environment at the beginning.\n",
    "def epsilon_greedy(q_values, step):\n",
    "    eps_min = 0.05 # At the start 5% of the time the actor will choose the greedy action.\n",
    "    eps_max = 1.0 # Once it has finished exploring, the actor will choose the greedy action 100% of the time.\n",
    "    eps_decay_steps = 200000\n",
    "    \n",
    "    epsilon = max(eps_min, eps_max - (eps_max - eps_min) * step / eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        #return np.random.uniform(env.action_space.low, env.action_space.high, size=n_actions)\n",
    "        return np.random.randint(len(action_space_universe))\n",
    "    else:\n",
    "        #return q_values.reshape(n_outputs)\n",
    "        return np.argmax(q_values) # the optimal/greedy action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent died: current_steps 85 total_reward -6.476626205024618\n",
      "Agent died: current_steps 42 total_reward -10.239901858949413\n",
      "Agent died: current_steps 60 total_reward -2.0543943202539077\n",
      "Agent died: current_steps 68 total_reward -17.565393702150008\n",
      "Agent died: current_steps 1599 total_reward -122.18657861847262\n",
      "Agent died: current_steps 1599 total_reward -120.91493402304148\n",
      "Agent died: current_steps 51 total_reward -18.877167163921523\n",
      "Agent died: current_steps 1599 total_reward -116.83633279889213\n",
      "Agent died: current_steps 1599 total_reward -135.10966720016427\n",
      "Agent died: current_steps 34 total_reward -8.303397609007856\n",
      "Agent died: current_steps 63 total_reward -16.783349152429647\n",
      "Agent died: current_steps 1471 total_reward -137.187065940012\n",
      "Agent died: current_steps 82 total_reward -22.65734494736729\n",
      "Agent died: current_steps 1599 total_reward -109.58809775914712\n",
      "Agent died: current_steps 70 total_reward -21.175027260713893\n",
      "Agent died: current_steps 57 total_reward -4.1809118406636\n",
      "Agent died: current_steps 54 total_reward -15.262842088038715\n",
      "Agent died: current_steps 87 total_reward -23.24613466167698\n",
      "Agent died: current_steps 45 total_reward -8.953758537727095\n",
      "Agent died: current_steps 62 total_reward -11.81865858413402\n",
      "Agent died: current_steps 1599 total_reward -121.5534084355081\n",
      "Agent died: current_steps 64 total_reward -18.76350673057884\n",
      "Agent died: current_steps 1599 total_reward -130.0499593884892\n",
      "Agent died: current_steps 76 total_reward -19.70076783561831\n",
      "Agent died: current_steps 1599 total_reward -120.83346347986264\n",
      "Agent died: current_steps 55 total_reward -8.67770244587275\n",
      "Agent died: current_steps 107 total_reward -0.6117453607600232\n",
      "Agent died: current_steps 83 total_reward -11.774924601939183\n",
      "Agent died: current_steps 1599 total_reward -111.5985131095478\n",
      "Agent died: current_steps 1599 total_reward -104.36027438346973\n",
      "Agent died: current_steps 290 total_reward -31.27117824178743\n",
      "Agent died: current_steps 72 total_reward -12.355331556559227\n",
      "Agent died: current_steps 95 total_reward -22.211536902881527\n",
      "Agent died: current_steps 1599 total_reward -120.01090407693923\n",
      "Agent died: current_steps 68 total_reward -9.61618646216081\n",
      "Agent died: current_steps 1599 total_reward -118.04154417651257\n",
      "Agent died: current_steps 71 total_reward -4.25574939489675\n",
      "Agent died: current_steps 85 total_reward 0.8241158630245636\n",
      "Agent died: current_steps 65 total_reward -14.102008214309807\n",
      "Agent died: current_steps 1599 total_reward -89.57993002669774\n",
      "Agent died: current_steps 1599 total_reward -126.71560562003073\n",
      "Agent died: current_steps 1599 total_reward -109.11885883953259\n",
      "Agent died: current_steps 72 total_reward -0.9907058229384291\n",
      "Agent died: current_steps 1599 total_reward -126.16811168937265\n",
      "Agent died: current_steps 78 total_reward -19.91020087941363\n",
      "Agent died: current_steps 1599 total_reward -106.8411806149449\n",
      "global_step 10000\n",
      "global_step 10000\n",
      "global_step 10000\n",
      "Agent died: current_steps 1599 total_reward -109.87567825051467\n",
      "Agent died: current_steps 55 total_reward -17.777397374732423\n",
      "Agent died: current_steps 62 total_reward -1.1040235214071978\n",
      "Agent died: current_steps 79 total_reward -11.889731168107431\n",
      "Agent died: current_steps 116 total_reward -17.31561334136374\n",
      "Agent died: current_steps 1599 total_reward -105.08690370286652\n",
      "Agent died: current_steps 72 total_reward -12.244742409832146\n",
      "Agent died: current_steps 104 total_reward 4.985046389741822\n",
      "Agent died: current_steps 71 total_reward -5.991062492754308\n",
      "Agent died: current_steps 946 total_reward -87.86032971824022\n",
      "Agent died: current_steps 81 total_reward -5.644206617206336\n",
      "Agent died: current_steps 40 total_reward -11.122608336573466\n",
      "Agent died: current_steps 69 total_reward -5.931250442914663\n",
      "Agent died: current_steps 1599 total_reward -105.00889105612505\n",
      "Agent died: current_steps 51 total_reward -7.5479460095452735\n",
      "Agent died: current_steps 1599 total_reward -116.13371172063692\n",
      "Agent died: current_steps 119 total_reward -1.8546654870609294\n",
      "Agent died: current_steps 83 total_reward -5.2446306908925395\n",
      "Agent died: current_steps 1599 total_reward -128.60838979448403\n",
      "Agent died: current_steps 83 total_reward -0.7839098808653637\n",
      "Agent died: current_steps 99 total_reward -20.7819668193118\n",
      "Agent died: current_steps 1599 total_reward -113.6552028965877\n",
      "Agent died: current_steps 69 total_reward -15.894473864994938\n",
      "Agent died: current_steps 80 total_reward -4.223833350231251\n",
      "Agent died: current_steps 55 total_reward -14.581039985906955\n",
      "Agent died: current_steps 63 total_reward -18.306327048220986\n",
      "Agent died: current_steps 70 total_reward -21.815996527525293\n",
      "Agent died: current_steps 53 total_reward -14.184974366728827\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    # I don't think I need this method.\n",
    "    return obs\n",
    "\n",
    "n_steps = 210000\n",
    "# Start training the critic DQN after 1000 game iterations.\n",
    "# This has to be a lot bigger than the batch_size defined below.\n",
    "training_start = 1000\n",
    "training_interval = 3 # Run a training step every 3 game iterations start training_start.\n",
    "save_steps = 50\n",
    "copy_steps = 25 # Copy the critic to the actor every 25 training steps.\n",
    "discount_rate = 0.99999 # 0.95\n",
    "batch_size = 50\n",
    "iteration = 0\n",
    "checkpoint_path = \"./BipedalWalker-v2.ckpt\"\n",
    "done = True # Environment needs to be reset\n",
    "total_reward = 0\n",
    "current_steps = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    file_writer = tf.summary.FileWriter(\"tf_logs\", sess.graph)\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        print(\"Restoring checkpoint\")\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        elif step != 0and step % 10000 == 0:\n",
    "            print(\"global_step\", step)\n",
    "        iteration += 1\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            state = preprocess_observation(obs)\n",
    "            total_reward = 0\n",
    "            current_steps = 0\n",
    "        \n",
    "        # Actor evaluates what to do\n",
    "        q_values, summary = sess.run([actor_q_values, actor_summary], feed_dict={X_state: [state]})\n",
    "        file_writer.add_summary(summary, step)\n",
    "        #print(q_values)\n",
    "        action_index = epsilon_greedy(q_values, step)\n",
    "        action = action_index_to_action(action_index)\n",
    "        #print(action)\n",
    "        \n",
    "        #render_env(env)\n",
    "        # Actor takes action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Agent died: current_steps\", current_steps, \"total_reward\", total_reward)\n",
    "            render_env(env)\n",
    "        elif (150 < current_steps and current_steps % 10 == 0) or step > 90000:\n",
    "            render_env(env)\n",
    "        total_reward += reward\n",
    "        current_steps += 1\n",
    "        next_state = preprocess_observation(obs)\n",
    "        \n",
    "        # Memorise action.\n",
    "        replay_memory.append((state, action_index, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "        \n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue\n",
    "        \n",
    "        # Train the critic.\n",
    "        X_state_val, X_action_index_val, rewards, X_next_state_val, continues = (\n",
    "            sample_memory(batch_size))\n",
    "        next_q_values = actor_q_values.eval(feed_dict={X_state: X_next_state_val})\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        # Calculate the Actor's reward + future discounted estimated Q-value.\n",
    "        #print(\"rewards\", rewards.shape, \"continues\", continues.shape, \"max_next_q_values\", max_next_q_values.shape)\n",
    "        y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "        #print(X_state, X_action_index, y)\n",
    "        #print(\"X_state_val\", X_state_val.shape, \"X_action_index\", X_action_index_val.shape, \"y_val\", y_val.shape)\n",
    "        training_op.run(feed_dict={X_state: X_state_val,\n",
    "                                  X_action_index: X_action_index_val,\n",
    "                                  y: y_val})\n",
    "        \n",
    "        if step % copy_steps == 0:\n",
    "            copy_critic_to_actor.run()\n",
    "        \n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For:\n",
    "neurons_per_layer = [40, 10]\n",
    "learning_rate = 0.001\n",
    "\n",
    "The RL agent learnt how to get an average reward of 3 +- 2. I assume it is learning how to fall over. This is good progress. Running the agent with these hyper paremeters again, did not produce these same results unfortunately.\n",
    "\n",
    "Rendering the environment has shown that a reward of 3-5 is probably just the agent falling as far forward as possible. Dumb robot.\n",
    "\n",
    "After rendering the environment every 50 frames, I can see that it either falls over quickly or it splits it legs and very slowly inches forward and then gets killed after 1599 steps.\n",
    "\n",
    "I couldn't get the agent to learn how to walk and when I checked my code, I realised that I was using the Q-values from the agent as the torque values for it's action, which basically made it stateless (I think).\n",
    "\n",
    "Watching the agent, I could see that it was using too much force and flipping itself when it wanted to take a step. Let's add some granularity to the torque to see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
